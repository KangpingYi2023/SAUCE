Input arguments = Namespace(data_update='tupleskew', update_size=80000, dataset='bjaq', debug=False, drift_test='js', model_update='adapt', model='face', end2end=False, query_seed=85, random_seed=42, num_workload=3)
JSON-passed parameters = {'data_updater': {'skew_ratio': '1e-3'}, 'random_seed': 8, 'face': {'learning_rate': '1e-4', 'epochs': 0, 'factor': 'auto'}}
MODEL-PATH=/home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
DATASET-PATH=/home/kangping/code_copy/SAUCE/data/bjaq/end2end/bjaq.npy
Workloads: ['QueryWorkload', 'DataUpdateWorkload', 'QueryWorkload']

WORKLOAD-START | Type: QueryWorkload | Progress: 1/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/evaluate/Evaluate-FACE-end2end.py', '--dataset=bjaq', '--end2end']
/home/kangping/anaconda3/envs/LndCar/lib/python3.9/site-packages/torchquad/integration/vegas_map.py:314: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/BucketizationUtils.h:33.)
  iy = torch.searchsorted(self.x_edges[d, :], x[:, d], side='right')
DEVICE NAME
 Tesla V100S-PCIE-32GB
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
data shape: (382168, 5)
Save oracle results to :
./FACE/evaluate/oracle/bjaq_rng-1234.csv
Found oracle card!
torch.Size([5, 2])
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
Took  22.386817932128906
tensor(0.9946)
result is  tensor(380091.5312)
Enter testHyper
ReportEsts: [1.107813835144043, 1.1538176536560059, 1.3555811643600464, 1.0250377655029297, 1.2517668008804321, 1.1567052602767944, 1.267344355583191, 1.0256297588348389, 1.0989646911621094, 1.0162466764450073, 1.0990525484085083, 1.1211864948272705, 1.1046595573425293, 1.03125, 1.0978872776031494, 1.3419970273971558, 1.2374838590621948, 1.169833779335022, 1.1546565294265747, 1.0765305757522583, 1.1320470571517944, 1.057898998260498, 1.1165008544921875, 1.0811926126480103, 1.1462876796722412, 1.0153707265853882, 1.0399774312973022, 1.4037375450134277, 1.0098069906234741, 1.14535653591156, 1.0897481441497803, 1.0444161891937256, 1.1780389547348022, 1.0660362243652344, 1.0481798648834229, 1.1748738288879395, 1.4836739301681519, 1.157979965209961, 1.1211940050125122, 1.1025149822235107, 1.0188918113708496, 1.2353652715682983, 1.226057767868042, 1.0227930545806885, 1.078808307647705, 1.0249433517456055, 1.1154086589813232, 1.2872419357299805, 1.0122766494750977, 1.0507614612579346, 1.2131578922271729, 1.296216607093811, 1.0820701122283936, 1.380935788154602, 1.1490339040756226, 1.006767749786377, 1.436448574066162, 1.1856162548065186, 1.1618789434432983, 1.4760850667953491, 1.3515877723693848, 1.191171646118164, 1.397362470626831, 1.2209826707839966, 1.3147410154342651, 1.2857142686843872, 1.0904347896575928, 1.0564560890197754, 1.065742015838623, 1.3052631616592407, 1.0104585886001587, 1.007714033126831, 1.32305908203125, 1.0505170822143555, 1.149954080581665, 1.0089508295059204, 1.9654428958892822, 1.060150384902954, 1.1048177480697632, 1.2591745853424072, 1.0798742771148682, 1.0707478523254395, 1.0337738990783691, 1.32220458984375, 1.228757381439209, 1.1083136796951294, 1.0878684520721436, 1.0007061958312988, 1.0539191961288452, 1.0056389570236206, 1.0486159324645996, 1.284354567527771, 1.0065336227416992, 1.3980188369750977, 1.2264150381088257, 1.0022488832473755, 1.0592951774597168, 1.2613836526870728, 1.0, 1.1376276016235352, 1.117326021194458, 1.1912503242492676, 1.0520806312561035, 1.0204977989196777, 1.0710251331329346, 1.1161106824874878, 1.1959799528121948, 1.1849912405014038, 1.0920400619506836, 1.0621179342269897, 1.2558401823043823, 1.070319652557373, 1.1315587759017944, 1.090645432472229, 1.0696649551391602, 1.1822916269302368, 1.051510214805603, 1.0318586826324463, 1.2562494277954102, 1.128193974494934, 1.1782846450805664, 1.0980405807495117, 1.1725678443908691, 1.19907808303833, 1.1574612855911255, 1.200165033340454, 1.1090201139450073, 1.0059815645217896, 1.2663934230804443, 1.1432865858078003, 1.0441943407058716, 1.218589186668396, 1.2588564157485962, 1.3571428060531616, 1.079636812210083, 1.0099763870239258, 1.0155335664749146, 1.1167479753494263, 1.1355624198913574, 1.2405966520309448, 1.0863158702850342, 1.1153903007507324, 1.2131856679916382, 1.3195724487304688, 1.0157747268676758, 1.026637077331543, 1.0703306198120117, 1.516051173210144, 1.0387120246887207, 1.031818151473999, 1.1825625896453857, 1.1740802526474, 1.2942721843719482, 1.174419641494751, 1.0359997749328613, 1.0381182432174683, 1.0197724103927612, 1.0270118713378906, 1.075593113899231, 1.1404356956481934, 1.253670573234558, 1.0359448194503784, 1.004868984222412, 1.1610904932022095, 1.182957410812378, 1.1110329627990723, 1.2442423105239868, 1.264365792274475, 1.1989291906356812, 1.0299092531204224, 1.0021551847457886, 1.03049898147583, 1.1428571939468384, 1.1537089347839355, 1.2288827896118164, 1.1907087564468384, 1.0281827449798584, 1.297982096672058, 1.055271029472351, 1.228102684020996, 1.0197943449020386, 1.0718119144439697, 1.0202988386154175, 1.1542390584945679, 1.1713147163391113, 1.163847804069519, 1.0994598865509033, 1.319985270500183, 1.0953301191329956, 1.410254955291748, 1.0208255052566528, 1.0174461603164673, 1.4392335414886475, 1.2636728286743164, 2.1757924556732178, 1.2708196640014648, 1.079949975013733, 1.0704225301742554, 1.2311524152755737, 1.004770278930664]
********** total_n=[200] batchn=[100]  N=[3200]  nitr=[4]  alpha=[0.4]  beta=[0.2] ******
@ Average per query          [24.856845140457153] ms
 --  Average per query NF    [5.991095304489136] ms
 --  Average per query vegas [18.865749835968018] ms
Mean [1.153]  Median [1.117]  95th [1.397]  99th [1.521]  max [2.176]
Mean [1.153]  Median [1.117]  95th [1.397]  99th [1.521]  max [2.176]


WORKLOAD-FINISHED | Type: QueryWorkload | Progress: 1/3 | Workload-time: 30.529846 | Model-update-time: 0.000000


WORKLOAD-START | Type: DataUpdateWorkload | Progress: 2/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/Naru/eval_model.py', '--model=face', '--data_update=tupleskew', '--dataset=bjaq', '--drift_test=js', '--end2end', '--update_size=80000', '--query_seed=85', '--eval_type=drift']
Torch device: cuda:1
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
TupleSkewSample - START
0.001
TupleSkewSample - END
load_npy_dataset_from_path - df.shape = (458601, 5)
Parsing... done, took 0.0s
Data updated!
Previous data size: (382168, 5), new data size: (76433, 5)
[1.4603138e-05 8.7618828e-06 5.2630901e-05 3.3438206e-05 5.8770180e-05]
Distance score: 3.364086296642199e-05
SAUCE Drift detection: True
Detection latency: 0.0233s

DRIFT-DETECTED after 2-th workload
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/incremental_train.py', '--dataset=bjaq', '--end2end', '--model=face', '--model_update=adapt', '--update_size=80000', '--epochs=0']
Torch device: cuda:1
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
update_size: 76433
AdaptTask - START
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
train data shape: 80000, transfer data shape:16000
Training done; evaluating likelihood on full data:
Saved to: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
AdaptTask - END

WORKLOAD-FINISHED | Type: DataUpdateWorkload | Progress: 2/3 | Workload-time: 2.037944 | Model-update-time: 3.122842


WORKLOAD-START | Type: QueryWorkload | Progress: 3/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/evaluate/Evaluate-FACE-end2end.py', '--dataset=bjaq', '--query_seed=85', '--end2end', '--num_workload=3', '--data_update=tupleskew', '--model_update=adapt']
/home/kangping/anaconda3/envs/LndCar/lib/python3.9/site-packages/torchquad/integration/vegas_map.py:314: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/BucketizationUtils.h:33.)
  iy = torch.searchsorted(self.x_edges[d, :], x[:, d], side='right')
DEVICE NAME
 Tesla V100S-PCIE-32GB
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
data shape: (458601, 5)
Save oracle results to :
./FACE/evaluate/oracle/bjaq_rng-85.csv
Found oracle card!
torch.Size([5, 2])
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
Took  22.319783210754395
tensor(0.9957)
result is  tensor(456628.2500)
Enter testHyper
ReportEsts: [1.0608468055725098, 1.0745139122009277, 1.3150684833526611, 1.3689651489257812, 1.0165069103240967, 1.0549306869506836, 1.0303295850753784, 1.0877916812896729, 1.3698316812515259, 1.1684993505477905, 1.0495014190673828, 1.186286211013794, 1.0215047597885132, 1.5152041912078857, 1.0666366815567017, 1.265562891960144, 1.091464877128601, 1.032064437866211, 1.1112004518508911, 1.6085823774337769, 1.1392303705215454, 1.0560506582260132, 1.1694386005401611, 1.2187350988388062, 1.0924171209335327, 1.3032159805297852, 1.0950000286102295, 1.0348517894744873, 1.1935604810714722, 1.104316234588623, 1.043938398361206, 1.0502874851226807, 1.177321195602417, 1.1890958547592163, 1.152688980102539, 1.2583415508270264, 1.0198017358779907, 1.1649638414382935, 1.1529375314712524, 1.1153849363327026, 1.0310956239700317, 1.2531731128692627, 1.1439708471298218, 1.0415232181549072, 1.0892436504364014, 1.341451644897461, 1.0252889394760132, 1.5442304611206055, 1.145551085472107, 1.1692513227462769, 1.7993730306625366, 1.3102067708969116, 1.090878963470459, 1.1924914121627808, 1.1057476997375488, 1.4777878522872925, 1.0666565895080566, 1.5101351737976074, 1.0550360679626465, 1.0412087440490723, 1.0623341798782349, 1.0779556035995483, 1.1122710704803467, 1.1116937398910522, 1.0079504251480103, 1.2496178150177002, 1.04818856716156, 1.0203297138214111, 1.0596684217453003, 1.0592713356018066, 1.1676846742630005, 1.184787392616272, 1.0179802179336548, 1.0761682987213135, 1.1075165271759033, 1.0761712789535522, 1.0368715524673462, 1.000144362449646, 1.2298264503479004, 1.381042718887329, 1.0596920251846313, 1.130826473236084, 1.6959847211837769, 1.3043460845947266, 1.2412713766098022, 1.0159834623336792, 1.0962791442871094, 1.7002285718917847, 1.0794968605041504, 1.0525792837142944, 1.1181976795196533, 1.2172844409942627, 1.4234527349472046, 1.1938673257827759, 1.247292399406433, 1.0115327835083008, 1.4091178178787231, 1.004733920097351, 1.0293724536895752, 1.07667076587677, 1.126556396484375, 1.035599708557129, 1.1000014543533325, 1.1857142448425293, 1.0527677536010742, 1.1501821279525757, 1.070725440979004, 1.445610523223877, 1.3544516563415527, 1.0102571249008179, 1.4136691093444824, 1.0294749736785889, 1.1016185283660889, 1.0450153350830078, 1.6336514949798584, 1.0997086763381958, 1.016475796699524, 1.1839962005615234, 1.0507570505142212, 1.2028470039367676, 1.169846534729004, 1.183585286140442, 1.164484977722168, 1.0623703002929688, 1.2576531171798706, 1.0484658479690552, 1.098908543586731, 1.0078462362289429, 1.0532419681549072, 1.0634863376617432, 1.2237809896469116, 1.0316282510757446, 1.1345620155334473, 1.0921157598495483, 1.2156955003738403, 1.0078142881393433, 1.1996679306030273, 1.1034826040267944, 1.1269224882125854, 1.1144613027572632, 1.266939640045166, 1.0551083087921143, 1.3457592725753784, 1.2973690032958984, 1.214076280593872, 1.38999342918396, 1.4131627082824707, 1.7418752908706665, 1.468493103981018, 1.0646214485168457, 1.3599432706832886, 1.0500043630599976, 1.0490131378173828, 1.1523120403289795, 1.064774990081787, 1.0562070608139038, 1.0049874782562256, 1.2062124013900757, 1.1770546436309814, 3.380281686782837, 1.1837738752365112, 1.0411850214004517, 1.0876401662826538, 1.0510320663452148, 1.3489362001419067, 1.240841031074524, 1.2974841594696045, 1.3808600902557373, 1.0276240110397339, 1.136694312095642, 1.074564814567566, 1.0258831977844238, 1.3484119176864624, 1.1093195676803589, 1.03937828540802, 1.0132488012313843, 1.249564528465271, 1.0862503051757812, 1.019335150718689, 1.2177153825759888, 1.0394699573516846, 1.185089349746704, 1.2177248001098633, 1.0692514181137085, 1.6143884658813477, 1.033075213432312, 1.0207115411758423, 1.1584876775741577, 1.2231941223144531, 1.3403092622756958, 1.0999884605407715, 1.0575860738754272, 44.33333206176758, 1.0434961318969727, 1.1266956329345703, 1.4901961088180542, 1.1285278797149658, 1.0885435342788696, 1.0204815864562988, 1.3368237018585205]
********** total_n=[200] batchn=[100]  N=[3200]  nitr=[4]  alpha=[0.4]  beta=[0.2] ******
@ Average per query          [26.054213047027588] ms
 --  Average per query NF    [4.717539548873901] ms
 --  Average per query vegas [21.336673498153687] ms
Mean [1.394]  Median [1.112]  95th [1.517]  99th [1.815]  max [44.333]
Mean [1.394]  Median [1.112]  95th [1.517]  99th [1.815]  max [44.333]


WORKLOAD-FINISHED | Type: QueryWorkload | Progress: 3/3 | Workload-time: 31.461045 | Model-update-time: 0.000000


Experiment Summary: #data-update-times=1, #drift=1 | total-time=67.205195