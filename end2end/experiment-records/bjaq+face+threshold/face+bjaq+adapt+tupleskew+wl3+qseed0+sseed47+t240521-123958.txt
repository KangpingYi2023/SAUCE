Input arguments = Namespace(data_update='tupleskew', update_size=80000, dataset='bjaq', debug=False, drift_test='js', model_update='adapt', model='face', end2end=False, query_seed=0, random_seed=42, num_workload=3)
JSON-passed parameters = {'data_updater': {'skew_ratio': '1e-3'}, 'random_seed': 47, 'face': {'learning_rate': '1e-4', 'epochs': 0, 'factor': 'auto'}}
MODEL-PATH=/home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
DATASET-PATH=/home/kangping/code_copy/SAUCE/data/bjaq/end2end/bjaq.npy
Workloads: ['QueryWorkload', 'DataUpdateWorkload', 'QueryWorkload']

WORKLOAD-START | Type: QueryWorkload | Progress: 1/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/evaluate/Evaluate-FACE-end2end.py', '--dataset=bjaq', '--end2end']
/home/kangping/anaconda3/envs/LndCar/lib/python3.9/site-packages/torchquad/integration/vegas_map.py:314: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/BucketizationUtils.h:33.)
  iy = torch.searchsorted(self.x_edges[d, :], x[:, d], side='right')
DEVICE NAME
 Tesla V100S-PCIE-32GB
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
data shape: (382168, 5)
Save oracle results to :
./FACE/evaluate/oracle/bjaq_rng-1234.csv
Found oracle card!
torch.Size([5, 2])
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
Took  8.169574737548828
tensor(0.9936)
result is  tensor(379729.5000)
Enter testHyper
ReportEsts: [1.136592149734497, 1.132623553276062, 1.595919132232666, 1.0949673652648926, 1.241530418395996, 1.1466829776763916, 1.1294541358947754, 1.096265435218811, 1.082808256149292, 1.112606406211853, 1.1576226949691772, 1.1082626581192017, 1.078730821609497, 1.0200892686843872, 1.1578640937805176, 1.336775541305542, 1.4337657690048218, 1.1081948280334473, 1.1848660707473755, 1.1045918464660645, 1.1560026407241821, 1.1306532621383667, 1.1286672353744507, 1.0142533779144287, 1.2403823137283325, 1.106690764427185, 1.039414405822754, 1.3463943004608154, 1.0462042093276978, 1.127657413482666, 1.1238027811050415, 1.2012605667114258, 1.048049807548523, 1.041902780532837, 1.098624587059021, 1.070835828781128, 1.274468183517456, 1.0030479431152344, 1.1007399559020996, 1.0093413591384888, 1.0603759288787842, 1.2123850584030151, 1.0279752016067505, 1.0148955583572388, 1.1345869302749634, 1.0716888904571533, 1.140397071838379, 1.0887260437011719, 1.2508302927017212, 1.1522842645645142, 1.1943005323410034, 1.4041986465454102, 1.0238733291625977, 1.123641014099121, 1.1270617246627808, 1.0324524641036987, 1.3242990970611572, 1.0618793964385986, 1.0828970670700073, 1.4441074132919312, 1.1672030687332153, 1.1795574426651, 1.3919280767440796, 1.0371932983398438, 1.2840466499328613, 1.3142857551574707, 1.070879578590393, 1.0247453451156616, 1.0191065073013306, 1.3473684787750244, 1.086082100868225, 1.0475608110427856, 1.2445391416549683, 1.1996110677719116, 1.204028844833374, 1.0226712226867676, 1.8157830238342285, 1.1553364992141724, 1.0486680269241333, 1.2562119960784912, 1.1836923360824585, 1.0495880842208862, 1.0014952421188354, 1.207911729812622, 1.290672779083252, 1.1162017583847046, 1.3090617656707764, 1.1510719060897827, 1.0719363689422607, 1.007603645324707, 1.0522757768630981, 1.2882959842681885, 1.0420894622802734, 1.3853510618209839, 1.048387050628662, 1.159529447555542, 1.162358045578003, 1.2785923480987549, 1.0714285373687744, 1.1611272096633911, 1.1904069185256958, 1.1878347396850586, 1.0447527170181274, 1.019033670425415, 1.139853596687317, 1.0646836757659912, 1.1484915018081665, 1.2198952436447144, 1.04023015499115, 1.0720809698104858, 1.2078936100006104, 1.079189658164978, 1.285577416419983, 1.050953984260559, 1.1228700876235962, 1.207446813583374, 1.0004488229751587, 1.0953179597854614, 1.3398443460464478, 1.0763174295425415, 1.3942992687225342, 1.108527660369873, 1.014002799987793, 1.1052955389022827, 1.1472867727279663, 1.2152290344238281, 1.1373955011367798, 1.2272062301635742, 1.3137755393981934, 1.0925183296203613, 1.0819998979568481, 1.068515419960022, 1.0602859258651733, 1.3928571939468384, 1.2212058305740356, 1.0412181615829468, 1.0040522813796997, 1.2162601947784424, 1.0786869525909424, 1.0981630086898804, 1.1274826526641846, 1.1631945371627808, 1.143700122833252, 1.3049501180648804, 1.083160638809204, 1.0268877744674683, 1.1801811456680298, 1.501778244972229, 1.0234320163726807, 1.2641335725784302, 1.2505743503570557, 1.1661325693130493, 1.351069688796997, 1.425110936164856, 1.1068713665008545, 1.0794018507003784, 1.0017306804656982, 1.011383056640625, 1.068396806716919, 1.22500741481781, 1.292419672012329, 1.0052605867385864, 1.1493260860443115, 1.2090322971343994, 1.1582821607589722, 1.097132921218872, 1.3327351808547974, 1.2651474475860596, 1.0750524997711182, 1.044154167175293, 1.032880425453186, 1.0769872665405273, 1.1883116960525513, 1.0976425409317017, 1.0139645338058472, 1.0492874383926392, 1.0295147895812988, 1.2382887601852417, 1.1312592029571533, 1.0830848217010498, 1.078866720199585, 1.0485241413116455, 1.0233029127120972, 1.054136872291565, 1.1369068622589111, 1.1057082414627075, 1.0358176231384277, 1.2584939002990723, 1.0005146265029907, 1.4244991540908813, 1.111946702003479, 1.0790083408355713, 1.457982063293457, 1.127350091934204, 1.8744457960128784, 1.2447013854980469, 1.0173444747924805, 1.085714340209961, 1.0020588636398315, 1.1890438795089722]
********** total_n=[200] batchn=[100]  N=[3200]  nitr=[4]  alpha=[0.4]  beta=[0.2] ******
@ Average per query          [3.757038116455078] ms
 --  Average per query NF    [1.361473798751831] ms
 --  Average per query vegas [2.395564317703247] ms
Mean [1.154]  Median [1.124]  95th [1.395]  99th [1.598]  max [1.874]
Mean [1.154]  Median [1.124]  95th [1.395]  99th [1.598]  max [1.874]


WORKLOAD-FINISHED | Type: QueryWorkload | Progress: 1/3 | Workload-time: 11.821727 | Model-update-time: 0.000000


WORKLOAD-START | Type: DataUpdateWorkload | Progress: 2/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/Naru/eval_model.py', '--model=face', '--data_update=tupleskew', '--dataset=bjaq', '--drift_test=js', '--end2end', '--update_size=80000', '--query_seed=0', '--eval_type=drift']
Torch device: cuda:1
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
TupleSkewSample - START
0.001
TupleSkewSample - END
load_npy_dataset_from_path - df.shape = (458601, 5)
Parsing... done, took 0.0s
Data updated!
Previous data size: (382168, 5), new data size: (76433, 5)
[9.7155571e-06 5.3048134e-06 2.3174286e-04 5.6624413e-06 2.2590160e-05]
Distance score: 5.500316547113471e-05
SAUCE Drift detection: True
Detection latency: 0.0229s

DRIFT-DETECTED after 2-th workload
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/incremental_train.py', '--dataset=bjaq', '--end2end', '--model=face', '--model_update=adapt', '--update_size=80000', '--epochs=0']
Torch device: cuda:1
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
update_size: 76433
AdaptTask - START
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
train data shape: 80000, transfer data shape:16000
Training done; evaluating likelihood on full data:
Saved to: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
AdaptTask - END

WORKLOAD-FINISHED | Type: DataUpdateWorkload | Progress: 2/3 | Workload-time: 2.011722 | Model-update-time: 2.199177


WORKLOAD-START | Type: QueryWorkload | Progress: 3/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/evaluate/Evaluate-FACE-end2end.py', '--dataset=bjaq', '--query_seed=0', '--end2end', '--num_workload=3', '--data_update=tupleskew', '--model_update=adapt']
/home/kangping/anaconda3/envs/LndCar/lib/python3.9/site-packages/torchquad/integration/vegas_map.py:314: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/BucketizationUtils.h:33.)
  iy = torch.searchsorted(self.x_edges[d, :], x[:, d], side='right')
DEVICE NAME
 Tesla V100S-PCIE-32GB
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
data shape: (458601, 5)
Save oracle results to :
./FACE/evaluate/oracle/bjaq_rng-0.csv
Found oracle card!
torch.Size([5, 2])
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
Took  8.171167373657227
tensor(0.9936)
result is  tensor(455683.5625)
Enter testHyper
ReportEsts: [1.095257043838501, 1.0068769454956055, 1.0945736169815063, 2.143702507019043, 1.157707691192627, 1.044789433479309, 1.1951745748519897, 1.006308913230896, 1.0850142240524292, 1.043469786643982, 1.09628427028656, 1.0633906126022339, 1.0143197774887085, 1.0305700302124023, 1.0285403728485107, 1.0015785694122314, 1.1015337705612183, 1.2026042938232422, 1.0090594291687012, 1.2010313272476196, 1.1461045742034912, 1.1055537462234497, 1.482118844985962, 1.0649296045303345, 1.2721264362335205, 1.308230996131897, 1.0477467775344849, 1.101162314414978, 1.1058621406555176, 1.2955434322357178, 1.1432347297668457, 1.7989418506622314, 1.1277471780776978, 1.0784387588500977, 1.0118714570999146, 1.0131367444992065, 1.1704665422439575, 1.245018720626831, 1.3976413011550903, 1.064987301826477, 1.1150280237197876, 1.0275132656097412, 1.2034868001937866, 1.1517530679702759, 1.131540298461914, 1.012553334236145, 1.302703857421875, 1.102730393409729, 1.3142857551574707, 1.1642597913742065, 1.106740951538086, 1.0255649089813232, 1.0264437198638916, 1.137198567390442, 1.0097827911376953, 1.0950193405151367, 1.1309231519699097, 1.253430962562561, 1.2837142944335938, 1.0640274286270142, 1.3300248384475708, 1.028519868850708, 1.1046077013015747, 1.3592926263809204, 1.009161353111267, 1.0728930234909058, 1.1459113359451294, 1.091672658920288, 1.2302751541137695, 1.1519479751586914, 1.178384780883789, 1.135984182357788, 1.1716145277023315, 1.0670359134674072, 1.0725730657577515, 1.335329532623291, 1.2184631824493408, 1.066228985786438, 1.337087631225586, 1.0915125608444214, 1.2400739192962646, 1.2854394912719727, 1.0190900564193726, 1.315476894378662, 1.4434207677841187, 1.5829787254333496, 1.2644808292388916, 1.550872564315796, 1.2697346210479736, 5.952381134033203, 1.0537335872650146, 1.2045503854751587, 1.460341215133667, 1.2813191413879395, 1.2672605514526367, 1.0196824073791504, 1.1102244853973389, 1.2455285787582397, 1.1761752367019653, 1.5924479961395264, 1.087752103805542, 1.1227046251296997, 2.073232412338257, 1.177100419998169, 1.091692328453064, 1.0822309255599976, 1.1290687322616577, 1.0779019594192505, 1.1117478609085083, 1.1805224418640137, 1.107972502708435, 1.1224005222320557, 1.2134947776794434, 1.2241276502609253, 1.1768707036972046, 1.040700078010559, 1.0786781311035156, 1.4211758375167847, 1.3557876348495483, 1.0707964897155762, 1.0444614887237549, 1.4996885061264038, 1.1076215505599976, 1.1541944742202759, 1.3319203853607178, 1.2856215238571167, 1.0559076070785522, 1.0482114553451538, 1.049880027770996, 1.1703346967697144, 4.059602737426758, 1.115570068359375, 1.1769810914993286, 2.093560218811035, 1.6363636255264282, 1.0391286611557007, 1.141121506690979, 1.2304508686065674, 1.0293914079666138, 1.009841799736023, 1.0568134784698486, 1.0380706787109375, 1.1359825134277344, 1.012405514717102, 1.2105531692504883, 1.0905444622039795, 1.3024691343307495, 1.1070342063903809, 1.132938265800476, 1.0802732706069946, 1.0746634006500244, 1.0780036449432373, 1.1210635900497437, 1.1011897325515747, 1.040094256401062, 1.0178354978561401, 1.2312241792678833, 2.580152750015259, 1.1099735498428345, 1.1291159391403198, 1.1540496349334717, 1.0849947929382324, 1.4470813274383545, 1.0647201538085938, 1.215910792350769, 1.205572485923767, 1.0356568098068237, 1.1769447326660156, 1.518175721168518, 1.121500849723816, 1.2052333354949951, 1.1241257190704346, 1.3994393348693848, 1.196579933166504, 1.1669504642486572, 1.0228573083877563, 1.1500465869903564, 1.0870965719223022, 1.0030258893966675, 1.0084134340286255, 1.9473683834075928, 1.167839765548706, 1.028491735458374, 1.3659783601760864, 1.2678048610687256, 1.4458296298980713, 1.0136425495147705, 1.12594473361969, 1.406691074371338, 1.0309385061264038, 1.1488583087921143, 1.118605375289917, 1.232391595840454, 2.234138011932373, 1.0856428146362305, 1.2034248113632202, 1.4002912044525146, 1.4548494815826416, 1.0781759023666382, 1.080964207649231]
********** total_n=[200] batchn=[100]  N=[3200]  nitr=[4]  alpha=[0.4]  beta=[0.2] ******
@ Average per query          [3.7753677368164062] ms
 --  Average per query NF    [1.3701653480529785] ms
 --  Average per query vegas [2.4052023887634277] ms
Mean [1.234]  Median [1.130]  95th [1.595]  99th [2.595]  max [5.952]
Mean [1.234]  Median [1.130]  95th [1.595]  99th [2.595]  max [5.952]


WORKLOAD-FINISHED | Type: QueryWorkload | Progress: 3/3 | Workload-time: 12.383765 | Model-update-time: 0.000000


Experiment Summary: #data-update-times=1, #drift=1 | total-time=28.490151