Input arguments = Namespace(data_update='sample', update_size=80000, dataset='bjaq', debug=False, drift_test='js', model_update='adapt', model='face', end2end=False, query_seed=0, random_seed=42, num_workload=3)
JSON-passed parameters = {'data_updater': {'skew_ratio': '1e-3'}, 'random_seed': 6, 'face': {'learning_rate': '1e-4', 'epochs': 0, 'factor': 'auto'}}
MODEL-PATH=/home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
DATASET-PATH=/home/kangping/code_copy/SAUCE/data/bjaq/end2end/bjaq.npy
Workloads: ['QueryWorkload', 'DataUpdateWorkload', 'QueryWorkload']

WORKLOAD-START | Type: QueryWorkload | Progress: 1/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/evaluate/Evaluate-FACE-end2end.py', '--dataset=bjaq', '--end2end']
/home/kangping/anaconda3/envs/LndCar/lib/python3.9/site-packages/torchquad/integration/vegas_map.py:314: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/BucketizationUtils.h:33.)
  iy = torch.searchsorted(self.x_edges[d, :], x[:, d], side='right')
DEVICE NAME
 Tesla V100S-PCIE-32GB
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
data shape: (382168, 5)
Save oracle results to :
./FACE/evaluate/oracle/bjaq_rng-1234.csv
Found oracle card!
torch.Size([5, 2])
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
Took  8.150890588760376
tensor(0.9969)
result is  tensor(380998.5625)
Enter testHyper
ReportEsts: [1.185951590538025, 1.0171607732772827, 1.215298056602478, 1.118130087852478, 1.2521355152130127, 1.1149688959121704, 1.271263837814331, 1.0853159427642822, 1.050349235534668, 1.1483205556869507, 1.0749354362487793, 1.1584745645523071, 1.0436674356460571, 1.0691964626312256, 1.0986945629119873, 1.2562544345855713, 1.2548162937164307, 1.1839667558670044, 1.2222042083740234, 1.0204081535339355, 1.1657241582870483, 1.097561001777649, 1.1518391370773315, 1.0244882106781006, 1.117618203163147, 1.0822784900665283, 1.1241554021835327, 1.3868613243103027, 1.0232704877853394, 1.0668294429779053, 1.1763036251068115, 1.1419380903244019, 1.0105462074279785, 1.0514756441116333, 1.250905990600586, 1.1488978862762451, 1.3773566484451294, 1.0912604331970215, 1.1298588514328003, 1.028578519821167, 1.0434316396713257, 1.1082487106323242, 1.0877496004104614, 1.061636209487915, 1.0756391286849976, 1.0816326141357422, 1.113498330116272, 1.3136405944824219, 1.0330934524536133, 1.133671760559082, 1.2228116989135742, 1.2274978160858154, 1.020249366760254, 1.2336244583129883, 1.0670946836471558, 1.0417245626449585, 1.2570093870162964, 1.214733600616455, 1.2953579425811768, 1.45292067527771, 1.2997143268585205, 1.196466326713562, 1.3557238578796387, 1.0720738172531128, 1.2313432693481445, 1.0571428537368774, 1.0961538553237915, 1.0101752281188965, 1.0118762254714966, 1.2526315450668335, 1.0605802536010742, 1.0906611680984497, 1.210130214691162, 1.1435524225234985, 1.149954080581665, 1.135581374168396, 1.7624280452728271, 1.0532779693603516, 1.1236016750335693, 1.5087946653366089, 1.3104392290115356, 1.0479729175567627, 1.0485634803771973, 1.2679041624069214, 1.2293847799301147, 1.0382263660430908, 1.0109323263168335, 1.1773518323898315, 1.109471082687378, 1.012521743774414, 1.020749568939209, 1.4080872535705566, 1.021637201309204, 1.2773265838623047, 1.21875, 1.0469660758972168, 1.082491397857666, 1.3284465074539185, 1.01694917678833, 1.22488534450531, 1.2044117450714111, 1.3310998678207397, 1.0211986303329468, 1.0066323280334473, 1.0503138303756714, 1.0422654151916504, 1.1885026693344116, 1.1849912405014038, 1.051790714263916, 1.0340476036071777, 1.0128504037857056, 1.085185170173645, 1.2284132242202759, 1.0772230625152588, 1.0483366250991821, 1.1464647054672241, 1.0204802751541138, 1.1489745378494263, 1.2257236242294312, 1.0280563831329346, 1.245563268661499, 1.0919691324234009, 1.0704140663146973, 1.1334834098815918, 1.3328487873077393, 1.1843788623809814, 1.158469557762146, 1.1010984182357788, 1.3493449687957764, 1.1165664196014404, 1.0719770193099976, 1.0785428285598755, 1.2029210329055786, 1.4642857313156128, 1.0578792095184326, 1.0897873640060425, 1.0701276063919067, 1.2552845478057861, 1.1496092081069946, 1.168601155281067, 1.1231021881103516, 1.1013197898864746, 1.1411439180374146, 1.2527118921279907, 1.014225959777832, 1.0043213367462158, 1.0508309602737427, 1.3767707347869873, 1.0891810655593872, 1.0559659004211426, 1.2703630924224854, 1.180233359336853, 1.294952630996704, 1.0692633390426636, 1.351365566253662, 1.0441476106643677, 1.0916242599487305, 1.0309510231018066, 1.0163781642913818, 1.1880632638931274, 1.1411523818969727, 1.0833861827850342, 1.0098669528961182, 1.373900294303894, 1.1611316204071045, 1.0795007944107056, 1.046313762664795, 1.2106152772903442, 1.0578265190124512, 1.020131230354309, 1.0186700820922852, 1.1208993196487427, 1.2142857313156128, 1.127905011177063, 1.0610769987106323, 1.0420893430709839, 1.0714285373687744, 1.285016655921936, 1.1932579278945923, 1.1266120672225952, 1.0046520233154297, 1.0017452239990234, 1.0165730714797974, 1.1491317749023438, 1.1528431177139282, 1.1448203325271606, 1.0739362239837646, 1.2727594375610352, 1.009400725364685, 1.4611488580703735, 1.0371848344802856, 1.021262288093567, 1.4810585975646973, 1.0059559345245361, 1.6826744079589844, 1.1853210926055908, 1.072905421257019, 1.1875, 1.0282903909683228, 1.1630810499191284]
********** total_n=[200] batchn=[100]  N=[3200]  nitr=[4]  alpha=[0.4]  beta=[0.2] ******
@ Average per query          [3.788418769836426] ms
 --  Average per query NF    [1.3637292385101318] ms
 --  Average per query vegas [2.424689531326294] ms
Mean [1.146]  Median [1.117]  95th [1.377]  99th [1.511]  max [1.762]
Mean [1.146]  Median [1.117]  95th [1.377]  99th [1.511]  max [1.762]


WORKLOAD-FINISHED | Type: QueryWorkload | Progress: 1/3 | Workload-time: 11.820609 | Model-update-time: 0.000000


WORKLOAD-START | Type: DataUpdateWorkload | Progress: 2/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/Naru/eval_model.py', '--model=face', '--data_update=sample', '--dataset=bjaq', '--drift_test=js', '--end2end', '--update_size=80000', '--query_seed=0', '--eval_type=drift']
Torch device: cuda:1
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
Sample - START
Sample - END
load_npy_dataset_from_path - df.shape = (458601, 5)
Parsing... done, took 0.0s
Data updated!
Previous data size: (382168, 5), new data size: (76433, 5)
[4.7683716e-07 1.2516975e-06 0.0000000e+00 1.7881393e-07 8.9406967e-07]
Distance score: 5.602836381513043e-07
SAUCE Drift detection: False
Detection latency: 0.0232s

WORKLOAD-FINISHED | Type: DataUpdateWorkload | Progress: 2/3 | Workload-time: 2.020917 | Model-update-time: 0.000000


WORKLOAD-START | Type: QueryWorkload | Progress: 3/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/evaluate/Evaluate-FACE-end2end.py', '--dataset=bjaq', '--query_seed=0', '--end2end', '--num_workload=3', '--data_update=sample', '--model_update=adapt']
/home/kangping/code_copy/SAUCE/./FACE/FACE_utils/dataUtils.py:321: RuntimeWarning: invalid value encountered in cast
  oracle_cards = oracle_cards.astype(np.int32)
/home/kangping/anaconda3/envs/LndCar/lib/python3.9/site-packages/torchquad/integration/vegas_map.py:314: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/BucketizationUtils.h:33.)
  iy = torch.searchsorted(self.x_edges[d, :], x[:, d], side='right')
DEVICE NAME
 Tesla V100S-PCIE-32GB
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
data shape: (458601, 5)
Save oracle results to :
./FACE/evaluate/oracle/bjaq_rng-0.csv
Found oracle card!
torch.Size([5, 2])
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
Took  8.211395978927612
tensor(0.9934)
result is  tensor(455562.3750)
Enter testHyper
ReportEsts: [1.0295878648757935, 1.0452759265899658, 1.1689373254776, 1.2902475595474243, 1.0214155912399292, 1.4657505750656128, 1.228893756866455, 1.0393917560577393, 1.105094075202942, 1.1316808462142944, 1.0168558359146118, 1.1468009948730469, 1.2338308095932007, 1.0984606742858887, 1.320274829864502, 1.0285320281982422, 1.0519239902496338, 1.0483620166778564, 1.0996782779693604, 1.3154947757720947, 1.0403151512145996, 1.3860644102096558, 1.107928991317749, 1.0048913955688477, 1.1178950071334839, 1.1262911558151245, 1.2340655326843262, 1.1211411952972412, 1.2560902833938599, 1.4905983209609985, 1.1871992349624634, 1.2127659320831299, 1.0416972637176514, 1.0896234512329102, 1.087554693222046, 1.034417748451233, 1.1293917894363403, 1.173438310623169, 1.196143627166748, 1.0159034729003906, 1.0368355512619019, 1.0086300373077393, 1.18559992313385, 1.1844273805618286, 1.0012272596359253, 1.1676363945007324, 1.5317738056182861, 1.1064199209213257, 1.1440329551696777, 1.222857117652893, 1.0018044710159302, 1.0150872468948364, 1.0421005487442017, 1.0173828601837158, 1.0047990083694458, 1.1168345212936401, 1.0039291381835938, 1.0737204551696777, 1.2990467548370361, 1.153906226158142, 1.5069124698638916, 1.0015710592269897, 1.001723289489746, 1.522902250289917, 1.089552640914917, 1.0326629877090454, 1.2692142724990845, 1.0776236057281494, 1.107050895690918, 1.04501473903656, 1.1954057216644287, 1.2516043186187744, 1.051834225654602, 1.110797643661499, 1.1759002208709717, 1.042549967765808, 1.0069291591644287, 1.2443478107452393, 1.34260892868042, 1.1676795482635498, 1.215078353881836, 1.0114763975143433, 1.1523727178573608, 1.2475262880325317, 1.394299864768982, 1.3438596725463867, 1.14235520362854, 1.4811729192733765, 1.061327576637268, 1.0256260633468628, 1.177080512046814, 1.1615245342254639, 1.1756476163864136, 1.2558698654174805, 1.3593827486038208, 1.0136818885803223, 1.04300057888031, 1.2744015455245972, 1.8740156888961792, 1.0061755180358887, 1.1196556091308594, 1.1134275197982788, 1.0714285373687744, 1.2265729904174805, 1.1703513860702515, 1.2174443006515503, 1.09876549243927, 1.1409465074539185, 1.234956979751587, 1.050986647605896, 1.036947250366211, 1.0794920921325684, 1.1953518390655518, 1.3467730283737183, 1.0855950117111206, 1.1348228454589844, 1.107638955116272, 1.3399063348770142, 1.0887058973312378, 1.0, 1.034973382949829, 1.1609907150268555, 1.0906448364257812, 1.5211210250854492, 1.0571057796478271, 1.0223807096481323, 1.1757444143295288, 1.3034591674804688, 1.1586745977401733, 1.0156491994857788, 1.0100082159042358, 1.0697940587997437, 1.0229623317718506, 1.3951473236083984, 1.7191119194030762, 1.1164203882217407, 1.0324212312698364, 1.0940803289413452, 1.1013495922088623, 1.045095682144165, 1.0150598287582397, 1.2531704902648926, 1.0868743658065796, 1.055431604385376, 1.0258111953735352, 1.1886539459228516, 1.5214723348617554, 1.0211268663406372, 1.138971209526062, 1.0648030042648315, 1.2204538583755493, 1.0707786083221436, 1.212670087814331, 1.0220263004302979, 1.0487463474273682, 1.2225303649902344, 1.1631929874420166, 1.2619047164916992, 1.0085283517837524, 1.095700979232788, 1.0659486055374146, 1.1323603391647339, 1.1992331743240356, 1.0066540241241455, 1.005447268486023, 1.0053791999816895, 1.024379849433899, 1.2421519756317139, 1.031584620475769, 1.3250950574874878, 1.0659220218658447, 1.0271512269973755, 1.406701683998108, 1.0724852085113525, 1.0829946994781494, 1.0234533548355103, 1.2816975116729736, 1.0368062257766724, 1.0802487134933472, 1.0056300163269043, 1.8409091234207153, 1.0866420269012451, 1.0566022396087646, 1.5579286813735962, 1.232185959815979, 1.16291081905365, 1.2302838563919067, 1.0041859149932861, 1.5047202110290527, 1.05100417137146, 1.1017422676086426, 1.042906403541565, 1.1976081132888794, 2.067302942276001, 1.0766544342041016, 1.118644118309021, 1.3991830348968506, 1.0693333148956299, 1.4098073244094849, 1.077068567276001]
********** total_n=[200] batchn=[100]  N=[3200]  nitr=[4]  alpha=[0.4]  beta=[0.2] ******
@ Average per query          [3.7582242488861084] ms
 --  Average per query NF    [1.356419324874878] ms
 --  Average per query vegas [2.4018049240112305] ms
Mean [1.160]  Median [1.108]  95th [1.505]  99th [1.841]  max [2.067]
Mean [1.160]  Median [1.108]  95th [1.505]  99th [1.841]  max [2.067]


WORKLOAD-FINISHED | Type: QueryWorkload | Progress: 3/3 | Workload-time: 12.221370 | Model-update-time: 0.000000


Experiment Summary: #data-update-times=1, #drift=0 | total-time=26.122375