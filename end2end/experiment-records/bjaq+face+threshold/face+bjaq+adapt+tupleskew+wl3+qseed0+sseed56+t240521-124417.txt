Input arguments = Namespace(data_update='tupleskew', update_size=80000, dataset='bjaq', debug=False, drift_test='js', model_update='adapt', model='face', end2end=False, query_seed=0, random_seed=42, num_workload=3)
JSON-passed parameters = {'data_updater': {'skew_ratio': '1e-3'}, 'random_seed': 56, 'face': {'learning_rate': '1e-4', 'epochs': 0, 'factor': 'auto'}}
MODEL-PATH=/home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
DATASET-PATH=/home/kangping/code_copy/SAUCE/data/bjaq/end2end/bjaq.npy
Workloads: ['QueryWorkload', 'DataUpdateWorkload', 'QueryWorkload']

WORKLOAD-START | Type: QueryWorkload | Progress: 1/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/evaluate/Evaluate-FACE-end2end.py', '--dataset=bjaq', '--end2end']
/home/kangping/anaconda3/envs/LndCar/lib/python3.9/site-packages/torchquad/integration/vegas_map.py:314: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/BucketizationUtils.h:33.)
  iy = torch.searchsorted(self.x_edges[d, :], x[:, d], side='right')
DEVICE NAME
 Tesla V100S-PCIE-32GB
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
data shape: (382168, 5)
Save oracle results to :
./FACE/evaluate/oracle/bjaq_rng-1234.csv
Found oracle card!
torch.Size([5, 2])
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
Took  8.192086935043335
tensor(0.9979)
result is  tensor(381348.2188)
Enter testHyper
ReportEsts: [1.1807893514633179, 1.0379666090011597, 1.2679837942123413, 1.0437986850738525, 1.2444379329681396, 1.1341758966445923, 1.1196377277374268, 1.0280507802963257, 1.0373671054840088, 1.0022263526916504, 1.1085271835327148, 1.101271152496338, 1.0083622932434082, 1.0178571939468384, 1.1819813251495361, 1.239018440246582, 1.1777476072311401, 1.144774317741394, 1.1651517152786255, 1.0688775777816772, 1.2122645378112793, 1.0029292106628418, 1.1530721187591553, 1.1190358400344849, 1.1382014751434326, 1.3001807928085327, 1.0464527606964111, 1.4191564321517944, 1.0052177906036377, 1.1958091259002686, 1.1050018072128296, 1.2402431964874268, 1.1455261707305908, 1.1387438774108887, 1.0861884355545044, 1.1974557638168335, 1.4017311334609985, 1.0521204471588135, 1.1201411485671997, 1.0078455209732056, 1.0338265895843506, 1.1502176523208618, 1.0309503078460693, 1.0637657642364502, 1.1109232902526855, 1.0184757709503174, 1.0371229648590088, 1.3055347204208374, 1.0281718969345093, 1.038917064666748, 1.1912144422531128, 1.3619909286499023, 1.1160823106765747, 1.158125877380371, 1.2188383340835571, 1.0377839803695679, 1.1794393062591553, 1.2800252437591553, 1.3769770860671997, 1.4573677778244019, 1.5017101764678955, 1.2031512260437012, 1.3056095838546753, 1.0726617574691772, 1.2741312980651855, 1.2571429014205933, 1.13279128074646, 1.0641223192214966, 1.1113559007644653, 1.1789473295211792, 1.1998069286346436, 1.034592866897583, 1.1223043203353882, 1.019845962524414, 1.0167572498321533, 1.0450682640075684, 1.8237590789794922, 1.0243902206420898, 1.0851689577102661, 1.3812264204025269, 1.3518918752670288, 1.1091907024383545, 1.0220829248428345, 1.205936074256897, 1.2535468339920044, 1.0451102256774902, 1.1749651432037354, 1.0328748226165771, 1.0157713890075684, 1.0755451917648315, 1.0332330465316772, 1.3658381700515747, 1.0183916091918945, 1.222024917602539, 1.133720874786377, 1.0308713912963867, 1.0981379747390747, 1.3295032978057861, 1.034482717514038, 1.1026121377944946, 1.0636364221572876, 1.0171709060668945, 1.2169588804244995, 1.0126019716262817, 1.0117155313491821, 1.0577844381332397, 1.1796202659606934, 1.1849912405014038, 1.108275055885315, 1.037753939628601, 1.121991515159607, 1.0801843404769897, 1.1199696063995361, 1.0432639122009277, 1.1623971462249756, 1.065727710723877, 1.1004798412322998, 1.0367599725723267, 1.2052407264709473, 1.0737574100494385, 1.2854868173599243, 1.109838604927063, 1.116089940071106, 1.1774327754974365, 1.2684108018875122, 1.1645686626434326, 1.11642587184906, 1.292923927307129, 1.3171355724334717, 1.081830382347107, 1.0910849571228027, 1.208552360534668, 1.2740832567214966, 1.4285714626312256, 1.017745018005371, 1.02310311794281, 1.040060043334961, 1.1499186754226685, 1.1232337951660156, 1.0047268867492676, 1.2257952690124512, 1.110388159751892, 1.1716902256011963, 1.3171449899673462, 1.0437864065170288, 1.0092519521713257, 1.1601957082748413, 1.5576910972595215, 1.057064414024353, 1.0926846265792847, 1.1975727081298828, 1.1526727676391602, 1.2875065803527832, 1.1471554040908813, 1.141191005706787, 1.0520718097686768, 1.1501022577285767, 1.0253236293792725, 1.0534918308258057, 1.127364993095398, 1.0358778238296509, 1.1751296520233154, 1.094881534576416, 1.2510013580322266, 1.0227519273757935, 1.0721684694290161, 1.1279847621917725, 1.4359899759292603, 1.0821489095687866, 1.0125099420547485, 1.0268852710723877, 1.1822553873062134, 1.149350643157959, 1.1850303411483765, 1.0592643022537231, 1.113682508468628, 1.0249004364013672, 1.2592995166778564, 1.1964035034179688, 1.1194829940795898, 1.0879796743392944, 1.0128343105316162, 1.012402057647705, 1.1950970888137817, 1.157551646232605, 1.1374207735061646, 1.004374623298645, 1.2768300771713257, 1.0108561515808105, 1.3703644275665283, 1.00420343875885, 1.0500690937042236, 1.6124293804168701, 1.3868391513824463, 1.5520532131195068, 1.0930626392364502, 1.1384743452072144, 1.1692308187484741, 1.0960296392440796, 1.004183292388916]
********** total_n=[200] batchn=[100]  N=[3200]  nitr=[4]  alpha=[0.4]  beta=[0.2] ******
@ Average per query          [3.7334060668945312] ms
 --  Average per query NF    [1.3587641716003418] ms
 --  Average per query vegas [2.3746418952941895] ms
Mean [1.147]  Median [1.120]  95th [1.388]  99th [1.558]  max [1.824]
Mean [1.147]  Median [1.120]  95th [1.388]  99th [1.558]  max [1.824]


WORKLOAD-FINISHED | Type: QueryWorkload | Progress: 1/3 | Workload-time: 11.853578 | Model-update-time: 0.000000


WORKLOAD-START | Type: DataUpdateWorkload | Progress: 2/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/Naru/eval_model.py', '--model=face', '--data_update=tupleskew', '--dataset=bjaq', '--drift_test=js', '--end2end', '--update_size=80000', '--query_seed=0', '--eval_type=drift']
Torch device: cuda:1
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
TupleSkewSample - START
0.001
TupleSkewSample - END
load_npy_dataset_from_path - df.shape = (458601, 5)
Parsing... done, took 0.0s
Data updated!
Previous data size: (382168, 5), new data size: (76433, 5)
[1.7881393e-05 1.2278557e-05 2.2232533e-05 1.7106533e-05 9.1195107e-06]
Distance score: 1.5723704564152285e-05
SAUCE Drift detection: True
Detection latency: 0.0228s

DRIFT-DETECTED after 2-th workload
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/incremental_train.py', '--dataset=bjaq', '--end2end', '--model=face', '--model_update=adapt', '--update_size=80000', '--epochs=0']
Torch device: cuda:1
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
update_size: 76433
AdaptTask - START
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
train data shape: 80000, transfer data shape:16000
Training done; evaluating likelihood on full data:
Saved to: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
AdaptTask - END

WORKLOAD-FINISHED | Type: DataUpdateWorkload | Progress: 2/3 | Workload-time: 2.000368 | Model-update-time: 2.214278


WORKLOAD-START | Type: QueryWorkload | Progress: 3/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/evaluate/Evaluate-FACE-end2end.py', '--dataset=bjaq', '--query_seed=0', '--end2end', '--num_workload=3', '--data_update=tupleskew', '--model_update=adapt']
/home/kangping/anaconda3/envs/LndCar/lib/python3.9/site-packages/torchquad/integration/vegas_map.py:314: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/BucketizationUtils.h:33.)
  iy = torch.searchsorted(self.x_edges[d, :], x[:, d], side='right')
DEVICE NAME
 Tesla V100S-PCIE-32GB
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
data shape: (458601, 5)
Save oracle results to :
./FACE/evaluate/oracle/bjaq_rng-0.csv
Found oracle card!
torch.Size([5, 2])
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
Took  8.17018985748291
tensor(0.9971)
result is  tensor(457283.3438)
Enter testHyper
ReportEsts: [1.1123958826065063, 1.0681711435317993, 1.0481586456298828, 1.1215647459030151, 1.0596250295639038, 1.005742073059082, 1.1501336097717285, 1.2288963794708252, 1.0974231958389282, 1.752212405204773, 1.0128672122955322, 1.2095792293548584, 1.5111111402511597, 1.0674883127212524, 1.015535593032837, 1.0830066204071045, 1.2955516576766968, 1.1381402015686035, 1.0318408012390137, 1.304675817489624, 1.1069499254226685, 1.8051759004592896, 1.5566730499267578, 1.0266976356506348, 1.0426604747772217, 1.3807122707366943, 1.040908932685852, 1.2141424417495728, 1.0563769340515137, 1.269054889678955, 1.1529080867767334, 1.5762711763381958, 1.3577309846878052, 1.0715606212615967, 1.2385348081588745, 1.0952272415161133, 1.0111461877822876, 1.091283917427063, 1.6498905420303345, 1.0934605598449707, 1.1275339126586914, 1.3636363744735718, 1.7639198303222656, 1.1608623266220093, 1.1276519298553467, 1.0819882154464722, 1.1929340362548828, 1.0656952857971191, 1.3571428060531616, 1.021048903465271, 1.0089023113250732, 1.1036012172698975, 1.142533779144287, 1.0334066152572632, 1.0341508388519287, 1.1282179355621338, 1.1030237674713135, 1.2137658596038818, 1.3033866882324219, 1.1357241868972778, 1.2523365020751953, 1.0682311058044434, 1.0858618021011353, 1.1330469846725464, 1.0579811334609985, 1.0420831441879272, 1.4756699800491333, 1.0346113443374634, 1.1931586265563965, 1.0862399339675903, 1.0542337894439697, 1.0814061164855957, 1.084285855293274, 1.0827630758285522, 1.1952749490737915, 1.2204583883285522, 1.2070634365081787, 1.0678273439407349, 1.5042787790298462, 1.0408018827438354, 1.2747294902801514, 1.141717791557312, 1.0526901483535767, 1.1456313133239746, 1.1485567092895508, 1.570212721824646, 1.0395429134368896, 1.7539458274841309, 1.2262623310089111, 1.0346033573150635, 1.1764706373214722, 1.000624179840088, 1.3094794750213623, 1.0509462356567383, 1.0966317653656006, 1.1278024911880493, 1.1338589191436768, 1.2089108228683472, 1.0844926834106445, 1.0342422723770142, 1.0551097393035889, 1.0946640968322754, 2.0479798316955566, 1.1228994131088257, 1.0636118650436401, 1.1967467069625854, 1.0301105976104736, 1.0258876085281372, 1.0372493267059326, 1.196000099182129, 1.6615970134735107, 1.0843983888626099, 1.1492512226104736, 1.414932370185852, 1.2778284549713135, 1.0716012716293335, 1.002605676651001, 1.4654563665390015, 1.1174355745315552, 1.036697268486023, 1.0430107116699219, 1.085329294204712, 1.1320829391479492, 1.133406639099121, 1.0359784364700317, 1.078736662864685, 1.0175285339355469, 1.4240000247955322, 1.0588393211364746, 1.037405252456665, 1.176422119140625, 1.1577056646347046, 1.1126596927642822, 1.547931432723999, 2.1325795650482178, 1.1105374097824097, 1.0292943716049194, 1.3198734521865845, 1.1626505851745605, 1.0054494142532349, 1.1120513677597046, 1.2969938516616821, 1.0889195203781128, 1.2056312561035156, 1.0616354942321777, 1.0559287071228027, 1.3105590343475342, 1.0636367797851562, 1.0302938222885132, 1.080937147140503, 1.084334135055542, 1.095338225364685, 1.2062911987304688, 1.099236011505127, 1.0978776216506958, 1.1072065830230713, 1.2304317951202393, 1.0873016119003296, 1.18238365650177, 1.0549348592758179, 1.0528302192687988, 1.0805127620697021, 1.7310606241226196, 1.0630043745040894, 1.2265154123306274, 1.097383737564087, 1.1122599840164185, 1.0777082443237305, 1.0188907384872437, 1.2355952262878418, 1.0981581211090088, 1.1213210821151733, 1.4190813302993774, 1.0586540699005127, 1.0018354654312134, 1.0322270393371582, 1.0739117860794067, 1.1368526220321655, 1.089606523513794, 1.164465308189392, 1.8421052694320679, 1.0337111949920654, 1.2763911485671997, 1.3711159229278564, 1.4050923585891724, 1.0928853750228882, 1.2841726541519165, 1.0892330408096313, 1.3522872924804688, 1.0987799167633057, 1.135096788406372, 1.0456938743591309, 1.1558945178985596, 1.856178879737854, 1.08165442943573, 1.0348650217056274, 1.5697203874588013, 1.1580477952957153, 1.1014975309371948, 1.0451449155807495]
********** total_n=[200] batchn=[100]  N=[3200]  nitr=[4]  alpha=[0.4]  beta=[0.2] ******
@ Average per query          [3.733888864517212] ms
 --  Average per query NF    [1.357131004333496] ms
 --  Average per query vegas [2.376757860183716] ms
Mean [1.183]  Median [1.107]  95th [1.650]  99th [1.858]  max [2.133]
Mean [1.183]  Median [1.107]  95th [1.650]  99th [1.858]  max [2.133]


WORKLOAD-FINISHED | Type: QueryWorkload | Progress: 3/3 | Workload-time: 12.384392 | Model-update-time: 0.000000


Experiment Summary: #data-update-times=1, #drift=1 | total-time=28.509958