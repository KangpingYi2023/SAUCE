Input arguments = Namespace(data_update='valueskew', update_size=80000, dataset='bjaq', debug=False, drift_test='js', model_update='adapt', model='face', end2end=False, query_seed=0, random_seed=42, num_workload=3)
JSON-passed parameters = {'data_updater': {'skew_ratio': '1e-3'}, 'random_seed': 38, 'face': {'learning_rate': '1e-4', 'epochs': 0, 'factor': 'auto'}}
MODEL-PATH=/home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
DATASET-PATH=/home/kangping/code_copy/SAUCE/data/bjaq/end2end/bjaq.npy
Workloads: ['QueryWorkload', 'DataUpdateWorkload', 'QueryWorkload']

WORKLOAD-START | Type: QueryWorkload | Progress: 1/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/evaluate/Evaluate-FACE-end2end.py', '--dataset=bjaq', '--end2end']
/home/kangping/anaconda3/envs/LndCar/lib/python3.9/site-packages/torchquad/integration/vegas_map.py:314: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/BucketizationUtils.h:33.)
  iy = torch.searchsorted(self.x_edges[d, :], x[:, d], side='right')
DEVICE NAME
 Tesla V100S-PCIE-32GB
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
data shape: (382168, 5)
Save oracle results to :
./FACE/evaluate/oracle/bjaq_rng-1234.csv
Found oracle card!
torch.Size([5, 2])
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
Took  8.16152834892273
tensor(0.9943)
result is  tensor(379987.)
Enter testHyper
ReportEsts: [1.1461971998214722, 1.086456060409546, 1.4683690071105957, 1.0035797357559204, 1.2480915784835815, 1.1623492240905762, 1.141792893409729, 1.2373807430267334, 1.1043452024459839, 1.1449522972106934, 1.0990525484085083, 1.177330493927002, 1.0584174394607544, 1.0558035373687744, 1.1936827898025513, 1.310213327407837, 1.0949455499649048, 1.211163878440857, 1.2050669193267822, 1.0561224222183228, 1.0213756561279297, 1.0181586742401123, 1.1183555126190186, 1.00639009475708, 1.0548884868621826, 1.005424976348877, 1.0610923767089844, 1.3391376733779907, 1.0342801809310913, 1.1940799951553345, 1.1202553510665894, 1.2942795753479004, 1.2043629884719849, 1.0260396003723145, 1.27322518825531, 1.1270978450775146, 1.2961159944534302, 1.0085877180099487, 1.0453660488128662, 1.07856285572052, 1.0334988832473755, 1.0424528121948242, 1.1641008853912354, 1.0214447975158691, 1.0872596502304077, 1.064625859260559, 1.126299262046814, 1.392778754234314, 1.0485469102859497, 1.1353638172149658, 1.1326781511306763, 1.3135910034179688, 1.0582441091537476, 1.0510231256484985, 1.097254991531372, 1.024571180343628, 1.2439252138137817, 1.2329800128936768, 1.061192512512207, 1.3014447689056396, 1.2468736171722412, 1.1675341129302979, 1.3742320537567139, 1.0065138339996338, 1.1956521272659302, 1.0285714864730835, 1.2282075881958008, 1.0243257284164429, 1.236636996269226, 1.378947377204895, 1.0089285373687744, 1.146193504333496, 1.2749121189117432, 1.0134912729263306, 1.1353482007980347, 1.0966085195541382, 1.9133127927780151, 1.0677871704101562, 1.030590295791626, 1.328450322151184, 1.115310788154602, 1.09158456325531, 1.0047847032546997, 1.2492649555206299, 1.2034237384796143, 1.1152368783950806, 1.0447344779968262, 1.2422194480895996, 1.0256364345550537, 1.010728359222412, 1.0842268466949463, 1.3224536180496216, 1.012043833732605, 1.324350357055664, 1.174698829650879, 1.179433822631836, 1.1325347423553467, 1.2015336751937866, 1.034482717514038, 1.1275161504745483, 1.1956204175949097, 1.16694974899292, 1.005670428276062, 1.004392385482788, 1.0171295404434204, 1.0253578424453735, 1.1457488536834717, 1.1972076892852783, 1.0410346984863281, 1.0432380437850952, 1.0738660097122192, 1.0912476778030396, 1.212327480316162, 1.0405975580215454, 1.0343109369277954, 1.0558139085769653, 1.0960713624954224, 1.0266993045806885, 1.2288798093795776, 1.0060378313064575, 1.2740726470947266, 1.142265796661377, 1.0293391942977905, 1.1588037014007568, 1.1865310668945312, 1.255365252494812, 1.1433137655258179, 1.1159332990646362, 1.2864279747009277, 1.1189044713974, 1.0, 1.2632590532302856, 1.2044748067855835, 1.4642857313156128, 1.2278872728347778, 1.1564714908599854, 1.0359848737716675, 1.1577235460281372, 1.0409518480300903, 1.2470664978027344, 1.1580467224121094, 1.1121870279312134, 1.0838640928268433, 1.0482548475265503, 1.0285236835479736, 1.084319829940796, 1.1007598638534546, 1.2662348747253418, 1.0600112676620483, 1.0524858236312866, 1.5525466203689575, 1.1654915809631348, 1.317816972732544, 1.1284278631210327, 1.078094482421875, 1.0280170440673828, 1.1378822326660156, 1.0116709470748901, 1.033855676651001, 1.1635929346084595, 1.0300002098083496, 1.0308846235275269, 1.042022705078125, 1.3328591585159302, 1.2519893646240234, 1.0745404958724976, 1.1163945198059082, 1.2522073984146118, 1.052272081375122, 1.0398643016815186, 1.064024567604065, 1.1587029695510864, 1.1363636255264282, 1.075182557106018, 1.0619890689849854, 1.0082184076309204, 1.0092322826385498, 1.2495143413543701, 1.0372745990753174, 1.1613858938217163, 1.016288161277771, 1.130129098892212, 1.0325249433517456, 1.0643513202667236, 1.0901846885681152, 1.167019009590149, 1.046787142753601, 1.2607017755508423, 1.0258270502090454, 1.3719196319580078, 1.0404975414276123, 1.023911476135254, 1.3283686637878418, 1.2774536609649658, 1.7898063659667969, 1.1447135210037231, 1.0684034824371338, 1.1343283653259277, 1.0920445919036865, 1.0349270105361938]
********** total_n=[200] batchn=[100]  N=[3200]  nitr=[4]  alpha=[0.4]  beta=[0.2] ******
@ Average per query          [3.770883083343506] ms
 --  Average per query NF    [1.3671350479125977] ms
 --  Average per query vegas [2.403748035430908] ms
Mean [1.140]  Median [1.115]  95th [1.333]  99th [1.555]  max [1.913]
Mean [1.140]  Median [1.115]  95th [1.333]  99th [1.555]  max [1.913]


WORKLOAD-FINISHED | Type: QueryWorkload | Progress: 1/3 | Workload-time: 11.817976 | Model-update-time: 0.000000


WORKLOAD-START | Type: DataUpdateWorkload | Progress: 2/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/Naru/eval_model.py', '--model=face', '--data_update=valueskew', '--dataset=bjaq', '--drift_test=js', '--end2end', '--update_size=80000', '--query_seed=0', '--eval_type=drift']
Torch device: cuda:1
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
ValueSample - START
ValueSample - END
load_npy_dataset_from_path - df.shape = (458601, 5)
Parsing... done, took 0.0s
Data updated!
Previous data size: (382168, 5), new data size: (76433, 5)
[0.0000000e+00 1.7881393e-07 5.3644180e-07 8.8431835e-03 2.9802322e-07]
Distance score: 0.0017688393127173185
SAUCE Drift detection: True
Detection latency: 0.0233s

DRIFT-DETECTED after 2-th workload
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/incremental_train.py', '--dataset=bjaq', '--end2end', '--model=face', '--model_update=adapt', '--update_size=80000', '--epochs=0']
Torch device: cuda:1
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
update_size: 76433
AdaptTask - START
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
train data shape: 80000, transfer data shape:16000
Training done; evaluating likelihood on full data:
Saved to: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
AdaptTask - END

WORKLOAD-FINISHED | Type: DataUpdateWorkload | Progress: 2/3 | Workload-time: 2.052461 | Model-update-time: 2.224742


WORKLOAD-START | Type: QueryWorkload | Progress: 3/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/evaluate/Evaluate-FACE-end2end.py', '--dataset=bjaq', '--query_seed=0', '--end2end', '--num_workload=3', '--data_update=valueskew', '--model_update=adapt']
/home/kangping/code_copy/SAUCE/./FACE/FACE_utils/dataUtils.py:321: RuntimeWarning: invalid value encountered in cast
  oracle_cards = oracle_cards.astype(np.int32)
/home/kangping/anaconda3/envs/LndCar/lib/python3.9/site-packages/torchquad/integration/vegas_map.py:314: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/BucketizationUtils.h:33.)
  iy = torch.searchsorted(self.x_edges[d, :], x[:, d], side='right')
DEVICE NAME
 Tesla V100S-PCIE-32GB
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
data shape: (458601, 5)
Save oracle results to :
./FACE/evaluate/oracle/bjaq_rng-0.csv
Found oracle card!
torch.Size([5, 2])
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
Took  8.200727939605713
tensor(0.9966)
result is  tensor(457061.6562)
Enter testHyper
ReportEsts: [1.115868330001831, 1.0526522397994995, 1.029745101928711, 1.180735468864441, 1.0667601823806763, 1.2978116273880005, 1.0993019342422485, 1.057383418083191, 1.190277099609375, 1.2329918146133423, 1.0491050481796265, 1.1298139095306396, 1.007159948348999, 1.2806962728500366, 1.0525439977645874, 1.1500554084777832, 1.8628979921340942, 1.03360915184021, 1.1923102140426636, 1.2290512323379517, 1.2452375888824463, 1.3629764318466187, 2.289982795715332, 1.7274092435836792, 1.04447603225708, 1.2172739505767822, 1.0266445875167847, 1.057276964187622, 1.372642159461975, 1.2374203205108643, 1.5279719829559326, 1.6864407062530518, 1.0130873918533325, 1.1356966495513916, 1.0861124992370605, 1.042527675628662, 1.1044598817825317, 1.1240770816802979, 1.0055440664291382, 1.0987579822540283, 1.3279393911361694, 1.0602798461914062, 22.1124324798584, 1.0223588943481445, 1.0009300708770752, 1.1030094623565674, 1.07905912399292, 1.3760343790054321, 1.3190475702285767, 1.2768758535385132, 1.0476086139678955, 1.1878424882888794, 1.0445365905761719, 1.0065298080444336, 1.0251562595367432, 1.1974053382873535, 1.0368432998657227, 1.1121695041656494, 1.0365970134735107, 1.0961436033248901, 1.4408602714538574, 1.055031657218933, 1.011793613433838, 1.7082289457321167, 1.0249083042144775, 1.0457097291946411, 1.0357930660247803, 1.1552815437316895, 1.026304841041565, 1.0779024362564087, 1.1546576023101807, 1.0123145580291748, 1.093627691268921, 1.1595096588134766, 1.2453209161758423, 1.1070854663848877, 1.0379678010940552, 3.56862735748291, 1.0889403820037842, 1.0368808507919312, 3.349726676940918, 1.2022751569747925, 1.0425841808319092, 1.0555163621902466, 1.070988416671753, 1.3546099662780762, 1.1836682558059692, 1.1243243217468262, 1.2127752304077148, 1.1552314758300781, 1.0921162366867065, 1.5146235227584839, 1.063988208770752, 1.003166675567627, 30.735837936401367, 1.0009080171585083, 1.080639123916626, 1.2523908615112305, 1.1693785190582275, 1.0495131015777588, 1.1657865047454834, 1.0878582000732422, 1.0857843160629272, 1.0683555603027344, 1.3841859102249146, 1.9161747694015503, 1.2251110076904297, 1.077087163925171, 1.0143266916275024, 1.0169247388839722, 1.7810379266738892, 1.1126124858856201, 1.0294017791748047, 1.1389062404632568, 1.0874525308609009, 3.8863003253936768, 1.0552369356155396, 1.2109893560409546, 2.278008222579956, 1.194690227508545, 1.101729393005371, 1.0008070468902588, 1.0134960412979126, 1.064679741859436, 1.1175258159637451, 1.361398458480835, 1.2713568210601807, 1.3637770414352417, 1.203575849533081, 1.1154422760009766, 1.1689684391021729, 1.1441508531570435, 1.032723307609558, 1.6930586099624634, 1.2675213813781738, 1.056955099105835, 1.254654884338379, 6.805483818054199, 1.0711195468902588, 1.0122863054275513, 1.1572414636611938, 1.1287953853607178, 1.0088993310928345, 1.1337311267852783, 1.0432132482528687, 6.505915641784668, 1.1722222566604614, 1.058264970779419, 1.1590458154678345, 1.0300074815750122, 26.273584365844727, 1.1677346229553223, 1.4071558713912964, 1.0217475891113281, 1.0093767642974854, 1.1300551891326904, 1.0865520238876343, 1.0458015203475952, 3.3865180015563965, 1.0976009368896484, 1.112331509590149, 1.082218885421753, 1.2421308755874634, 1.3160459995269775, 1.0616834163665771, 1.0175189971923828, 1.0182435512542725, 1.320297360420227, 1.0166987180709839, 1.1635693311691284, 1.135023593902588, 1.0498071908950806, 1.3366645574569702, 1.085407018661499, 2.0518527030944824, 1.0155433416366577, 1.1825238466262817, 1.0219464302062988, 1.2743369340896606, 1.0148800611495972, 1.8684210777282715, 1.6728986501693726, 1.0467054843902588, 1.5475239753723145, 1.3411602973937988, 1.2002110481262207, 1.4240390062332153, 1.1300067901611328, 1.3967864513397217, 1.015249252319336, 1.0720453262329102, 1.090506672859192, 1.1491353511810303, 1.483146071434021, 1.1692768335342407, 1.2376779317855835, 1.1015348434448242, 1.019088625907898, 1.112605094909668, 1.304250717163086]
********** total_n=[200] batchn=[100]  N=[3200]  nitr=[4]  alpha=[0.4]  beta=[0.2] ******
@ Average per query          [3.73049259185791] ms
 --  Average per query NF    [1.3660573959350586] ms
 --  Average per query vegas [2.3644351959228516] ms
Mean [1.664]  Median [1.117]  95th [2.279]  99th [22.154]  max [30.736]
Mean [1.664]  Median [1.117]  95th [2.279]  99th [22.154]  max [30.736]


WORKLOAD-FINISHED | Type: QueryWorkload | Progress: 3/3 | Workload-time: 12.414247 | Model-update-time: 0.000000


Experiment Summary: #data-update-times=1, #drift=1 | total-time=28.575123