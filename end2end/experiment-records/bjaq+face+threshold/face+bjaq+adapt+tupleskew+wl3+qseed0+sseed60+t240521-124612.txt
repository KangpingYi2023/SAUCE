Input arguments = Namespace(data_update='tupleskew', update_size=80000, dataset='bjaq', debug=False, drift_test='js', model_update='adapt', model='face', end2end=False, query_seed=0, random_seed=42, num_workload=3)
JSON-passed parameters = {'data_updater': {'skew_ratio': '1e-3'}, 'random_seed': 60, 'face': {'learning_rate': '1e-4', 'epochs': 0, 'factor': 'auto'}}
MODEL-PATH=/home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
DATASET-PATH=/home/kangping/code_copy/SAUCE/data/bjaq/end2end/bjaq.npy
Workloads: ['QueryWorkload', 'DataUpdateWorkload', 'QueryWorkload']

WORKLOAD-START | Type: QueryWorkload | Progress: 1/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/evaluate/Evaluate-FACE-end2end.py', '--dataset=bjaq', '--end2end']
/home/kangping/anaconda3/envs/LndCar/lib/python3.9/site-packages/torchquad/integration/vegas_map.py:314: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/BucketizationUtils.h:33.)
  iy = torch.searchsorted(self.x_edges[d, :], x[:, d], side='right')
DEVICE NAME
 Tesla V100S-PCIE-32GB
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
data shape: (382168, 5)
Save oracle results to :
./FACE/evaluate/oracle/bjaq_rng-1234.csv
Found oracle card!
torch.Size([5, 2])
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
Took  8.190478324890137
tensor(0.9953)
result is  tensor(380359.7500)
Enter testHyper
ReportEsts: [1.1652348041534424, 1.0774399042129517, 1.3810560703277588, 1.0103179216384888, 1.2408056259155273, 1.1650364398956299, 1.2112222909927368, 1.1170694828033447, 1.0693333148956299, 1.0401564836502075, 1.0947458744049072, 1.1349576711654663, 1.0894525051116943, 1.0558035373687744, 1.0930516719818115, 1.2431645393371582, 1.3469314575195312, 1.1631828546524048, 1.2029584646224976, 1.0433673858642578, 1.0596319437026978, 1.0958808660507202, 1.1415854692459106, 1.0316373109817505, 1.068447232246399, 1.0849909782409668, 1.1663851737976074, 1.2646828889846802, 1.0497797727584839, 1.0248194932937622, 1.2685348987579346, 1.0612353086471558, 1.0373865365982056, 1.0381752252578735, 1.1106901168823242, 1.088255763053894, 1.4068050384521484, 1.015019416809082, 1.0984865427017212, 1.0114970207214355, 1.1055011749267578, 1.1434446573257446, 1.0538982152938843, 1.0945734977722168, 1.1172617673873901, 1.037647008895874, 1.0662751197814941, 1.2648550271987915, 1.1558200120925903, 1.1556683778762817, 1.3171428442001343, 1.4270234107971191, 1.0700017213821411, 1.1571093797683716, 1.151213526725769, 1.0280481576919556, 1.327102780342102, 1.2815660238265991, 1.2109031677246094, 1.391068458557129, 1.0474635362625122, 1.198864221572876, 1.459273338317871, 1.248431921005249, 1.3043478727340698, 1.2000000476837158, 1.2342519760131836, 1.0469549894332886, 1.063520073890686, 1.263157844543457, 1.037007212638855, 1.0031787157058716, 1.305126667022705, 1.0815355777740479, 1.011620283126831, 1.013553500175476, 1.9364149570465088, 1.0975887775421143, 1.0512772798538208, 1.3918814659118652, 1.2044321298599243, 1.0382813215255737, 1.0825510025024414, 1.2137600183486938, 1.1770960092544556, 1.0785146951675415, 1.0320340394973755, 1.1613115072250366, 1.0661312341690063, 1.0103657245635986, 1.0664860010147095, 1.2707475423812866, 1.022115707397461, 1.3969542980194092, 1.0153846740722656, 1.1624244451522827, 1.1155251264572144, 1.1605138778686523, 1.01694917678833, 1.2337795495986938, 1.3231017589569092, 1.096903681755066, 1.0936927795410156, 1.010354995727539, 1.0654184818267822, 1.1165779829025269, 1.1728914976119995, 1.1902269124984741, 1.1066045761108398, 1.0029933452606201, 1.132565975189209, 1.0781968832015991, 1.2038171291351318, 1.026255488395691, 1.0842703580856323, 1.1127450466156006, 1.0423043966293335, 1.0239907503128052, 1.4461852312088013, 1.2148964405059814, 1.1865123510360718, 1.0892783403396606, 1.1993732452392578, 1.1829556226730347, 1.301356554031372, 1.1936649084091187, 1.1142421960830688, 1.147391438484192, 1.248989462852478, 1.1192384958267212, 1.2465271949768066, 1.2347356081008911, 1.3321938514709473, 1.4642857313156128, 1.179420828819275, 1.1525335311889648, 1.039408564567566, 1.0946341753005981, 1.0902535915374756, 1.2381876707077026, 1.131813406944275, 1.1881760358810425, 1.2718713283538818, 1.369234323501587, 1.0021436214447021, 1.054255723953247, 1.0076680183410645, 1.5214219093322754, 1.062389850616455, 1.1272016763687134, 1.048039197921753, 1.1695936918258667, 1.2617827653884888, 1.1363465785980225, 1.1878597736358643, 1.0784035921096802, 1.4207793474197388, 1.0168812274932861, 1.0950425863265991, 1.1564905643463135, 1.113415002822876, 1.0348469018936157, 1.1336445808410645, 1.2977839708328247, 1.0913294553756714, 1.021222710609436, 1.0580120086669922, 1.398008942604065, 1.11857271194458, 1.0262646675109863, 1.0401455163955688, 1.0947800874710083, 1.1428571939468384, 1.092961072921753, 1.0667574405670166, 1.0886833667755127, 1.0048590898513794, 1.2568389177322388, 1.1470787525177002, 1.0034642219543457, 1.0808062553405762, 1.0584262609481812, 1.0225660800933838, 1.0316649675369263, 1.1224193572998047, 1.0729386806488037, 1.016017198562622, 1.1624070405960083, 1.0021538734436035, 1.4152079820632935, 1.0271127223968506, 1.0799235105514526, 1.5864368677139282, 1.6396484375, 1.744704246520996, 1.1460673809051514, 1.0131304264068604, 1.1875, 1.090895175933838, 1.1332005262374878]
********** total_n=[200] batchn=[100]  N=[3200]  nitr=[4]  alpha=[0.4]  beta=[0.2] ******
@ Average per query          [3.749229907989502] ms
 --  Average per query NF    [1.3568055629730225] ms
 --  Average per query vegas [2.3924243450164795] ms
Mean [1.154]  Median [1.115]  95th [1.415]  99th [1.641]  max [1.936]
Mean [1.154]  Median [1.115]  95th [1.415]  99th [1.641]  max [1.936]


WORKLOAD-FINISHED | Type: QueryWorkload | Progress: 1/3 | Workload-time: 11.855027 | Model-update-time: 0.000000


WORKLOAD-START | Type: DataUpdateWorkload | Progress: 2/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/Naru/eval_model.py', '--model=face', '--data_update=tupleskew', '--dataset=bjaq', '--drift_test=js', '--end2end', '--update_size=80000', '--query_seed=0', '--eval_type=drift']
Torch device: cuda:1
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
TupleSkewSample - START
0.001
TupleSkewSample - END
load_npy_dataset_from_path - df.shape = (458601, 5)
Parsing... done, took 0.0s
Data updated!
Previous data size: (382168, 5), new data size: (76433, 5)
[2.4431944e-04 3.2687187e-04 4.1127205e-06 6.6101551e-05 2.3841858e-07]
Distance score: 0.00012832880020141602
SAUCE Drift detection: True
Detection latency: 0.0230s

DRIFT-DETECTED after 2-th workload
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/incremental_train.py', '--dataset=bjaq', '--end2end', '--model=face', '--model_update=adapt', '--update_size=80000', '--epochs=0']
Torch device: cuda:1
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
update_size: 76433
AdaptTask - START
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
train data shape: 80000, transfer data shape:16000
Training done; evaluating likelihood on full data:
Saved to: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
AdaptTask - END

WORKLOAD-FINISHED | Type: DataUpdateWorkload | Progress: 2/3 | Workload-time: 2.000456 | Model-update-time: 2.236732


WORKLOAD-START | Type: QueryWorkload | Progress: 3/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/evaluate/Evaluate-FACE-end2end.py', '--dataset=bjaq', '--query_seed=0', '--end2end', '--num_workload=3', '--data_update=tupleskew', '--model_update=adapt']
/home/kangping/code_copy/SAUCE/./FACE/FACE_utils/dataUtils.py:321: RuntimeWarning: invalid value encountered in cast
  oracle_cards = oracle_cards.astype(np.int32)
/home/kangping/anaconda3/envs/LndCar/lib/python3.9/site-packages/torchquad/integration/vegas_map.py:314: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/BucketizationUtils.h:33.)
  iy = torch.searchsorted(self.x_edges[d, :], x[:, d], side='right')
DEVICE NAME
 Tesla V100S-PCIE-32GB
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
data shape: (458601, 5)
Save oracle results to :
./FACE/evaluate/oracle/bjaq_rng-0.csv
Found oracle card!
torch.Size([5, 2])
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
Took  8.17029070854187
tensor(0.9946)
result is  tensor(456139.4375)
Enter testHyper
ReportEsts: [1.2254127264022827, 1.0112603902816772, 1.0521609783172607, 1.241209626197815, 1.0048385858535767, 1.052742838859558, 1.1941447257995605, 1.0421110391616821, 1.6822742223739624, 1.0075206756591797, 1.046825885772705, 1.0319651365280151, 1.0397021770477295, 1.0841554403305054, 1.122158169746399, 1.0323442220687866, 1.1838933229446411, 1.066872239112854, 1.0013673305511475, 1.0795977115631104, 1.0413403511047363, 1.0747302770614624, 1.0885484218597412, 1.0550343990325928, 1.1321994066238403, 1.0574434995651245, 1.0394526720046997, 1.2990443706512451, 1.0229254961013794, 1.6139576435089111, 1.3618881702423096, 1.4322034120559692, 1.0621458292007446, 1.0208086967468262, 1.0665756464004517, 1.0174938440322876, 1.0159366130828857, 1.1097899675369263, 1.2130218744277954, 1.0730650424957275, 1.0082638263702393, 1.1498216390609741, 1.4316084384918213, 1.0064640045166016, 1.0440516471862793, 1.058719277381897, 1.06954026222229, 1.049169659614563, 1.4136691093444824, 1.1086028814315796, 1.03644859790802, 1.133806824684143, 1.1897050142288208, 1.0820162296295166, 1.04856276512146, 1.064087986946106, 1.2955410480499268, 1.067670464515686, 1.1691992282867432, 1.293036937713623, 1.5283018350601196, 1.014760971069336, 1.02617347240448, 1.10256028175354, 1.0579105615615845, 1.1479387283325195, 1.2260397672653198, 1.1335649490356445, 1.1929073333740234, 1.0858988761901855, 1.000745415687561, 1.1014267206192017, 1.1011836528778076, 1.153003454208374, 1.0228670835494995, 1.1723029613494873, 1.2922252416610718, 1.1255520582199097, 1.0033687353134155, 1.061699390411377, 1.002087116241455, 1.1019976139068604, 1.0269495248794556, 1.1519168615341187, 1.2928993701934814, 1.0621761083602905, 1.094068169593811, 1.5869492292404175, 1.079071283340454, 1.0357357263565063, 1.124197006225586, 1.1096588373184204, 1.3672688007354736, 1.1639212369918823, 1.1471964120864868, 1.0672296285629272, 1.2657501697540283, 1.2798173427581787, 1.0514074563980103, 1.119639277458191, 1.0719504356384277, 1.0627893209457397, 1.031862735748291, 1.0682027339935303, 1.1442439556121826, 1.1663293838500977, 1.0341023206710815, 1.0142908096313477, 1.6776119470596313, 1.014215350151062, 1.1423155069351196, 1.0387099981307983, 1.2961212396621704, 1.242053508758545, 1.3000915050506592, 1.055345892906189, 1.1044414043426514, 1.3084466457366943, 1.0786103010177612, 1.52212393283844, 1.090908408164978, 1.330841064453125, 1.033307433128357, 1.3890379667282104, 1.0861293077468872, 1.097922682762146, 1.2951982021331787, 1.0074738264083862, 1.1503270864486694, 1.1342002153396606, 1.2383995056152344, 1.0902050733566284, 1.0826725959777832, 2.1081080436706543, 1.4110370874404907, 1.0056966543197632, 1.1476606130599976, 1.1025927066802979, 1.0484657287597656, 1.0016238689422607, 1.0657985210418701, 1.1341552734375, 1.054939866065979, 1.0184669494628906, 1.1074323654174805, 1.152822494506836, 1.2126436233520508, 1.0507808923721313, 1.1351081132888794, 1.00009286403656, 1.0464274883270264, 1.1465744972229004, 1.051802635192871, 1.0197569131851196, 1.0556366443634033, 1.111608862876892, 1.0941271781921387, 1.1048387289047241, 1.0748134851455688, 1.1698635816574097, 1.121266484260559, 1.3741722106933594, 1.0152556896209717, 1.0344970226287842, 1.2532496452331543, 1.0826256275177002, 1.0290583372116089, 1.4359955787658691, 1.040727138519287, 1.326133370399475, 1.1031118631362915, 1.0514131784439087, 1.4884757995605469, 1.0163387060165405, 1.1108392477035522, 1.0049480199813843, 1.349888801574707, 1.0254077911376953, 1.0325250625610352, 1.0684888362884521, 1.8421052694320679, 1.1540404558181763, 1.0525929927825928, 1.3921432495117188, 1.3320682048797607, 2.4882593154907227, 1.4186947345733643, 1.0504539012908936, 1.3878309726715088, 1.0713566541671753, 1.1402684450149536, 1.144741415977478, 1.2068244218826294, 1.9013283252716064, 1.0395971536636353, 2.3136730194091797, 1.0507709980010986, 1.1415929794311523, 1.087027907371521, 1.2117642164230347]
********** total_n=[200] batchn=[100]  N=[3200]  nitr=[4]  alpha=[0.4]  beta=[0.2] ******
@ Average per query          [3.6421620845794678] ms
 --  Average per query NF    [1.3553249835968018] ms
 --  Average per query vegas [2.286837100982666] ms
Mean [1.164]  Median [1.094]  95th [1.522]  99th [2.110]  max [2.488]
Mean [1.164]  Median [1.094]  95th [1.522]  99th [2.110]  max [2.488]


WORKLOAD-FINISHED | Type: QueryWorkload | Progress: 3/3 | Workload-time: 12.322981 | Model-update-time: 0.000000


Experiment Summary: #data-update-times=1, #drift=1 | total-time=28.473242