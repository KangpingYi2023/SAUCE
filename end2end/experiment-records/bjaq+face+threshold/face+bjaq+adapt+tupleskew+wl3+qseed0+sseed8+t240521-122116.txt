Input arguments = Namespace(data_update='tupleskew', update_size=80000, dataset='bjaq', debug=False, drift_test='js', model_update='adapt', model='face', end2end=False, query_seed=0, random_seed=42, num_workload=3)
JSON-passed parameters = {'data_updater': {'skew_ratio': '1e-3'}, 'random_seed': 8, 'face': {'learning_rate': '1e-4', 'epochs': 0, 'factor': 'auto'}}
MODEL-PATH=/home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
DATASET-PATH=/home/kangping/code_copy/SAUCE/data/bjaq/end2end/bjaq.npy
Workloads: ['QueryWorkload', 'DataUpdateWorkload', 'QueryWorkload']

WORKLOAD-START | Type: QueryWorkload | Progress: 1/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/evaluate/Evaluate-FACE-end2end.py', '--dataset=bjaq', '--end2end']
/home/kangping/anaconda3/envs/LndCar/lib/python3.9/site-packages/torchquad/integration/vegas_map.py:314: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/BucketizationUtils.h:33.)
  iy = torch.searchsorted(self.x_edges[d, :], x[:, d], side='right')
DEVICE NAME
 Tesla V100S-PCIE-32GB
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
data shape: (382168, 5)
Save oracle results to :
./FACE/evaluate/oracle/bjaq_rng-1234.csv
Found oracle card!
torch.Size([5, 2])
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
Took  8.195900440216064
tensor(0.9932)
result is  tensor(379555.8750)
Enter testHyper
ReportEsts: [1.0626795291900635, 1.0699058771133423, 1.3868564367294312, 1.053695559501648, 1.2651785612106323, 1.1194463968276978, 1.2818684577941895, 1.0356141328811646, 1.0893449783325195, 1.1494548320770264, 1.116279125213623, 1.1027542352676392, 1.1069248914718628, 1.0334821939468384, 1.1583267450332642, 1.1793475151062012, 1.357771635055542, 1.1235154867172241, 1.093867540359497, 1.1811224222183228, 1.026286005973816, 1.1069722175598145, 1.136286973953247, 1.0934202671051025, 1.0640366077423096, 1.1166365146636963, 1.0667229890823364, 1.344712734222412, 1.0495671033859253, 1.1540026664733887, 1.079460859298706, 1.1520559787750244, 1.0550713539123535, 1.0033148527145386, 1.1357272863388062, 1.0983867645263672, 1.2065982818603516, 1.061368703842163, 1.0000665187835693, 1.100856900215149, 1.0094459056854248, 1.2320996522903442, 1.0664867162704468, 1.0777621269226074, 1.0910627841949463, 1.0450236797332764, 1.0761359930038452, 1.2134166955947876, 1.0408034324645996, 1.1032148599624634, 1.2036553621292114, 1.306666612625122, 1.1232887506484985, 1.1594840288162231, 1.0839420557022095, 1.0419564247131348, 1.314018726348877, 1.1245566606521606, 1.2161742448806763, 1.396898627281189, 1.318893313407898, 1.1751800775527954, 1.3864164352416992, 1.0878113508224487, 1.2941176891326904, 1.085714340209961, 1.101933240890503, 1.0487617254257202, 1.0866767168045044, 1.3157894611358643, 1.0032180547714233, 1.0023596286773682, 1.3070120811462402, 1.0221800804138184, 1.0508003234863281, 1.240019679069519, 1.7386469841003418, 1.013319492340088, 1.1232413053512573, 1.0585248470306396, 1.269534945487976, 1.0948150157928467, 1.031773328781128, 1.150429129600525, 1.1840423345565796, 1.0554840564727783, 1.1118723154067993, 1.315208077430725, 1.011399507522583, 1.007131576538086, 1.0633578300476074, 1.357617735862732, 1.0018031597137451, 1.3008744716644287, 1.0540540218353271, 1.0531948804855347, 1.2422137260437012, 1.2722121477127075, 1.0166666507720947, 1.183877944946289, 1.1112618446350098, 1.1185911893844604, 1.0098577737808228, 1.0014641284942627, 1.1531380414962769, 1.0198726654052734, 1.1892274618148804, 1.1553229093551636, 1.0135542154312134, 1.0470106601715088, 1.217442274093628, 1.0732600688934326, 1.1969423294067383, 1.0259432792663574, 1.3440362215042114, 1.213903784751892, 1.0138463973999023, 1.0241197347640991, 1.3389644622802734, 1.0516611337661743, 1.1834677457809448, 1.0689250230789185, 1.0886844396591187, 1.074877381324768, 1.2146317958831787, 1.1953157186508179, 1.2395036220550537, 1.104346513748169, 1.1504095792770386, 1.081830382347107, 1.2387316226959229, 1.305823802947998, 1.2072715759277344, 1.4642857313156128, 1.0699948072433472, 1.038330316543579, 1.012728214263916, 1.1785365343093872, 1.1504043340682983, 1.3174930810928345, 1.1136940717697144, 1.1052285432815552, 1.203791618347168, 1.2577290534973145, 1.1494965553283691, 1.0457425117492676, 1.082301139831543, 1.4060890674591064, 1.021588683128357, 1.079687476158142, 1.12230384349823, 1.1426739692687988, 1.3429661989212036, 1.1932307481765747, 1.0846203565597534, 1.0415395498275757, 1.0169402360916138, 1.000844120979309, 1.019934058189392, 1.141510009765625, 1.2385879755020142, 1.0240315198898315, 1.1261563301086426, 1.2543507814407349, 1.169764518737793, 1.1114240884780884, 1.1629995107650757, 1.314852237701416, 1.0254502296447754, 1.09735107421875, 1.3680591583251953, 1.0845493078231812, 1.1688311100006104, 1.134481430053711, 1.0725476741790771, 1.1717272996902466, 1.0461612939834595, 1.1561813354492188, 1.1835265159606934, 1.1055593490600586, 1.041293978691101, 1.0140422582626343, 1.0274438858032227, 1.0715014934539795, 1.1133646965026855, 1.0803382396697998, 1.0301538705825806, 1.2254433631896973, 1.0021859407424927, 1.3793740272521973, 1.0209146738052368, 1.0248796939849854, 1.6454309225082397, 1.2836391925811768, 1.718233585357666, 1.2391303777694702, 1.09303879737854, 1.0555555820465088, 1.0147678852081299, 1.1478419303894043]
********** total_n=[200] batchn=[100]  N=[3200]  nitr=[4]  alpha=[0.4]  beta=[0.2] ******
@ Average per query          [3.7713372707366943] ms
 --  Average per query NF    [1.3665986061096191] ms
 --  Average per query vegas [2.404738664627075] ms
Mean [1.144]  Median [1.109]  95th [1.358]  99th [1.646]  max [1.739]
Mean [1.144]  Median [1.109]  95th [1.358]  99th [1.646]  max [1.739]


WORKLOAD-FINISHED | Type: QueryWorkload | Progress: 1/3 | Workload-time: 11.864690 | Model-update-time: 0.000000


WORKLOAD-START | Type: DataUpdateWorkload | Progress: 2/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/Naru/eval_model.py', '--model=face', '--data_update=tupleskew', '--dataset=bjaq', '--drift_test=js', '--end2end', '--update_size=80000', '--query_seed=0', '--eval_type=drift']
Torch device: cuda:1
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
TupleSkewSample - START
0.001
TupleSkewSample - END
load_npy_dataset_from_path - df.shape = (458601, 5)
Parsing... done, took 0.0s
Data updated!
Previous data size: (382168, 5), new data size: (76433, 5)
[1.4603138e-05 8.7618828e-06 5.2630901e-05 3.3438206e-05 5.8770180e-05]
Distance score: 3.364086296642199e-05
SAUCE Drift detection: True
Detection latency: 0.0229s

DRIFT-DETECTED after 2-th workload
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/incremental_train.py', '--dataset=bjaq', '--end2end', '--model=face', '--model_update=adapt', '--update_size=80000', '--epochs=0']
Torch device: cuda:1
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
update_size: 76433
AdaptTask - START
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
train data shape: 80000, transfer data shape:16000
Training done; evaluating likelihood on full data:
Saved to: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
AdaptTask - END

WORKLOAD-FINISHED | Type: DataUpdateWorkload | Progress: 2/3 | Workload-time: 1.999124 | Model-update-time: 2.203701


WORKLOAD-START | Type: QueryWorkload | Progress: 3/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/evaluate/Evaluate-FACE-end2end.py', '--dataset=bjaq', '--query_seed=0', '--end2end', '--num_workload=3', '--data_update=tupleskew', '--model_update=adapt']
/home/kangping/anaconda3/envs/LndCar/lib/python3.9/site-packages/torchquad/integration/vegas_map.py:314: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/BucketizationUtils.h:33.)
  iy = torch.searchsorted(self.x_edges[d, :], x[:, d], side='right')
DEVICE NAME
 Tesla V100S-PCIE-32GB
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
data shape: (458601, 5)
Save oracle results to :
./FACE/evaluate/oracle/bjaq_rng-0.csv
Found oracle card!
torch.Size([5, 2])
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
Took  8.171794176101685
tensor(0.9929)
result is  tensor(455362.2812)
Enter testHyper
ReportEsts: [1.1021273136138916, 1.0066914558410645, 1.0509915351867676, 1.0133790969848633, 1.0088794231414795, 1.3105815649032593, 1.1031229496002197, 1.0999325513839722, 8.954545021057129, 1.0331642627716064, 1.0450981855392456, 1.0018376111984253, 1.0194647312164307, 1.0068316459655762, 1.1248719692230225, 1.0623921155929565, 1.0842121839523315, 1.1179277896881104, 1.038684368133545, 1.3538715839385986, 1.2092169523239136, 1.0732804536819458, 1.1118006706237793, 1.0820015668869019, 1.0036624670028687, 1.1780585050582886, 1.0516068935394287, 1.1431102752685547, 1.5142894983291626, 1.2507575750350952, 1.1065573692321777, 1.4576271772384644, 1.036847472190857, 1.4990240335464478, 1.0741655826568604, 1.1255688667297363, 1.214253306388855, 1.0816118717193604, 1.340588927268982, 1.070702075958252, 1.0503041744232178, 1.0307528972625732, 1.030652403831482, 1.0508308410644531, 1.0511130094528198, 1.2192018032073975, 1.361102819442749, 1.0538005828857422, 1.3714286088943481, 1.1092411279678345, 1.0321929454803467, 1.0009126663208008, 1.0208805799484253, 1.0092658996582031, 1.079858422279358, 1.1404951810836792, 1.2703328132629395, 1.004368543624878, 1.2327277660369873, 1.1102988719940186, 1.2017936706542969, 1.0843143463134766, 1.2926828861236572, 1.438261866569519, 1.1033248901367188, 1.0386974811553955, 1.2948724031448364, 1.0880365371704102, 1.1574028730392456, 1.0919686555862427, 1.068162202835083, 1.3187150955200195, 1.0755515098571777, 1.075500726699829, 1.0198543071746826, 1.0307226181030273, 1.0773375034332275, 1.047929048538208, 1.2880719900131226, 1.037077784538269, 1.1881651878356934, 1.047286033630371, 1.138930320739746, 1.101270079612732, 1.4862060546875, 1.6297872066497803, 1.156786322593689, 1.678533673286438, 1.3820987939834595, 1.026892066001892, 1.2838174104690552, 1.140229344367981, 1.2426801919937134, 1.0551143884658813, 1.0275521278381348, 1.0186963081359863, 1.1560118198394775, 1.2627924680709839, 1.4466822147369385, 1.0179022550582886, 1.046844244003296, 1.109026551246643, 1.034313678741455, 1.065625786781311, 1.0385857820510864, 1.1882330179214478, 1.0796482563018799, 1.0473614931106567, 1.054441213607788, 1.017055869102478, 1.1757549047470093, 1.0946170091629028, 1.0374616384506226, 1.124267816543579, 1.0236709117889404, 1.2870413064956665, 1.0362436771392822, 1.2005137205123901, 1.2081884145736694, 1.1238938570022583, 1.0585774183273315, 1.1016525030136108, 1.0103421211242676, 1.511237382888794, 1.1313412189483643, 1.1270307302474976, 1.3122414350509644, 1.0784000158309937, 1.0890843868255615, 1.0519987344741821, 1.1318920850753784, 1.1589235067367554, 1.0790960788726807, 1.6389776468276978, 1.1294745206832886, 1.239823818206787, 1.036213994026184, 1.2132067680358887, 1.0776557922363281, 1.0670055150985718, 1.1058669090270996, 1.19233238697052, 1.1580233573913574, 1.093717098236084, 1.050292730331421, 1.0694280862808228, 2.2528090476989746, 1.0037226676940918, 1.0267332792282104, 1.0201935768127441, 1.0232582092285156, 1.0205345153808594, 1.2465859651565552, 1.0025036334991455, 1.0453161001205444, 1.0559035539627075, 1.3680355548858643, 1.1048387289047241, 1.0929020643234253, 1.0446174144744873, 1.0980075597763062, 1.4322646856307983, 1.5197744369506836, 1.031736135482788, 1.1506333351135254, 5.358974456787109, 1.3316267728805542, 1.1644278764724731, 2.33695650100708, 1.359417200088501, 1.0983294248580933, 1.016165018081665, 1.3536173105239868, 1.0687994956970215, 1.0930063724517822, 1.0049936771392822, 1.2499940395355225, 1.110984444618225, 1.0682035684585571, 1.0491397380828857, 3.299999952316284, 1.1687215566635132, 1.0913575887680054, 1.4395396709442139, 1.1087684631347656, 1.5330535173416138, 35.83333206176758, 1.0262887477874756, 1.3546860218048096, 1.0274971723556519, 1.1280080080032349, 1.2287440299987793, 1.2012914419174194, 1.634209394454956, 1.1572519540786743, 1.4645600318908691, 1.1090426445007324, 1.0009342432022095, 1.1533100605010986, 1.1889655590057373]
********** total_n=[200] batchn=[100]  N=[3200]  nitr=[4]  alpha=[0.4]  beta=[0.2] ******
@ Average per query          [3.623143434524536] ms
 --  Average per query NF    [1.3577532768249512] ms
 --  Average per query vegas [2.265390157699585] ms
Mean [1.405]  Median [1.102]  95th [1.538]  99th [5.395]  max [35.833]
Mean [1.405]  Median [1.102]  95th [1.538]  99th [5.395]  max [35.833]


WORKLOAD-FINISHED | Type: QueryWorkload | Progress: 3/3 | Workload-time: 12.322600 | Model-update-time: 0.000000


Experiment Summary: #data-update-times=1, #drift=1 | total-time=28.432958