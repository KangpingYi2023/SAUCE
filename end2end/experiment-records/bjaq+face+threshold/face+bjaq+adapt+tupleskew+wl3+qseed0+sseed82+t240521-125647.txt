Input arguments = Namespace(data_update='tupleskew', update_size=80000, dataset='bjaq', debug=False, drift_test='js', model_update='adapt', model='face', end2end=False, query_seed=0, random_seed=42, num_workload=3)
JSON-passed parameters = {'data_updater': {'skew_ratio': '1e-3'}, 'random_seed': 82, 'face': {'learning_rate': '1e-4', 'epochs': 0, 'factor': 'auto'}}
MODEL-PATH=/home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
DATASET-PATH=/home/kangping/code_copy/SAUCE/data/bjaq/end2end/bjaq.npy
Workloads: ['QueryWorkload', 'DataUpdateWorkload', 'QueryWorkload']

WORKLOAD-START | Type: QueryWorkload | Progress: 1/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/evaluate/Evaluate-FACE-end2end.py', '--dataset=bjaq', '--end2end']
/home/kangping/anaconda3/envs/LndCar/lib/python3.9/site-packages/torchquad/integration/vegas_map.py:314: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/BucketizationUtils.h:33.)
  iy = torch.searchsorted(self.x_edges[d, :], x[:, d], side='right')
DEVICE NAME
 Tesla V100S-PCIE-32GB
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
data shape: (382168, 5)
Save oracle results to :
./FACE/evaluate/oracle/bjaq_rng-1234.csv
Found oracle card!
torch.Size([5, 2])
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
Took  8.188982725143433
tensor(0.9964)
result is  tensor(380809.8750)
Enter testHyper
ReportEsts: [1.168245792388916, 1.0489587783813477, 1.2426352500915527, 1.2714256048202515, 1.2651785612106323, 1.1718093156814575, 1.2419474124908447, 1.0635325908660889, 1.059911847114563, 1.0845115184783936, 1.0335917472839355, 1.1508474349975586, 1.0798810720443726, 1.0290178060531616, 1.1122955083847046, 1.2875820398330688, 1.3139173984527588, 1.1819477081298828, 1.1479963064193726, 1.1428571939468384, 1.153752088546753, 1.1086864471435547, 1.1178911924362183, 1.0757784843444824, 1.1054480075836182, 1.1772152185440063, 1.193130612373352, 1.3904434442520142, 1.0362638235092163, 1.1832977533340454, 1.1379921436309814, 1.2677052021026611, 1.1622059345245361, 1.0095199346542358, 1.1683825254440308, 1.0969878435134888, 1.6580301523208618, 1.0254327058792114, 1.0203056335449219, 1.0046706199645996, 1.0732650756835938, 1.0700290203094482, 1.0284936428070068, 1.0522894859313965, 1.0948657989501953, 1.0532879829406738, 1.1677688360214233, 1.247231125831604, 1.026016116142273, 1.133671760559082, 1.1582914590835571, 1.3272440433502197, 1.1185085773468018, 1.3921154737472534, 1.1003180742263794, 1.0057951211929321, 1.1140186786651611, 1.1589477062225342, 1.1664611101150513, 1.4422328472137451, 1.2716463804244995, 1.143600583076477, 1.4568973779678345, 1.0345454216003418, 1.3147410154342651, 1.0571428537368774, 1.0951964855194092, 1.0246148109436035, 1.1609865427017212, 1.221052646636963, 1.0434433221817017, 1.113516926765442, 1.2862253189086914, 1.15969717502594, 1.1803230047225952, 1.0883439779281616, 1.753249168395996, 1.0585973262786865, 1.0370776653289795, 1.275908350944519, 1.1960495710372925, 1.2518171072006226, 1.0258848667144775, 1.20235013961792, 1.2467314004898071, 1.0507699251174927, 1.028488278388977, 1.0617907047271729, 1.1049710512161255, 1.0026535987854004, 1.0210447311401367, 1.2012559175491333, 1.0144248008728027, 1.2950588464736938, 1.101694941520691, 1.093819260597229, 1.0561368465423584, 1.2668547630310059, 1.01694917678833, 1.2249789237976074, 1.1502809524536133, 1.122193455696106, 1.056984782218933, 1.0088626146316528, 1.044456124305725, 1.0920774936676025, 1.2445597648620605, 1.2076789140701294, 1.0602599382400513, 1.0020766258239746, 1.1239360570907593, 1.0801843404769897, 1.230316162109375, 1.076594352722168, 1.0657238960266113, 1.1073170900344849, 1.006973147392273, 1.063588261604309, 1.305759310722351, 1.003781795501709, 1.252084493637085, 1.1238443851470947, 1.1207574605941772, 1.198143482208252, 1.196220874786377, 1.2165703773498535, 1.136471152305603, 1.2641147375106812, 1.162528157234192, 1.1048763990402222, 1.1256667375564575, 1.040097713470459, 1.1491609811782837, 1.3928571939468384, 1.1266483068466187, 1.28196382522583, 1.0685830116271973, 1.2507317066192627, 1.0358654260635376, 1.1038661003112793, 1.1481904983520508, 1.1568244695663452, 1.1353498697280884, 1.303904414176941, 1.0201843976974487, 1.046924352645874, 1.0506922006607056, 1.4270135164260864, 1.0466183423995972, 1.0130681991577148, 1.1662330627441406, 1.1521599292755127, 1.2868338823318481, 1.1058913469314575, 1.023506760597229, 1.0165016651153564, 1.118739128112793, 1.0540236234664917, 1.0336425304412842, 1.1713517904281616, 1.011583924293518, 1.0834981203079224, 1.1395472288131714, 1.2280471324920654, 1.1526252031326294, 1.1050052642822266, 1.1435950994491577, 1.3344513177871704, 1.1176605224609375, 1.005714774131775, 1.08879816532135, 1.0120329856872559, 1.1558442115783691, 1.2302290201187134, 1.0258855819702148, 1.0051847696304321, 1.0276968479156494, 1.146790862083435, 1.2085380554199219, 1.2182612419128418, 1.1370630264282227, 1.0838439464569092, 1.0198196172714233, 1.148110270500183, 1.148134708404541, 1.1046512126922607, 1.006447434425354, 1.2678192853927612, 1.1260309219360352, 1.2816680669784546, 1.0454131364822388, 1.0074450969696045, 1.4428715705871582, 1.4281258583068848, 1.5135315656661987, 1.2447013854980469, 1.13184654712677, 1.1343283653259277, 1.0743896961212158, 1.131274938583374]
********** total_n=[200] batchn=[100]  N=[3200]  nitr=[4]  alpha=[0.4]  beta=[0.2] ******
@ Average per query          [3.752216100692749] ms
 --  Average per query NF    [1.3597846031188965] ms
 --  Average per query vegas [2.3924314975738525] ms
Mean [1.146]  Median [1.125]  95th [1.391]  99th [1.515]  max [1.753]
Mean [1.146]  Median [1.125]  95th [1.391]  99th [1.515]  max [1.753]


WORKLOAD-FINISHED | Type: QueryWorkload | Progress: 1/3 | Workload-time: 11.862162 | Model-update-time: 0.000000


WORKLOAD-START | Type: DataUpdateWorkload | Progress: 2/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/Naru/eval_model.py', '--model=face', '--data_update=tupleskew', '--dataset=bjaq', '--drift_test=js', '--end2end', '--update_size=80000', '--query_seed=0', '--eval_type=drift']
Torch device: cuda:1
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
TupleSkewSample - START
0.001
TupleSkewSample - END
load_npy_dataset_from_path - df.shape = (458601, 5)
Parsing... done, took 0.0s
Data updated!
Previous data size: (382168, 5), new data size: (76433, 5)
[1.61886215e-04 6.55651093e-07 1.19805336e-04 5.12599945e-06
 2.88486481e-05]
Distance score: 6.326437141979113e-05
SAUCE Drift detection: True
Detection latency: 0.0241s

DRIFT-DETECTED after 2-th workload
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/incremental_train.py', '--dataset=bjaq', '--end2end', '--model=face', '--model_update=adapt', '--update_size=80000', '--epochs=0']
Torch device: cuda:1
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
update_size: 76433
AdaptTask - START
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
train data shape: 80000, transfer data shape:16000
Training done; evaluating likelihood on full data:
Saved to: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
AdaptTask - END

WORKLOAD-FINISHED | Type: DataUpdateWorkload | Progress: 2/3 | Workload-time: 2.008340 | Model-update-time: 2.228774


WORKLOAD-START | Type: QueryWorkload | Progress: 3/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/evaluate/Evaluate-FACE-end2end.py', '--dataset=bjaq', '--query_seed=0', '--end2end', '--num_workload=3', '--data_update=tupleskew', '--model_update=adapt']
/home/kangping/anaconda3/envs/LndCar/lib/python3.9/site-packages/torchquad/integration/vegas_map.py:314: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/BucketizationUtils.h:33.)
  iy = torch.searchsorted(self.x_edges[d, :], x[:, d], side='right')
DEVICE NAME
 Tesla V100S-PCIE-32GB
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
data shape: (458601, 5)
Save oracle results to :
./FACE/evaluate/oracle/bjaq_rng-0.csv
Found oracle card!
torch.Size([5, 2])
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
Took  8.166901588439941
tensor(0.9942)
result is  tensor(455931.9688)
Enter testHyper
ReportEsts: [1.1013433933258057, 1.0877606868743896, 1.5845481157302856, 2.75, 1.0172618627548218, 1.3119391202926636, 1.1801023483276367, 1.042760968208313, 1.1816091537475586, 1.4980988502502441, 1.0745075941085815, 1.134751796722412, 1.0475000143051147, 1.2977170944213867, 1.0775368213653564, 1.0707402229309082, 1.033934473991394, 1.200819492340088, 1.0383284091949463, 1.233963131904602, 1.003124713897705, 1.8828316926956177, 1.0085413455963135, 1.0306352376937866, 1.0248357057571411, 1.2336434125900269, 1.0333718061447144, 1.0907191038131714, 1.0952235460281372, 1.0217238664627075, 1.2018977403640747, 1.491525411605835, 1.0813357830047607, 1.1253708600997925, 1.0218231678009033, 1.0458731651306152, 1.2506920099258423, 1.0580644607543945, 1.0832550525665283, 1.078575849533081, 1.115110993385315, 1.036649227142334, 1.058622121810913, 1.0067427158355713, 1.087024211883545, 1.1566988229751587, 1.0967167615890503, 1.3019486665725708, 1.4555555582046509, 1.2070280313491821, 1.028493046760559, 1.0333536863327026, 1.040309190750122, 1.077842354774475, 1.092680811882019, 1.1567537784576416, 1.241172194480896, 1.0727797746658325, 1.3344899415969849, 1.208281397819519, 1.2761905193328857, 1.1077944040298462, 1.336492896080017, 1.4665533304214478, 1.0291974544525146, 1.0024278163909912, 1.3997619152069092, 1.0567952394485474, 1.1272220611572266, 1.1482722759246826, 1.1694015264511108, 1.1368513107299805, 1.0537195205688477, 1.1268647909164429, 1.011519432067871, 1.3034682273864746, 1.1118980646133423, 1.215013027191162, 1.0237469673156738, 1.121982216835022, 1.0577311515808105, 1.0278664827346802, 1.2578315734863281, 1.2137116193771362, 1.1774766445159912, 1.612765908241272, 1.3378034830093384, 1.544938564300537, 1.246687889099121, 1.135648250579834, 1.0579813718795776, 1.0724272727966309, 1.330172061920166, 1.1209971904754639, 1.0719918012619019, 1.0444202423095703, 1.1017138957977295, 1.2395143508911133, 1.5113599300384521, 1.3921968936920166, 1.0948381423950195, 1.0901895761489868, 1.5782828330993652, 1.1515861749649048, 1.0166985988616943, 1.1258277893066406, 1.019735336303711, 1.045849323272705, 1.5218579769134521, 1.207436203956604, 2.590909004211426, 1.027572751045227, 1.0266776084899902, 1.1538774967193604, 1.0117416381835938, 1.040020227432251, 1.1202963590621948, 1.1231540441513062, 1.1643637418746948, 1.1150442361831665, 1.0044186115264893, 1.0607287883758545, 1.0115766525268555, 2.4909090995788574, 1.0800693035125732, 1.2363117933273315, 1.1931897401809692, 1.7006369829177856, 1.0273202657699585, 1.0812153816223145, 1.126375436782837, 1.1732934713363647, 1.0241734981536865, 1.3834244012832642, 1.8227026462554932, 1.2553790807724, 1.0084701776504517, 1.1052340269088745, 1.002928614616394, 1.012481451034546, 1.029468059539795, 1.1194180250167847, 1.021639347076416, 1.0041381120681763, 1.0792710781097412, 1.0072588920593262, 1.1530054807662964, 1.1253465414047241, 1.0356909036636353, 1.0782209634780884, 1.4556734561920166, 1.1791096925735474, 1.1078174114227295, 1.1008635759353638, 1.047845721244812, 1.2891194820404053, 1.0014170408248901, 1.0378787517547607, 1.2379053831100464, 1.0129972696304321, 1.0478901863098145, 1.11565363407135, 1.2182167768478394, 1.147990107536316, 1.2297024726867676, 1.3295774459838867, 1.0140093564987183, 1.0773533582687378, 1.224910020828247, 1.256608486175537, 1.0209928750991821, 1.0408726930618286, 1.5120275020599365, 1.0764107704162598, 1.0965297222137451, 1.0921528339385986, 1.095685362815857, 1.042427659034729, 1.100767970085144, 1.0547621250152588, 1.8157894611358643, 1.2285981178283691, 1.0138357877731323, 1.615694522857666, 1.3200161457061768, 1.9340721368789673, 1.2292993068695068, 1.0847903490066528, 1.2314717769622803, 1.0127630233764648, 1.0615061521530151, 1.0235313177108765, 1.245152235031128, 1.7259899377822876, 1.0657466650009155, 1.62198805809021, 1.3276393413543701, 1.0657695531845093, 1.0575079917907715, 1.1491498947143555]
********** total_n=[200] batchn=[100]  N=[3200]  nitr=[4]  alpha=[0.4]  beta=[0.2] ******
@ Average per query          [3.7071824073791504] ms
 --  Average per query NF    [1.356184482574463] ms
 --  Average per query vegas [2.3509979248046875] ms
Mean [1.191]  Median [1.107]  95th [1.616]  99th [2.492]  max [2.750]
Mean [1.191]  Median [1.107]  95th [1.616]  99th [2.492]  max [2.750]


WORKLOAD-FINISHED | Type: QueryWorkload | Progress: 3/3 | Workload-time: 12.371794 | Model-update-time: 0.000000


Experiment Summary: #data-update-times=1, #drift=1 | total-time=28.515409