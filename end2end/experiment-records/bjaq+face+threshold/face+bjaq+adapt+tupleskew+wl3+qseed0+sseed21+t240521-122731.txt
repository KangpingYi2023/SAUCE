Input arguments = Namespace(data_update='tupleskew', update_size=80000, dataset='bjaq', debug=False, drift_test='js', model_update='adapt', model='face', end2end=False, query_seed=0, random_seed=42, num_workload=3)
JSON-passed parameters = {'data_updater': {'skew_ratio': '1e-3'}, 'random_seed': 21, 'face': {'learning_rate': '1e-4', 'epochs': 0, 'factor': 'auto'}}
MODEL-PATH=/home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
DATASET-PATH=/home/kangping/code_copy/SAUCE/data/bjaq/end2end/bjaq.npy
Workloads: ['QueryWorkload', 'DataUpdateWorkload', 'QueryWorkload']

WORKLOAD-START | Type: QueryWorkload | Progress: 1/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/evaluate/Evaluate-FACE-end2end.py', '--dataset=bjaq', '--end2end']
/home/kangping/anaconda3/envs/LndCar/lib/python3.9/site-packages/torchquad/integration/vegas_map.py:314: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/BucketizationUtils.h:33.)
  iy = torch.searchsorted(self.x_edges[d, :], x[:, d], side='right')
DEVICE NAME
 Tesla V100S-PCIE-32GB
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
data shape: (382168, 5)
Save oracle results to :
./FACE/evaluate/oracle/bjaq_rng-1234.csv
Found oracle card!
torch.Size([5, 2])
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
Took  8.168232917785645
tensor(0.9975)
result is  tensor(381231.5938)
Enter testHyper
ReportEsts: [1.2193586826324463, 1.0398439168930054, 1.5451966524124146, 1.143819808959961, 1.3011937141418457, 1.0926527976989746, 1.336984395980835, 1.0261017084121704, 1.054646372795105, 1.1748547554016113, 1.0223945379257202, 1.0531779527664185, 1.1880710124969482, 1.0133928060531616, 1.0955220460891724, 1.352203130722046, 1.360520839691162, 1.1907362937927246, 1.1281324625015259, 1.0586735010147095, 1.1805864572525024, 1.1466220617294312, 1.1627806425094604, 1.0446200370788574, 1.1496365070343018, 1.1311030387878418, 1.0191441774368286, 1.315682291984558, 1.0302295684814453, 1.081680417060852, 1.1975878477096558, 1.1292606592178345, 1.0075461864471436, 1.1039466857910156, 1.0450090169906616, 1.2272727489471436, 1.2873294353485107, 1.2013145685195923, 1.0615416765213013, 1.0895808935165405, 1.053446888923645, 1.1136913299560547, 1.010494351387024, 1.0144364833831787, 1.0456370115280151, 1.0600906610488892, 1.018805980682373, 1.3476163148880005, 1.10969877243042, 1.0913705825805664, 1.1974025964736938, 1.2938287258148193, 1.0268752574920654, 1.2006678581237793, 1.1458529233932495, 1.0064904689788818, 1.2654205560684204, 1.122955083847046, 1.1351243257522583, 1.3194774389266968, 1.3824992179870605, 1.121445655822754, 1.2672909498214722, 1.2032474279403687, 1.3580247163772583, 1.2571429014205933, 1.1965649127960205, 1.009766936302185, 1.0061767101287842, 1.2526315450668335, 1.0032180547714233, 1.052003026008606, 1.2509876489639282, 1.2430633306503296, 1.2586748600006104, 1.1084402799606323, 2.0760598182678223, 1.0889055728912354, 1.0563228130340576, 1.260873794555664, 1.154952883720398, 1.1633015871047974, 1.037380337715149, 1.199703335762024, 1.2034237384796143, 1.1656235456466675, 1.1313345432281494, 1.0989660024642944, 1.1505160331726074, 1.0184094905853271, 1.0712906122207642, 1.3521517515182495, 1.0032562017440796, 1.2895970344543457, 1.0714285373687744, 1.0946033000946045, 1.211556315422058, 1.1114641427993774, 1.0, 1.277408480644226, 1.148667573928833, 1.0905578136444092, 1.0368125438690186, 1.0131771564483643, 1.110041856765747, 1.0129830837249756, 1.2053756713867188, 1.1815009117126465, 1.113112211227417, 1.0723457336425781, 1.3375918865203857, 1.0882079601287842, 1.1759790182113647, 1.0110852718353271, 1.082647442817688, 1.15816330909729, 1.0295069217681885, 1.0417898893356323, 1.2881618738174438, 1.0087426900863647, 1.1385998725891113, 1.0769973993301392, 1.1089551448822021, 1.0529133081436157, 1.3028100728988647, 1.1764341592788696, 1.1058307886123657, 1.0585846900939941, 1.2117646932601929, 1.1199064254760742, 1.151749610900879, 1.1101832389831543, 1.3088874816894531, 1.4285714626312256, 1.1706165075302124, 1.2260435819625854, 1.1034890413284302, 1.123252034187317, 1.01453697681427, 1.2397925853729248, 1.1102592945098877, 1.1410062313079834, 1.123910903930664, 1.3675605058670044, 1.107704758644104, 1.0337119102478027, 1.0206446647644043, 1.5835617780685425, 1.0589488744735718, 1.0205117464065552, 1.2814714908599854, 1.1726701259613037, 1.3004224300384521, 1.1741607189178467, 1.0829747915267944, 1.042083978652954, 1.1274453401565552, 1.0638717412948608, 1.0301175117492676, 1.1626380681991577, 1.26908278465271, 1.1144274473190308, 1.022993564605713, 1.310489535331726, 1.2227978706359863, 1.066914439201355, 1.1818108558654785, 1.2220799922943115, 1.2249034643173218, 1.018500566482544, 1.0510990619659424, 1.0927040576934814, 1.1558442115783691, 1.2098311185836792, 1.0940054655075073, 1.0738626718521118, 1.0093183517456055, 1.2939203977584839, 1.0537822246551514, 1.0701392889022827, 1.0358811616897583, 1.0042290687561035, 1.0204476118087769, 1.1123595237731934, 1.1492213010787964, 1.1236786842346191, 1.0319095849990845, 1.2175533771514893, 1.0477598905563354, 1.3099746704101562, 1.0708662271499634, 1.0457663536071777, 1.6593023538589478, 1.287247657775879, 1.8552539348602295, 1.2312579154968262, 1.1157565116882324, 1.1014492511749268, 1.0008394718170166, 1.094223141670227]
********** total_n=[200] batchn=[100]  N=[3200]  nitr=[4]  alpha=[0.4]  beta=[0.2] ******
@ Average per query          [3.80243182182312] ms
 --  Average per query NF    [1.3668644428253174] ms
 --  Average per query vegas [2.4355673789978027] ms
Mean [1.153]  Median [1.121]  95th [1.352]  99th [1.661]  max [2.076]
Mean [1.153]  Median [1.121]  95th [1.352]  99th [1.661]  max [2.076]


WORKLOAD-FINISHED | Type: QueryWorkload | Progress: 1/3 | Workload-time: 11.846230 | Model-update-time: 0.000000


WORKLOAD-START | Type: DataUpdateWorkload | Progress: 2/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/Naru/eval_model.py', '--model=face', '--data_update=tupleskew', '--dataset=bjaq', '--drift_test=js', '--end2end', '--update_size=80000', '--query_seed=0', '--eval_type=drift']
Torch device: cuda:1
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
TupleSkewSample - START
0.001
TupleSkewSample - END
load_npy_dataset_from_path - df.shape = (458601, 5)
Parsing... done, took 0.0s
Data updated!
Previous data size: (382168, 5), new data size: (76433, 5)
[2.7537346e-05 8.6963177e-05 5.9366226e-05 2.3424625e-05 1.7702579e-05]
Distance score: 4.299879219615832e-05
SAUCE Drift detection: True
Detection latency: 0.0227s

DRIFT-DETECTED after 2-th workload
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/incremental_train.py', '--dataset=bjaq', '--end2end', '--model=face', '--model_update=adapt', '--update_size=80000', '--epochs=0']
Torch device: cuda:1
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
update_size: 76433
AdaptTask - START
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
train data shape: 80000, transfer data shape:16000
Training done; evaluating likelihood on full data:
Saved to: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
AdaptTask - END

WORKLOAD-FINISHED | Type: DataUpdateWorkload | Progress: 2/3 | Workload-time: 1.987085 | Model-update-time: 2.223778


WORKLOAD-START | Type: QueryWorkload | Progress: 3/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/evaluate/Evaluate-FACE-end2end.py', '--dataset=bjaq', '--query_seed=0', '--end2end', '--num_workload=3', '--data_update=tupleskew', '--model_update=adapt']
/home/kangping/anaconda3/envs/LndCar/lib/python3.9/site-packages/torchquad/integration/vegas_map.py:314: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/BucketizationUtils.h:33.)
  iy = torch.searchsorted(self.x_edges[d, :], x[:, d], side='right')
DEVICE NAME
 Tesla V100S-PCIE-32GB
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
data shape: (458601, 5)
Save oracle results to :
./FACE/evaluate/oracle/bjaq_rng-0.csv
Found oracle card!
torch.Size([5, 2])
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
Took  8.16204285621643
tensor(0.9931)
result is  tensor(455458.7188)
Enter testHyper
ReportEsts: [1.0389658212661743, 1.1653302907943726, 1.0014184713363647, 1.4219508171081543, 1.0326297283172607, 1.2627383470535278, 1.2827633619308472, 1.0177452564239502, 1.0016945600509644, 1.0429375171661377, 1.086734652519226, 1.4004390239715576, 1.016990303993225, 1.0160316228866577, 1.0147877931594849, 1.0104460716247559, 1.1671323776245117, 1.204830288887024, 1.095978856086731, 1.251518964767456, 1.1090017557144165, 2.059007406234741, 1.802990198135376, 1.1087743043899536, 1.1063451766967773, 1.2579340934753418, 1.0097073316574097, 1.21489679813385, 1.336982250213623, 1.598312258720398, 1.1180016994476318, 1.4322034120559692, 1.0590053796768188, 1.0093295574188232, 1.0224483013153076, 1.0193400382995605, 1.0529392957687378, 1.051163673400879, 1.096272587776184, 1.027054786682129, 1.042870044708252, 1.1494252681732178, 1.1576597690582275, 1.1586108207702637, 1.0116015672683716, 1.1894570589065552, 1.0269676446914673, 1.171513319015503, 1.4944649934768677, 1.044714093208313, 1.1282984018325806, 1.0767066478729248, 1.0801349878311157, 1.0893818140029907, 1.0296226739883423, 1.0490933656692505, 1.150075912475586, 1.1754910945892334, 1.4231470823287964, 1.0345925092697144, 1.121338963508606, 1.0342131853103638, 1.1129964590072632, 1.2705858945846558, 1.023167610168457, 1.0410125255584717, 1.4187103509902954, 1.0351605415344238, 1.1290252208709717, 1.0920286178588867, 1.1423780918121338, 1.0246716737747192, 1.130081295967102, 1.0977352857589722, 1.0739003419876099, 1.240231990814209, 1.0633819103240967, 1.034450650215149, 1.2049447298049927, 1.216077208518982, 1.0020639896392822, 1.213542103767395, 1.0580079555511475, 1.2753815650939941, 1.2435110807418823, 1.6638298034667969, 1.1914575099945068, 1.6247327327728271, 1.4143166542053223, 1.1567631959915161, 1.296223759651184, 1.1170953512191772, 1.3948017358779907, 1.2209806442260742, 1.0142029523849487, 1.0013976097106934, 1.1135374307632446, 1.2098790407180786, 1.2468442916870117, 1.0184588432312012, 1.1659399271011353, 1.0673686265945435, 1.051546335220337, 1.262015461921692, 1.2328767776489258, 1.042097806930542, 1.0076934099197388, 1.1867924928665161, 1.048710584640503, 1.0351184606552124, 1.0944937467575073, 1.0818918943405151, 1.1715431213378906, 1.317885160446167, 1.1438946723937988, 1.118109107017517, 1.1548784971237183, 1.6456685066223145, 1.110008716583252, 1.2831858396530151, 1.000561237335205, 1.0190908908843994, 1.125335454940796, 1.9402390718460083, 1.0970805883407593, 1.1336859464645386, 1.15055251121521, 1.703703761100769, 1.041424036026001, 1.1495752334594727, 1.0740461349487305, 1.1665892601013184, 1.0627983808517456, 1.3076162338256836, 1.591549277305603, 1.2169784307479858, 1.1178090572357178, 1.0711814165115356, 1.1756223440170288, 1.014827847480774, 1.1065906286239624, 1.1485326290130615, 1.076446294784546, 1.051624059677124, 1.0301107168197632, 1.1433861255645752, 1.1657458543777466, 1.0520612001419067, 1.225329875946045, 1.074385643005371, 1.1427651643753052, 1.0927362442016602, 1.1936228275299072, 1.0537444353103638, 1.0326844453811646, 1.4254041910171509, 1.2464531660079956, 1.1138211488723755, 1.2690587043762207, 1.0692249536514282, 1.0676990747451782, 1.3436425924301147, 1.8754035234451294, 1.0609052181243896, 1.0553855895996094, 1.0031803846359253, 1.0583488941192627, 1.060444951057434, 1.0001171827316284, 1.2196133136749268, 1.0481964349746704, 1.0985589027404785, 1.3702422380447388, 1.053820013999939, 1.0496788024902344, 1.1342159509658813, 1.1945263147354126, 1.1486461162567139, 1.014167070388794, 1.2931854724884033, 1.763157844543457, 1.0258899927139282, 1.0872050523757935, 1.568941354751587, 1.392755150794983, 1.084465503692627, 1.5313286781311035, 1.1186902523040771, 1.3767253160476685, 1.1049187183380127, 1.1894655227661133, 1.0290066003799438, 1.2509597539901733, 1.7951806783676147, 1.0251325368881226, 1.0283321142196655, 1.2439846992492676, 1.2294318675994873, 1.4939758777618408, 1.0719084739685059]
********** total_n=[200] batchn=[100]  N=[3200]  nitr=[4]  alpha=[0.4]  beta=[0.2] ******
@ Average per query          [3.737795352935791] ms
 --  Average per query NF    [1.3520503044128418] ms
 --  Average per query vegas [2.385745048522949] ms
Mean [1.177]  Median [1.117]  95th [1.600]  99th [1.876]  max [2.059]
Mean [1.177]  Median [1.117]  95th [1.600]  99th [1.876]  max [2.059]


WORKLOAD-FINISHED | Type: QueryWorkload | Progress: 3/3 | Workload-time: 12.415063 | Model-update-time: 0.000000


Experiment Summary: #data-update-times=1, #drift=1 | total-time=28.527380