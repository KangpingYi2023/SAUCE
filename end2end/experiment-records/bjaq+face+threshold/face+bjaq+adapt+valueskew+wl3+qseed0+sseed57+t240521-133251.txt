Input arguments = Namespace(data_update='valueskew', update_size=80000, dataset='bjaq', debug=False, drift_test='js', model_update='adapt', model='face', end2end=False, query_seed=0, random_seed=42, num_workload=3)
JSON-passed parameters = {'data_updater': {'skew_ratio': '1e-3'}, 'random_seed': 57, 'face': {'learning_rate': '1e-4', 'epochs': 0, 'factor': 'auto'}}
MODEL-PATH=/home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
DATASET-PATH=/home/kangping/code_copy/SAUCE/data/bjaq/end2end/bjaq.npy
Workloads: ['QueryWorkload', 'DataUpdateWorkload', 'QueryWorkload']

WORKLOAD-START | Type: QueryWorkload | Progress: 1/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/evaluate/Evaluate-FACE-end2end.py', '--dataset=bjaq', '--end2end']
/home/kangping/anaconda3/envs/LndCar/lib/python3.9/site-packages/torchquad/integration/vegas_map.py:314: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/BucketizationUtils.h:33.)
  iy = torch.searchsorted(self.x_edges[d, :], x[:, d], side='right')
DEVICE NAME
 Tesla V100S-PCIE-32GB
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
data shape: (382168, 5)
Save oracle results to :
./FACE/evaluate/oracle/bjaq_rng-1234.csv
Found oracle card!
torch.Size([5, 2])
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
Took  8.164370775222778
tensor(0.9916)
result is  tensor(378939.)
Enter testHyper
ReportEsts: [1.1455518007278442, 1.0142035484313965, 1.3882547616958618, 1.304695725440979, 1.2862329483032227, 1.0591753721237183, 1.1386969089508057, 1.0647108554840088, 1.053722620010376, 1.2138161659240723, 1.038759708404541, 1.122033953666687, 1.0662672519683838, 1.0513392686843872, 1.2057386636734009, 1.225494146347046, 1.0968871116638184, 1.3180522918701172, 1.1362680196762085, 1.1301020383834839, 1.1034305095672607, 1.052948236465454, 1.1331790685653687, 1.1340256929397583, 1.1143510341644287, 1.0813742876052856, 1.0664414167404175, 1.5329853296279907, 1.0044444799423218, 1.0416030883789062, 1.1106775999069214, 1.0999833345413208, 1.0714895725250244, 1.1827751398086548, 1.1502635478973389, 1.1074252128601074, 1.2951945066452026, 1.1751883029937744, 1.0665909051895142, 1.0458682775497437, 1.0116244554519653, 1.2317367792129517, 1.0115770101547241, 1.0120806694030762, 1.0661314725875854, 1.0204081535339355, 1.0708190202713013, 1.1906548738479614, 1.0705757141113281, 1.1590524911880493, 1.2293332815170288, 1.408893346786499, 1.0092947483062744, 1.168389916419983, 1.1702402830123901, 1.0537784099578857, 1.4299064874649048, 1.2093656063079834, 1.24846351146698, 1.4267979860305786, 1.158021330833435, 1.113904356956482, 1.1804888248443604, 1.1038011312484741, 1.3360323905944824, 1.2000000476837158, 1.1875, 1.0682631731033325, 1.1497468948364258, 1.2736842632293701, 1.093227744102478, 1.0603420734405518, 1.2574840784072876, 1.1227185726165771, 1.2263734340667725, 1.088227391242981, 1.7146331071853638, 1.0365787744522095, 1.098907232284546, 1.1009531021118164, 1.1071137189865112, 1.0675173997879028, 1.1334632635116577, 1.1595818996429443, 1.1685270071029663, 1.0188376903533936, 1.0679024457931519, 1.1256494522094727, 1.0640003681182861, 1.173895001411438, 1.0415383577346802, 1.3035190105438232, 1.016588568687439, 1.3139173984527588, 1.133720874786377, 1.014661431312561, 1.0197193622589111, 1.394520878791809, 1.01694917678833, 1.1932402849197388, 1.153521180152893, 1.0011024475097656, 1.0360289812088013, 1.021229863166809, 1.0703974962234497, 1.1370266675949097, 1.2998473644256592, 1.1902269124984741, 1.032780647277832, 1.05669105052948, 1.164849042892456, 1.0882079601287842, 1.2749334573745728, 1.0995599031448364, 1.1333024501800537, 1.1884816884994507, 1.0029494762420654, 1.0388237237930298, 1.2573292255401611, 1.0411888360977173, 1.2700629234313965, 1.1209466457366943, 1.1893712282180786, 1.2552413940429688, 1.3406007289886475, 1.2096574306488037, 1.1499812602996826, 1.0651674270629883, 1.3171355724334717, 1.1048763990402222, 1.0133638381958008, 1.2156367301940918, 1.3595401048660278, 1.5, 1.0262434482574463, 1.038330316543579, 1.015875220298767, 1.2107317447662354, 1.1241097450256348, 1.0801806449890137, 1.179849624633789, 1.0250780582427979, 1.007775068283081, 1.1621112823486328, 1.0762066841125488, 1.0078483819961548, 1.0660629272460938, 1.3463373184204102, 1.0590307712554932, 1.1041193008422852, 1.3044531345367432, 1.159723162651062, 1.2915574312210083, 1.0122205018997192, 1.2169429063796997, 1.0237177610397339, 1.1251376867294312, 1.0360157489776611, 1.0954253673553467, 1.1850790977478027, 1.124955654144287, 1.0353318452835083, 1.0259008407592773, 1.3253182172775269, 1.114521861076355, 1.0126702785491943, 1.2249921560287476, 1.3299907445907593, 1.1105600595474243, 1.0025107860565186, 1.0054993629455566, 1.0096555948257446, 1.1753246784210205, 1.1065596342086792, 1.0711852312088013, 1.000757098197937, 1.0869776010513306, 1.291610836982727, 1.1297346353530884, 1.0758538246154785, 1.030151128768921, 1.0323920249938965, 1.0011065006256104, 1.0909091234207153, 1.085476279258728, 1.1279069185256958, 1.0135602951049805, 1.2296372652053833, 1.0684369802474976, 1.4574577808380127, 1.0394809246063232, 1.004143476486206, 1.4568657875061035, 1.2548580169677734, 1.5864518880844116, 1.247104287147522, 1.1717798709869385, 1.0410958528518677, 1.1744954586029053, 1.0740703344345093]
********** total_n=[200] batchn=[100]  N=[3200]  nitr=[4]  alpha=[0.4]  beta=[0.2] ******
@ Average per query          [3.7687301635742188] ms
 --  Average per query NF    [1.3655459880828857] ms
 --  Average per query vegas [2.403184175491333] ms
Mean [1.144]  Median [1.114]  95th [1.389]  99th [1.534]  max [1.715]
Mean [1.144]  Median [1.114]  95th [1.389]  99th [1.534]  max [1.715]


WORKLOAD-FINISHED | Type: QueryWorkload | Progress: 1/3 | Workload-time: 11.820333 | Model-update-time: 0.000000


WORKLOAD-START | Type: DataUpdateWorkload | Progress: 2/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/Naru/eval_model.py', '--model=face', '--data_update=valueskew', '--dataset=bjaq', '--drift_test=js', '--end2end', '--update_size=80000', '--query_seed=0', '--eval_type=drift']
Torch device: cuda:1
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
ValueSample - START
ValueSample - END
load_npy_dataset_from_path - df.shape = (458601, 5)
Parsing... done, took 0.0s
Data updated!
Previous data size: (382168, 5), new data size: (76433, 5)
[9.6667409e-03 7.7486038e-07 7.1525574e-07 5.9604645e-07 6.5565109e-07]
Distance score: 0.0019338965648785233
SAUCE Drift detection: True
Detection latency: 0.0232s

DRIFT-DETECTED after 2-th workload
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/incremental_train.py', '--dataset=bjaq', '--end2end', '--model=face', '--model_update=adapt', '--update_size=80000', '--epochs=0']
Torch device: cuda:1
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
update_size: 76433
AdaptTask - START
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
train data shape: 80000, transfer data shape:16000
Training done; evaluating likelihood on full data:
Saved to: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
AdaptTask - END

WORKLOAD-FINISHED | Type: DataUpdateWorkload | Progress: 2/3 | Workload-time: 2.051933 | Model-update-time: 2.206953


WORKLOAD-START | Type: QueryWorkload | Progress: 3/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/evaluate/Evaluate-FACE-end2end.py', '--dataset=bjaq', '--query_seed=0', '--end2end', '--num_workload=3', '--data_update=valueskew', '--model_update=adapt']
/home/kangping/anaconda3/envs/LndCar/lib/python3.9/site-packages/torchquad/integration/vegas_map.py:314: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/BucketizationUtils.h:33.)
  iy = torch.searchsorted(self.x_edges[d, :], x[:, d], side='right')
DEVICE NAME
 Tesla V100S-PCIE-32GB
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
data shape: (458601, 5)
Save oracle results to :
./FACE/evaluate/oracle/bjaq_rng-0.csv
Found oracle card!
torch.Size([5, 2])
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
Took  8.173386096954346
tensor(0.9981)
result is  tensor(457717.6250)
Enter testHyper
ReportEsts: [1.0564537048339844, 1.027653694152832, 1.0291545391082764, 1.0775933265686035, 1.2481448650360107, 1.180801272392273, 1.2905128002166748, 1.0058317184448242, 1.4426844120025635, 1.3073652982711792, 1.0709365606307983, 1.146265983581543, 1.0023866891860962, 1.0146596431732178, 17.63385772705078, 1.2194241285324097, 1.1470816135406494, 1.0539367198944092, 1.1415021419525146, 1.2331167459487915, 1.039658546447754, 1.8320693969726562, 1.1491228342056274, 1.0710878372192383, 1.1509982347488403, 1.622631311416626, 1.0365103483200073, 1.0965160131454468, 1.0008488893508911, 1.0082987546920776, 1.1701204776763916, 1.2127659320831299, 1.0138828754425049, 1.210094928741455, 1.0270100831985474, 1.102203369140625, 1.0756747722625732, 1.4358464479446411, 1.1994013786315918, 1.3264484405517578, 1.022316575050354, 1.0395604372024536, 83.77777862548828, 1.07951819896698, 1.0615756511688232, 1.076676845550537, 1.1388211250305176, 1.169999122619629, 1.1440000534057617, 2.821162462234497, 1.0241400003433228, 1.14590585231781, 1.1023122072219849, 1.1094086170196533, 1.7744293212890625, 1.495595097541809, 1.2948073148727417, 1.1010640859603882, 1.0711160898208618, 1.1225684881210327, 1.5749385356903076, 1.1447699069976807, 1.6101511716842651, 1.6059321165084839, 1.0369120836257935, 1.0255885124206543, 1.2294104099273682, 1.2279324531555176, 1.187840461730957, 1.360625982284546, 1.0586638450622559, 1.5314692258834839, 1.176817536354065, 1.0040478706359863, 1.1268327236175537, 1.0453757047653198, 17.104883193969727, 1.3704299926757812, 1.0889544486999512, 1.0027637481689453, 1.5383563041687012, 1.0894546508789062, 1.2386597394943237, 1.1437058448791504, 1.200614094734192, 1.5446808338165283, 1.2210607528686523, 1.60063898563385, 1.2749820947647095, 1.308004379272461, 1.2586861848831177, 1.1158877611160278, 1.3611042499542236, 1.0781539678573608, 1.231311559677124, 1.0267542600631714, 1.9658396244049072, 1.064524531364441, 1.4581958055496216, 1.0441004037857056, 1.3243647813796997, 1.04347825050354, 1.0329114198684692, 1.003567099571228, 1.220908522605896, 1.5333489179611206, 1.0341336727142334, 1.0570255517959595, 1.1878453493118286, 1.0101584196090698, 1.4346153736114502, 1.1733921766281128, 1.26814866065979, 1.3787343502044678, 1.0263818502426147, 1.7651506662368774, 1.07379949092865, 1.198377251625061, 1.2275840044021606, 1.0959999561309814, 1.1767326593399048, 1.0895366668701172, 8.885766983032227, 5.0, 1.018072247505188, 1.1454617977142334, 1.2166824340820312, 1.0614173412322998, 1.2507227659225464, 1.0153249502182007, 1.1459906101226807, 1.0781573057174683, 1.1777758598327637, 1.462389349937439, 1.4883720874786377, 1.0147608518600464, 1.008690595626831, 1.0250710248947144, 1.2881909608840942, 1.1432404518127441, 1.0311602354049683, 1.0075790882110596, 1.300506591796875, 1.1344643831253052, 1.6463979482650757, 1.0829155445098877, 1.3825136423110962, 1.052954912185669, 1.2522997856140137, 1.0556057691574097, 1.171920657157898, 1.4504334926605225, 1.098696231842041, 1.0128941535949707, 1.0494788885116577, 1.125793218612671, 1.1122021675109863, 1.0959999561309814, 1.2169572114944458, 1.4399632215499878, 1.825379490852356, 1.26369047164917, 1.2885372638702393, 1.018476963043213, 1.1819162368774414, 2.9931271076202393, 1.0381855964660645, 1.0147027969360352, 38.33333206176758, 1.2764168977737427, 1.139406681060791, 1.1500402688980103, 1.3583123683929443, 1.1411237716674805, 1.0273884534835815, 1.046207308769226, 1.1110607385635376, 1.1823861598968506, 1.005257487297058, 1.1389484405517578, 1.3061224222183228, 1.1085309982299805, 1.0611932277679443, 1.2174242734909058, 1.8171545267105103, 1.2831056118011475, 7.437963008880615, 1.3544467687606812, 1.4110747575759888, 1.1800175905227661, 2.2724525928497314, 1.0234098434448242, 1.1318106651306152, 1.621858835220337, 1.0230178833007812, 1.1476885080337524, 1.2101171016693115, 1.065528392791748, 1.3570220470428467, 1.0621696710586548]
********** total_n=[200] batchn=[100]  N=[3200]  nitr=[4]  alpha=[0.4]  beta=[0.2] ******
@ Average per query          [3.7057459354400635] ms
 --  Average per query NF    [1.3579392433166504] ms
 --  Average per query vegas [2.347806692123413] ms
Mean [2.070]  Median [1.147]  95th [1.981]  99th [17.841]  max [83.778]
Mean [2.070]  Median [1.147]  95th [1.981]  99th [17.841]  max [83.778]


WORKLOAD-FINISHED | Type: QueryWorkload | Progress: 3/3 | Workload-time: 12.378061 | Model-update-time: 0.000000


Experiment Summary: #data-update-times=1, #drift=1 | total-time=28.508262