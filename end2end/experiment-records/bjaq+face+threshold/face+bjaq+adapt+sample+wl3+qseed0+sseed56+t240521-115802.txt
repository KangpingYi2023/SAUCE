Input arguments = Namespace(data_update='sample', update_size=80000, dataset='bjaq', debug=False, drift_test='js', model_update='adapt', model='face', end2end=False, query_seed=0, random_seed=42, num_workload=3)
JSON-passed parameters = {'data_updater': {'skew_ratio': '1e-3'}, 'random_seed': 56, 'face': {'learning_rate': '1e-4', 'epochs': 0, 'factor': 'auto'}}
MODEL-PATH=/home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
DATASET-PATH=/home/kangping/code_copy/SAUCE/data/bjaq/end2end/bjaq.npy
Workloads: ['QueryWorkload', 'DataUpdateWorkload', 'QueryWorkload']

WORKLOAD-START | Type: QueryWorkload | Progress: 1/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/evaluate/Evaluate-FACE-end2end.py', '--dataset=bjaq', '--end2end']
/home/kangping/anaconda3/envs/LndCar/lib/python3.9/site-packages/torchquad/integration/vegas_map.py:314: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/BucketizationUtils.h:33.)
  iy = torch.searchsorted(self.x_edges[d, :], x[:, d], side='right')
DEVICE NAME
 Tesla V100S-PCIE-32GB
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
data shape: (382168, 5)
Save oracle results to :
./FACE/evaluate/oracle/bjaq_rng-1234.csv
Found oracle card!
torch.Size([5, 2])
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
Took  8.159538984298706
tensor(0.9979)
result is  tensor(381381.3125)
Enter testHyper
ReportEsts: [1.1299638748168945, 1.0376454591751099, 1.1010773181915283, 1.0125799179077148, 1.2960366010665894, 1.1205953359603882, 1.2059235572814941, 1.1651443243026733, 1.0460869073867798, 1.1295191049575806, 1.024978518486023, 1.1411017179489136, 1.031605839729309, 1.0625, 1.06696355342865, 1.2210404872894287, 1.4535338878631592, 1.1745842695236206, 1.1778539419174194, 1.015544056892395, 1.1877720355987549, 1.0323809385299683, 1.1007561683654785, 1.0528700351715088, 1.208854079246521, 1.129294753074646, 1.0768581628799438, 1.2147424221038818, 1.0919163227081299, 1.0725257396697998, 1.1826889514923096, 1.1164311170578003, 1.0937659740447998, 1.058443546295166, 1.1842777729034424, 1.128463864326477, 1.2590817213058472, 1.285341501235962, 1.0546118021011353, 1.1015568971633911, 1.0088417530059814, 1.1586841344833374, 1.0704500675201416, 1.0471107959747314, 1.0826114416122437, 1.0294784307479858, 1.1631159782409668, 1.1776015758514404, 1.0236042737960815, 1.0930626392364502, 1.2228116989135742, 1.3411839008331299, 1.046451210975647, 1.2370972633361816, 1.102556586265564, 1.0285117626190186, 1.2850466966629028, 1.1347928047180176, 1.0564745664596558, 1.4243589639663696, 1.1845505237579346, 1.16073739528656, 1.4581503868103027, 1.0052021741867065, 1.3692946434020996, 1.2571429014205933, 1.225806474685669, 1.0399824380874634, 1.0973687171936035, 1.2736842632293701, 1.1393216848373413, 1.1474969387054443, 1.2090816497802734, 1.087032437324524, 1.2395071983337402, 1.1916018724441528, 1.8976689577102661, 1.0356502532958984, 1.1557923555374146, 1.4278892278671265, 1.1757081747055054, 1.1243740320205688, 1.017302393913269, 1.2385257482528687, 1.3043066263198853, 1.1469089984893799, 1.2874277830123901, 1.0585119724273682, 1.007901906967163, 1.067501425743103, 1.0277856588363647, 1.2593116760253906, 1.0086548328399658, 1.298113226890564, 1.0833333730697632, 1.1075376272201538, 1.1905971765518188, 1.4181201457977295, 1.034482717514038, 1.225727915763855, 1.21875, 1.2203288078308105, 1.0733665227890015, 1.0014641284942627, 1.0149697065353394, 1.006786584854126, 1.2302244901657104, 1.1570680141448975, 1.0760825872421265, 1.0497945547103882, 1.251779556274414, 1.085185170173645, 1.1636914014816284, 1.0211273431777954, 1.0530891418457031, 1.065727710723877, 1.040704607963562, 1.0006452798843384, 1.264535665512085, 1.0725982189178467, 1.2329578399658203, 1.0882433652877808, 1.1828365325927734, 1.1376467943191528, 1.277616262435913, 1.2168798446655273, 1.0280042886734009, 1.204759955406189, 1.375779151916504, 1.0971944332122803, 1.0335854291915894, 1.127267837524414, 1.1783716678619385, 1.3571428060531616, 1.2619848251342773, 1.0430558919906616, 1.0073667764663696, 1.067642331123352, 1.0880227088928223, 1.2585526704788208, 1.127582311630249, 1.089600682258606, 1.149195909500122, 1.245477318763733, 1.0338034629821777, 1.014884352684021, 1.0614829063415527, 1.4754955768585205, 1.072068452835083, 1.0426136255264282, 1.156822919845581, 1.1635687351226807, 1.3256189823150635, 1.1381090879440308, 1.0263293981552124, 1.1502723693847656, 1.0318350791931152, 1.022228479385376, 1.0617905855178833, 1.1005669832229614, 1.040055274963379, 1.0435680150985718, 1.0524182319641113, 1.229658842086792, 1.3801169395446777, 1.0350819826126099, 1.2325063943862915, 1.2626938819885254, 1.0325132608413696, 1.0330359935760498, 1.0293123722076416, 1.1322009563446045, 1.1753246784210205, 1.1926099061965942, 1.045299768447876, 1.03373384475708, 1.0145772695541382, 1.0649374723434448, 1.1843132972717285, 1.0047234296798706, 1.126186728477478, 1.0527198314666748, 1.004328966140747, 1.1205312013626099, 1.0865628719329834, 1.1078224182128906, 1.0117992162704468, 1.2330130338668823, 1.028012990951538, 1.4149713516235352, 1.0630526542663574, 1.0346347093582153, 1.5278372764587402, 1.495990514755249, 1.326056957244873, 1.1313484907150269, 1.1385993957519531, 1.1343283653259277, 1.0126640796661377, 1.1010624170303345]
********** total_n=[200] batchn=[100]  N=[3200]  nitr=[4]  alpha=[0.4]  beta=[0.2] ******
@ Average per query          [3.744739294052124] ms
 --  Average per query NF    [1.3601958751678467] ms
 --  Average per query vegas [2.3845434188842773] ms
Mean [1.144]  Median [1.121]  95th [1.382]  99th [1.496]  max [1.898]
Mean [1.144]  Median [1.121]  95th [1.382]  99th [1.496]  max [1.898]


WORKLOAD-FINISHED | Type: QueryWorkload | Progress: 1/3 | Workload-time: 11.809967 | Model-update-time: 0.000000


WORKLOAD-START | Type: DataUpdateWorkload | Progress: 2/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/Naru/eval_model.py', '--model=face', '--data_update=sample', '--dataset=bjaq', '--drift_test=js', '--end2end', '--update_size=80000', '--query_seed=0', '--eval_type=drift']
Torch device: cuda:1
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
Sample - START
Sample - END
load_npy_dataset_from_path - df.shape = (458601, 5)
Parsing... done, took 0.0s
Data updated!
Previous data size: (382168, 5), new data size: (76433, 5)
[2.3841858e-07 2.3841858e-07 0.0000000e+00 5.9604645e-08 2.9802322e-07]
Distance score: 1.668930025289228e-07
SAUCE Drift detection: False
Detection latency: 0.0233s

WORKLOAD-FINISHED | Type: DataUpdateWorkload | Progress: 2/3 | Workload-time: 2.024891 | Model-update-time: 0.000000


WORKLOAD-START | Type: QueryWorkload | Progress: 3/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/evaluate/Evaluate-FACE-end2end.py', '--dataset=bjaq', '--query_seed=0', '--end2end', '--num_workload=3', '--data_update=sample', '--model_update=adapt']
/home/kangping/anaconda3/envs/LndCar/lib/python3.9/site-packages/torchquad/integration/vegas_map.py:314: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/BucketizationUtils.h:33.)
  iy = torch.searchsorted(self.x_edges[d, :], x[:, d], side='right')
DEVICE NAME
 Tesla V100S-PCIE-32GB
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
data shape: (458601, 5)
Save oracle results to :
./FACE/evaluate/oracle/bjaq_rng-0.csv
Found oracle card!
torch.Size([5, 2])
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
Took  8.190393447875977
tensor(0.9959)
result is  tensor(456736.)
Enter testHyper
ReportEsts: [1.0939621925354004, 1.0480461120605469, 1.1859503984451294, 1.0883941650390625, 1.1189254522323608, 1.3583192825317383, 1.193800926208496, 1.1461271047592163, 1.024193525314331, 1.0683584213256836, 1.0001373291015625, 1.3586901426315308, 1.0655391216278076, 1.0967426300048828, 1.1786197423934937, 1.050697922706604, 1.5055350065231323, 1.0851680040359497, 1.09932541847229, 1.351693868637085, 1.1311500072479248, 1.6419436931610107, 1.13979172706604, 1.0710145235061646, 1.0143532752990723, 1.2359423637390137, 1.0420819520950317, 1.10977303981781, 1.1357421875, 1.3289777040481567, 1.2402503490447998, 1.134751796722412, 1.0236080884933472, 1.0358774662017822, 1.0576121807098389, 1.0655689239501953, 1.0421926975250244, 1.1678409576416016, 1.267995834350586, 1.0182561874389648, 1.0546931028366089, 1.0615057945251465, 1.0399553775787354, 1.2110244035720825, 1.0251349210739136, 1.1761040687561035, 1.2730997800827026, 1.1363861560821533, 1.1673640012741089, 1.1799410581588745, 1.066917061805725, 1.0587838888168335, 1.1601974964141846, 1.0626617670059204, 1.0676755905151367, 1.525841236114502, 1.134124517440796, 1.085327386856079, 1.2208473682403564, 1.145738124847412, 1.4653244018554688, 1.029442548751831, 1.1041151285171509, 1.558931827545166, 1.0107712745666504, 1.0218441486358643, 1.2485055923461914, 1.062678575515747, 1.1226462125778198, 1.1453826427459717, 1.1345117092132568, 1.2302643060684204, 1.0520926713943481, 1.0138908624649048, 1.0231759548187256, 1.1447467803955078, 1.161149024963379, 1.0452213287353516, 1.3434842824935913, 1.1554744243621826, 1.2076034545898438, 1.1613723039627075, 1.0781253576278687, 1.1297889947891235, 1.2922018766403198, 1.3661972284317017, 1.150151014328003, 1.471895456314087, 1.1416912078857422, 1.2146931886672974, 1.1553784608840942, 1.309054970741272, 1.3448878526687622, 1.1616129875183105, 1.3145761489868164, 1.0390324592590332, 1.0158218145370483, 1.3140658140182495, 1.3887182474136353, 1.141632318496704, 1.2247605323791504, 1.1423165798187256, 1.2009804248809814, 1.0687382221221924, 1.0678300857543945, 1.3038294315338135, 1.0587071180343628, 1.1108850240707397, 1.113157868385315, 1.0365715026855469, 1.0135763883590698, 1.1126610040664673, 1.1240400075912476, 1.1402803659439087, 1.0111432075500488, 1.064517855644226, 1.105653166770935, 1.0415714979171753, 1.072150468826294, 1.038759708404541, 1.0823262929916382, 1.172897219657898, 1.074588418006897, 1.0027129650115967, 1.0478845834732056, 1.1940348148345947, 1.116052508354187, 1.2519810199737549, 1.0071609020233154, 1.147658348083496, 1.0831754207611084, 1.1838488578796387, 1.107474446296692, 1.7871853113174438, 1.597333312034607, 1.148593783378601, 1.07384192943573, 1.0492544174194336, 1.113121747970581, 1.0175118446350098, 1.0221949815750122, 1.2213249206542969, 1.0254716873168945, 1.039571762084961, 1.009644865989685, 1.0991100072860718, 1.3791208267211914, 1.0519397258758545, 1.156015157699585, 1.0901710987091064, 1.2464325428009033, 1.0357815027236938, 1.300084114074707, 1.1194857358932495, 1.0411757230758667, 1.3371989727020264, 1.175441026687622, 1.2222222089767456, 1.1228554248809814, 1.1213916540145874, 1.1214256286621094, 1.21077561378479, 1.149144172668457, 1.032091498374939, 1.0572600364685059, 1.0557454824447632, 1.0395575761795044, 1.1012753248214722, 1.2362240552902222, 1.2940402030944824, 1.164652705192566, 1.0854458808898926, 1.5725688934326172, 1.0128389596939087, 1.025246500968933, 1.0257409811019897, 1.0917431116104126, 1.2035106420516968, 1.1644967794418335, 1.018302321434021, 1.704545497894287, 1.2360122203826904, 1.00419020652771, 1.8505663871765137, 1.3022412061691284, 1.351983666419983, 1.2664932012557983, 1.0516222715377808, 1.245203971862793, 1.062715768814087, 1.129012942314148, 1.066917896270752, 1.3158937692642212, 1.9751825332641602, 1.0281736850738525, 1.108199119567871, 1.521371841430664, 1.1645314693450928, 1.466789722442627, 1.079030156135559]
********** total_n=[200] batchn=[100]  N=[3200]  nitr=[4]  alpha=[0.4]  beta=[0.2] ******
@ Average per query          [3.64454984664917] ms
 --  Average per query NF    [1.3543546199798584] ms
 --  Average per query vegas [2.2901952266693115] ms
Mean [1.165]  Median [1.123]  95th [1.506]  99th [1.788]  max [1.975]
Mean [1.165]  Median [1.123]  95th [1.506]  99th [1.788]  max [1.975]


WORKLOAD-FINISHED | Type: QueryWorkload | Progress: 3/3 | Workload-time: 12.216279 | Model-update-time: 0.000000


Experiment Summary: #data-update-times=1, #drift=0 | total-time=26.076121