Input arguments = Namespace(data_update='valueskew', update_size=80000, dataset='bjaq', debug=False, drift_test='js', model_update='adapt', model='face', end2end=False, query_seed=0, random_seed=42, num_workload=3)
JSON-passed parameters = {'data_updater': {'skew_ratio': '1e-3'}, 'random_seed': 81, 'face': {'learning_rate': '1e-4', 'epochs': 0, 'factor': 'auto'}}
MODEL-PATH=/home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
DATASET-PATH=/home/kangping/code_copy/SAUCE/data/bjaq/end2end/bjaq.npy
Workloads: ['QueryWorkload', 'DataUpdateWorkload', 'QueryWorkload']

WORKLOAD-START | Type: QueryWorkload | Progress: 1/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/evaluate/Evaluate-FACE-end2end.py', '--dataset=bjaq', '--end2end']
/home/kangping/anaconda3/envs/LndCar/lib/python3.9/site-packages/torchquad/integration/vegas_map.py:314: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/BucketizationUtils.h:33.)
  iy = torch.searchsorted(self.x_edges[d, :], x[:, d], side='right')
DEVICE NAME
 Tesla V100S-PCIE-32GB
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
data shape: (382168, 5)
Save oracle results to :
./FACE/evaluate/oracle/bjaq_rng-1234.csv
Found oracle card!
torch.Size([5, 2])
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
Took  8.153290271759033
tensor(0.9959)
result is  tensor(380582.9062)
Enter testHyper
ReportEsts: [1.1705983877182007, 1.087073564529419, 1.5017286539077759, 1.2076226472854614, 1.2339622974395752, 1.1243456602096558, 1.2906723022460938, 1.0101295709609985, 1.0939679145812988, 1.0704690217971802, 1.0129199028015137, 1.0771186351776123, 1.0293601751327515, 1.0223214626312256, 1.098636269569397, 1.4795207977294922, 1.3334587812423706, 1.162826657295227, 1.093393087387085, 1.0816326141357422, 1.0016705989837646, 1.1696124076843262, 1.1112028360366821, 1.0865882635116577, 1.1747937202453613, 1.2260397672653198, 1.045326590538025, 1.3850772380828857, 1.0661118030548096, 1.1410843133926392, 1.1936856508255005, 1.1149529218673706, 1.1226577758789062, 1.056029200553894, 1.1680119037628174, 1.1435589790344238, 1.3243526220321655, 1.3618707656860352, 1.0623668432235718, 1.0021556615829468, 1.0122699737548828, 1.238389015197754, 1.1365422010421753, 1.0253347158432007, 1.1147264242172241, 1.0907028913497925, 1.1624542474746704, 1.2997021675109863, 1.064210057258606, 1.1742808818817139, 1.2068063020706177, 1.4749737977981567, 1.00357186794281, 1.1642625331878662, 1.1889727115631104, 1.0338432788848877, 1.209345817565918, 1.104269027709961, 1.2934389114379883, 1.3755674362182617, 1.2670109272003174, 1.1650075912475586, 1.241262435913086, 1.190382957458496, 1.1619718074798584, 1.3142857551574707, 1.0528967380523682, 1.0248388051986694, 1.1101423501968384, 1.399999976158142, 1.0362027883529663, 1.0079666376113892, 1.2139666080474854, 1.1976768970489502, 1.132843017578125, 1.0609906911849976, 1.5547558069229126, 1.0162601470947266, 1.0142855644226074, 1.3954821825027466, 1.207471489906311, 1.0143756866455078, 1.009718894958496, 1.1905304193496704, 1.3071389198303223, 1.0826383829116821, 1.1039625406265259, 1.1255990266799927, 1.0382716655731201, 1.1138569116592407, 1.085260033607483, 1.573451280593872, 1.021637201309204, 1.2580571174621582, 1.0372340679168701, 1.1102107763290405, 1.1534751653671265, 1.2354825735092163, 1.01694917678833, 1.106637954711914, 1.378787875175476, 1.0365068912506104, 1.0087116956710815, 1.003673791885376, 1.0800209045410156, 1.0212271213531494, 1.1021531820297241, 1.2356021404266357, 1.0751276016235352, 1.0325725078582764, 1.324177861213684, 1.085185170173645, 1.276308298110962, 1.018805742263794, 1.1018893718719482, 1.0707547664642334, 1.0432804822921753, 1.0932542085647583, 1.3571932315826416, 1.0755928754806519, 1.195076823234558, 1.133158564567566, 1.1374942064285278, 1.1479278802871704, 1.2897286415100098, 1.2155386209487915, 1.1902035474777222, 1.3430554866790771, 1.307106614112854, 1.0928523540496826, 1.107262134552002, 1.15729558467865, 1.139838457107544, 1.4285714626312256, 1.0823533535003662, 1.0669467449188232, 1.0652005672454834, 1.1827641725540161, 1.0827741622924805, 1.2552493810653687, 1.1608343124389648, 1.1558948755264282, 1.1029289960861206, 1.2991726398468018, 1.0422675609588623, 1.0985946655273438, 1.072585940361023, 1.7294096946716309, 1.0350252389907837, 1.0644886493682861, 1.2771432399749756, 1.1634405851364136, 1.3291958570480347, 1.204540729522705, 1.0373820066452026, 1.0503287315368652, 1.0451107025146484, 1.0022510290145874, 1.0482063293457031, 1.1824530363082886, 1.206482172012329, 1.1377084255218506, 1.0601708889007568, 1.2510013580322266, 1.2791328430175781, 1.0451911687850952, 1.1128582954406738, 1.2455651760101318, 1.0745468139648438, 1.022508978843689, 1.091699242591858, 1.096745252609253, 1.1168831586837769, 1.1299114227294922, 1.2595367431640625, 1.2022626399993896, 1.0437318086624146, 1.1154142618179321, 1.230535864830017, 1.1555801630020142, 1.0037565231323242, 1.0187040567398071, 1.0412391424179077, 1.1440244913101196, 1.0789568424224854, 1.1490485668182373, 1.1000515222549438, 1.2254433631896973, 1.044774055480957, 1.221479058265686, 1.0641430616378784, 1.1833951473236084, 1.6074345111846924, 1.6159769296646118, 1.6923733949661255, 1.2022331953048706, 1.0654855966567993, 1.1176470518112183, 1.341263771057129, 1.0920982360839844]
********** total_n=[200] batchn=[100]  N=[3200]  nitr=[4]  alpha=[0.4]  beta=[0.2] ******
@ Average per query          [3.660902976989746] ms
 --  Average per query NF    [1.3620662689208984] ms
 --  Average per query vegas [2.2988367080688477] ms
Mean [1.159]  Median [1.120]  95th [1.401]  99th [1.617]  max [1.729]
Mean [1.159]  Median [1.120]  95th [1.401]  99th [1.617]  max [1.729]


WORKLOAD-FINISHED | Type: QueryWorkload | Progress: 1/3 | Workload-time: 11.779616 | Model-update-time: 0.000000


WORKLOAD-START | Type: DataUpdateWorkload | Progress: 2/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/Naru/eval_model.py', '--model=face', '--data_update=valueskew', '--dataset=bjaq', '--drift_test=js', '--end2end', '--update_size=80000', '--query_seed=0', '--eval_type=drift']
Torch device: cuda:1
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
ValueSample - START
ValueSample - END
load_npy_dataset_from_path - df.shape = (458601, 5)
Parsing... done, took 0.0s
Data updated!
Previous data size: (382168, 5), new data size: (76433, 5)
[1.0132790e-06 8.5171461e-03 1.0728836e-06 2.4437904e-06 2.3841858e-07]
Distance score: 0.0017043829429894686
SAUCE Drift detection: True
Detection latency: 0.0232s

DRIFT-DETECTED after 2-th workload
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/incremental_train.py', '--dataset=bjaq', '--end2end', '--model=face', '--model_update=adapt', '--update_size=80000', '--epochs=0']
Torch device: cuda:1
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
update_size: 76433
AdaptTask - START
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
train data shape: 80000, transfer data shape:16000
Training done; evaluating likelihood on full data:
Saved to: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
AdaptTask - END

WORKLOAD-FINISHED | Type: DataUpdateWorkload | Progress: 2/3 | Workload-time: 1.961112 | Model-update-time: 2.178578


WORKLOAD-START | Type: QueryWorkload | Progress: 3/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/evaluate/Evaluate-FACE-end2end.py', '--dataset=bjaq', '--query_seed=0', '--end2end', '--num_workload=3', '--data_update=valueskew', '--model_update=adapt']
/home/kangping/anaconda3/envs/LndCar/lib/python3.9/site-packages/torchquad/integration/vegas_map.py:314: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/BucketizationUtils.h:33.)
  iy = torch.searchsorted(self.x_edges[d, :], x[:, d], side='right')
DEVICE NAME
 Tesla V100S-PCIE-32GB
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
data shape: (458601, 5)
Save oracle results to :
./FACE/evaluate/oracle/bjaq_rng-0.csv
Found oracle card!
torch.Size([5, 2])
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
Took  8.149689674377441
tensor(0.9897)
result is  tensor(453894.4062)
Enter testHyper
ReportEsts: [1.0263921022415161, 1.263383150100708, 1.2569342851638794, 1.0903841257095337, 1.0232524871826172, 1.4536930322647095, 1.3288614749908447, 1.075758695602417, 1.0258724689483643, 7.511074066162109, 1.3365464210510254, 1.0032744407653809, 1.0929704904556274, 1.2729355096817017, 19.957637786865234, 1.006087303161621, 1.3472563028335571, 1.3213289976119995, 1.0265476703643799, 1.3484606742858887, 1.0699443817138672, 2.7905702590942383, 1.1571698188781738, 1.9568835496902466, 1.075984239578247, 1.1746233701705933, 1.0751264095306396, 1.3170320987701416, 1.8608769178390503, 1.358578085899353, 1.1812913417816162, 1.1691176891326904, 1.1858320236206055, 1.0169743299484253, 1.1972264051437378, 1.116841435432434, 1.3151187896728516, 1.1031556129455566, 1.0673701763153076, 1.0102728605270386, 1.3011257648468018, 1.2183908224105835, 1.32964026927948, 1.3711882829666138, 1.3663034439086914, 1.5, 1.644508957862854, 1.2023638486862183, 1.3333333730697632, 1.0733532905578613, 1.1627392768859863, 1.0074498653411865, 1.2111130952835083, 1.078863263130188, 1.0462467670440674, 1.5388717651367188, 1.0352789163589478, 1.3374922275543213, 1.5021939277648926, 1.1513042449951172, 1.3769062757492065, 1.024169921875, 1.9522184133529663, 1.2360485792160034, 1.0139381885528564, 33.580787658691406, 1.30159592628479, 1.012069582939148, 1.1552519798278809, 1.1183552742004395, 1.1267732381820679, 1.190010905265808, 1.1582098007202148, 1.236665964126587, 1.9995464086532593, 1.0101628303527832, 29.68202781677246, 1.366184949874878, 1.3102338314056396, 1.163014531135559, 1.2609385251998901, 1.1667351722717285, 1.1507102251052856, 1.2471283674240112, 1.354191780090332, 1.6382979154586792, 1.0126452445983887, 1.2050209045410156, 1.0154690742492676, 1.011639952659607, 1.0572614669799805, 1.109338641166687, 1.559259057044983, 1.1649342775344849, 48.62300491333008, 1.4261924028396606, 1.021597146987915, 1.1836597919464111, 1.0894848108291626, 1.0047204494476318, 1.0598340034484863, 1.329801321029663, 1.1421911716461182, 1.1433016061782837, 1.3617891073226929, 1.1860833168029785, 1.0960328578948975, 1.1160773038864136, 1.0200573205947876, 1.0223395824432373, 4.070724964141846, 1.0982277393341064, 1.2180155515670776, 1.1516005992889404, 1.2282990217208862, 1.1378463506698608, 1.1131188869476318, 1.280523657798767, 1.0172884464263916, 1.1764706373214722, 1.0636155605316162, 1.0779887437820435, 1.015260100364685, 1.5606497526168823, 1.0974340438842773, 1.0491904020309448, 1.0431365966796875, 1.2452830076217651, 1.424211025238037, 1.080291986465454, 1.1635661125183105, 1.0445899963378906, 1.0393738746643066, 1.3246644735336304, 1.442607045173645, 1.2124038934707642, 1.1192669868469238, 1.0237895250320435, 1.2875123023986816, 1.139819622039795, 1.315669298171997, 1.0688252449035645, 1.2304950952529907, 1.0851646661758423, 1.0223747491836548, 1.1534438133239746, 1.3105590343475342, 1.0600517988204956, 1.2108683586120605, 1.0387593507766724, 1.0671988725662231, 1.0393258333206177, 1.067865252494812, 1.0955567359924316, 1.366105318069458, 2.0642611980438232, 1.036074161529541, 1.0458015203475952, 1.4567927122116089, 1.1074473857879639, 1.385450005531311, 1.2934523820877075, 2.044318199157715, 1.021623969078064, 1.2067657709121704, 1.0697654485702515, 1.2917840480804443, 1.1071313619613647, 19.777143478393555, 1.3085986375808716, 1.081639289855957, 1.2535130977630615, 1.198803186416626, 1.1932622194290161, 1.509660005569458, 1.3103904724121094, 1.2580761909484863, 1.035578727722168, 1.0420587062835693, 1.3571112155914307, 1.850000023841858, 1.3359564542770386, 1.138677954673767, 1.363887071609497, 1.3412714004516602, 1.0892921686172485, 1.4615113735198975, 1.0008456707000732, 1.5218805074691772, 1.001395344734192, 1.1766828298568726, 1.1338497400283813, 1.2311087846755981, 1.5272154808044434, 1.0114208459854126, 1.1027764081954956, 1.264057993888855, 2.8995373249053955, 1.1634446382522583, 1.2049098014831543]
********** total_n=[200] batchn=[100]  N=[3200]  nitr=[4]  alpha=[0.4]  beta=[0.2] ******
@ Average per query          [3.7017714977264404] ms
 --  Average per query NF    [1.3560545444488525] ms
 --  Average per query vegas [2.345716953277588] ms
Mean [2.008]  Median [1.177]  95th [2.045]  99th [29.721]  max [48.623]
Mean [2.008]  Median [1.177]  95th [2.045]  99th [29.721]  max [48.623]


WORKLOAD-FINISHED | Type: QueryWorkload | Progress: 3/3 | Workload-time: 12.310971 | Model-update-time: 0.000000


Experiment Summary: #data-update-times=1, #drift=1 | total-time=28.273368