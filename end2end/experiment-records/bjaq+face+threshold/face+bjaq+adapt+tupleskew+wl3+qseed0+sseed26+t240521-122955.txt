Input arguments = Namespace(data_update='tupleskew', update_size=80000, dataset='bjaq', debug=False, drift_test='js', model_update='adapt', model='face', end2end=False, query_seed=0, random_seed=42, num_workload=3)
JSON-passed parameters = {'data_updater': {'skew_ratio': '1e-3'}, 'random_seed': 26, 'face': {'learning_rate': '1e-4', 'epochs': 0, 'factor': 'auto'}}
MODEL-PATH=/home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
DATASET-PATH=/home/kangping/code_copy/SAUCE/data/bjaq/end2end/bjaq.npy
Workloads: ['QueryWorkload', 'DataUpdateWorkload', 'QueryWorkload']

WORKLOAD-START | Type: QueryWorkload | Progress: 1/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/evaluate/Evaluate-FACE-end2end.py', '--dataset=bjaq', '--end2end']
/home/kangping/anaconda3/envs/LndCar/lib/python3.9/site-packages/torchquad/integration/vegas_map.py:314: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/BucketizationUtils.h:33.)
  iy = torch.searchsorted(self.x_edges[d, :], x[:, d], side='right')
DEVICE NAME
 Tesla V100S-PCIE-32GB
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
data shape: (382168, 5)
Save oracle results to :
./FACE/evaluate/oracle/bjaq_rng-1234.csv
Found oracle card!
torch.Size([5, 2])
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
Took  8.173705339431763
tensor(0.9970)
result is  tensor(381004.4062)
Enter testHyper
ReportEsts: [1.1862974166870117, 1.0606921911239624, 1.1366645097732544, 1.0848599672317505, 1.2731356620788574, 1.0904701948165894, 1.2186834812164307, 1.0021393299102783, 1.1009762287139893, 1.2358033657073975, 1.0473729372024536, 1.1627118587493896, 1.0867624282836914, 1.0290178060531616, 1.0112476348876953, 1.3354862928390503, 1.1517016887664795, 1.1539192199707031, 1.1293116807937622, 1.125, 1.098369836807251, 1.0955759286880493, 1.0927165746688843, 1.0281568765640259, 1.1967655420303345, 1.08589506149292, 1.0926238298416138, 1.3982683420181274, 1.0848383903503418, 1.0880887508392334, 1.2451224327087402, 1.2431384325027466, 1.0024774074554443, 1.0347230434417725, 1.0982539653778076, 1.0618585348129272, 1.478481411933899, 1.0878252983093262, 1.1092252731323242, 1.0177961587905884, 1.1104294061660767, 1.0850266218185425, 1.0314533710479736, 1.1355462074279785, 1.0574688911437988, 1.0544217824935913, 1.124705195426941, 1.4237357378005981, 1.0186270475387573, 1.0913705825805664, 1.1820513010025024, 1.3659642934799194, 1.0647714138031006, 1.2448850870132446, 1.0099351406097412, 1.018080711364746, 1.1794393062591553, 1.2740548849105835, 1.4499553442001343, 1.4329320192337036, 1.3648560047149658, 1.1944752931594849, 1.3076236248016357, 1.1107791662216187, 1.2941176891326904, 1.2571429014205933, 1.1665116548538208, 1.1146553754806519, 1.0665557384490967, 1.3894736766815186, 1.0619468688964844, 1.0412696599960327, 1.2322452068328857, 1.0878198146820068, 1.1286922693252563, 1.0702887773513794, 1.8850325345993042, 1.0141843557357788, 1.000259518623352, 1.2685463428497314, 1.1232163906097412, 1.0260192155838013, 1.005382776260376, 1.1914358139038086, 1.2682380676269531, 1.0465610027313232, 1.0169339179992676, 1.0543757677078247, 1.1072224378585815, 1.0811012983322144, 1.0689620971679688, 1.3833409547805786, 1.0142648220062256, 1.2638347148895264, 1.1206896305084229, 1.1165311336517334, 1.1475197076797485, 1.1706292629241943, 1.0166666507720947, 1.1795711517333984, 1.0963855981826782, 1.0112680196762085, 1.0130800008773804, 1.000732660293579, 1.0377614498138428, 1.0770864486694336, 1.1496272087097168, 1.1780104637145996, 1.043770432472229, 1.0790061950683594, 1.0826406478881836, 1.137864112854004, 1.085353136062622, 1.070611834526062, 1.0456706285476685, 1.2010581493377686, 1.0618914365768433, 1.113504409790039, 1.3019193410873413, 1.1103705167770386, 1.2640955448150635, 1.1183937788009644, 1.2139095067977905, 1.0688446760177612, 1.2684108018875122, 1.164878249168396, 1.0938767194747925, 1.213952660560608, 1.2896493673324585, 1.1095523834228516, 1.212472915649414, 1.1720041036605835, 1.1190180778503418, 1.4642857313156128, 1.1826183795928955, 1.0144394636154175, 1.0085824728012085, 1.185690999031067, 1.082828402519226, 1.1441388130187988, 1.1768131256103516, 1.052582025527954, 1.0054744482040405, 1.2594103813171387, 1.1704444885253906, 1.0018097162246704, 1.0450712442398071, 1.4076647758483887, 1.0314202308654785, 1.073224425315857, 1.2231221199035645, 1.1622868776321411, 1.3752093315124512, 1.0049867630004883, 1.080394983291626, 1.0152157545089722, 1.0446525812149048, 1.0002814531326294, 1.0360575914382935, 1.1540435552597046, 1.3423620462417603, 1.071745753288269, 1.0265175104141235, 1.2074742317199707, 1.1668727397918701, 1.0025404691696167, 1.0591198205947876, 1.273131251335144, 1.1090143918991089, 1.0533874034881592, 1.0644115209579468, 1.0853520631790161, 1.1818181276321411, 1.1465195417404175, 1.0919618606567383, 1.042673945426941, 1.0276968479156494, 1.261220097541809, 1.0522209405899048, 1.1322126388549805, 1.0390527248382568, 1.0226117372512817, 1.0095778703689575, 1.147088885307312, 1.1024991273880005, 1.1109936237335205, 1.017181634902954, 1.2562936544418335, 1.003798484802246, 1.4496345520019531, 1.0249831676483154, 1.012352705001831, 1.4388706684112549, 1.696530818939209, 1.7721023559570312, 1.2158092260360718, 1.1076282262802124, 1.1014492511749268, 1.057495355606079, 1.085026502609253]
********** total_n=[200] batchn=[100]  N=[3200]  nitr=[4]  alpha=[0.4]  beta=[0.2] ******
@ Average per query          [3.7559986114501953] ms
 --  Average per query NF    [1.365290880203247] ms
 --  Average per query vegas [2.3907077312469482] ms
Mean [1.144]  Median [1.105]  95th [1.408]  99th [1.697]  max [1.885]
Mean [1.144]  Median [1.105]  95th [1.408]  99th [1.697]  max [1.885]


WORKLOAD-FINISHED | Type: QueryWorkload | Progress: 1/3 | Workload-time: 11.834987 | Model-update-time: 0.000000


WORKLOAD-START | Type: DataUpdateWorkload | Progress: 2/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/Naru/eval_model.py', '--model=face', '--data_update=tupleskew', '--dataset=bjaq', '--drift_test=js', '--end2end', '--update_size=80000', '--query_seed=0', '--eval_type=drift']
Torch device: cuda:1
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
TupleSkewSample - START
0.001
TupleSkewSample - END
load_npy_dataset_from_path - df.shape = (458601, 5)
Parsing... done, took 0.0s
Data updated!
Previous data size: (382168, 5), new data size: (76433, 5)
[1.0025501e-04 1.4847517e-04 1.8888712e-04 1.3309717e-04 6.1988831e-06]
Distance score: 0.00011538267426658422
SAUCE Drift detection: True
Detection latency: 0.0229s

DRIFT-DETECTED after 2-th workload
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/incremental_train.py', '--dataset=bjaq', '--end2end', '--model=face', '--model_update=adapt', '--update_size=80000', '--epochs=0']
Torch device: cuda:1
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
update_size: 76433
AdaptTask - START
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
train data shape: 80000, transfer data shape:16000
Training done; evaluating likelihood on full data:
Saved to: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
AdaptTask - END

WORKLOAD-FINISHED | Type: DataUpdateWorkload | Progress: 2/3 | Workload-time: 1.996663 | Model-update-time: 2.232570


WORKLOAD-START | Type: QueryWorkload | Progress: 3/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/evaluate/Evaluate-FACE-end2end.py', '--dataset=bjaq', '--query_seed=0', '--end2end', '--num_workload=3', '--data_update=tupleskew', '--model_update=adapt']
/home/kangping/anaconda3/envs/LndCar/lib/python3.9/site-packages/torchquad/integration/vegas_map.py:314: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/BucketizationUtils.h:33.)
  iy = torch.searchsorted(self.x_edges[d, :], x[:, d], side='right')
DEVICE NAME
 Tesla V100S-PCIE-32GB
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
data shape: (458601, 5)
Save oracle results to :
./FACE/evaluate/oracle/bjaq_rng-0.csv
Found oracle card!
torch.Size([5, 2])
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
Took  8.223596572875977
tensor(0.9943)
result is  tensor(455996.2500)
Enter testHyper
ReportEsts: [1.3417452573776245, 1.0879827737808228, 1.5805991888046265, 1.0117297172546387, 1.0410162210464478, 1.2772769927978516, 1.1714304685592651, 1.0213727951049805, 1.11540949344635, 1.1905384063720703, 1.0930883884429932, 1.0862388610839844, 1.0692124366760254, 1.0938630104064941, 1.3039252758026123, 1.0005747079849243, 3.857142925262451, 1.1953908205032349, 1.087038278579712, 1.2810460329055786, 1.024376630783081, 1.2417391538619995, 1.0103424787521362, 1.2001569271087646, 1.2476476430892944, 1.1318572759628296, 1.03790283203125, 1.127337098121643, 1.4898585081100464, 1.2667245864868164, 1.216986060142517, 1.4576271772384644, 1.0120176076889038, 1.0311895608901978, 1.0184376239776611, 1.0708993673324585, 1.1640206575393677, 1.075887680053711, 1.0263676643371582, 1.037582516670227, 1.09990394115448, 1.28224778175354, 1.44748854637146, 1.0323777198791504, 1.0150338411331177, 1.1816233396530151, 1.098235011100769, 1.090680480003357, 1.409523844718933, 1.2306767702102661, 1.0556187629699707, 1.0097647905349731, 1.1165341138839722, 1.0791854858398438, 1.1232420206069946, 1.3022972345352173, 1.2222490310668945, 1.1257057189941406, 1.7232533693313599, 1.204024076461792, 1.2641509771347046, 1.0033680200576782, 1.1315675973892212, 1.3205235004425049, 1.046277403831482, 1.1733726263046265, 1.3167192935943604, 1.0335034132003784, 1.095633864402771, 1.0959455966949463, 1.2223237752914429, 1.403977394104004, 1.1130335330963135, 1.0237981081008911, 1.0570719242095947, 1.3948649168014526, 2.2299065589904785, 1.0628812313079834, 1.5097546577453613, 1.1686769723892212, 1.070938229560852, 1.4222028255462646, 1.0192935466766357, 1.2007579803466797, 1.36934232711792, 1.5744681358337402, 1.2522706985473633, 1.4402120113372803, 1.2937729358673096, 1.2303847074508667, 1.0446058511734009, 1.0878826379776, 1.3277965784072876, 1.1232154369354248, 1.0444718599319458, 1.0125529766082764, 1.0923405885696411, 1.2576671838760376, 1.1812788248062134, 1.1033204793930054, 1.1937228441238403, 1.1102254390716553, 1.0171568393707275, 1.3820801973342896, 1.0267553329467773, 1.5061296224594116, 1.0425091981887817, 1.0317896604537964, 1.0687679052352905, 1.0199718475341797, 1.1002379655838013, 1.0375787019729614, 1.0906540155410767, 1.3319603204727173, 1.0414568185806274, 1.079092025756836, 1.0879712104797363, 1.0772252082824707, 1.0567206144332886, 2.328000068664551, 1.0213899612426758, 1.3295949697494507, 1.0254652500152588, 1.9190291166305542, 1.1350314617156982, 1.0668820142745972, 1.0108627080917358, 1.3744149208068848, 1.0773299932479858, 1.0788767337799072, 1.0162383317947388, 1.0871012210845947, 1.1542458534240723, 1.1730506420135498, 1.8929293155670166, 1.080636739730835, 1.053856372833252, 1.1079926490783691, 1.083350658416748, 1.0355689525604248, 1.084579586982727, 1.0875279903411865, 1.1446855068206787, 1.057693362236023, 1.0424607992172241, 1.0607247352600098, 2.269662857055664, 1.0691242218017578, 1.2226572036743164, 1.1112743616104126, 1.0397404432296753, 1.0879087448120117, 1.1179720163345337, 1.0903427600860596, 1.0426043272018433, 1.2048503160476685, 1.0820280313491821, 1.0873016119003296, 1.066136360168457, 1.3252110481262207, 1.042571783065796, 1.1291667222976685, 1.3984410762786865, 1.0922006368637085, 1.2197715044021606, 1.0839649438858032, 1.1962929964065552, 1.563620686531067, 1.0208239555358887, 1.200230598449707, 1.3774584531784058, 1.1413151025772095, 1.2028018236160278, 1.1283193826675415, 1.1580076217651367, 1.005774974822998, 1.336043119430542, 1.0026029348373413, 1.0885703563690186, 1.0038347244262695, 1.763157844543457, 1.0349496603012085, 1.2383078336715698, 1.8256839513778687, 1.3715084791183472, 1.1837050914764404, 2.016282320022583, 1.0492546558380127, 1.3572369813919067, 1.0262356996536255, 1.1923741102218628, 1.0281295776367188, 1.0862890481948853, 1.6922043561935425, 1.113937258720398, 1.0103223323822021, 1.2249572277069092, 1.2679976224899292, 1.3966386318206787, 1.0461937189102173]
********** total_n=[200] batchn=[100]  N=[3200]  nitr=[4]  alpha=[0.4]  beta=[0.2] ******
@ Average per query          [3.7358927726745605] ms
 --  Average per query NF    [1.3572824001312256] ms
 --  Average per query vegas [2.378610372543335] ms
Mean [1.207]  Median [1.111]  95th [1.694]  99th [2.270]  max [3.857]
Mean [1.207]  Median [1.111]  95th [1.694]  99th [2.270]  max [3.857]


WORKLOAD-FINISHED | Type: QueryWorkload | Progress: 3/3 | Workload-time: 12.470219 | Model-update-time: 0.000000


Experiment Summary: #data-update-times=1, #drift=1 | total-time=28.586199