Input arguments = Namespace(data_update='tupleskew', update_size=80000, dataset='bjaq', debug=False, drift_test='js', model_update='adapt', model='face', end2end=False, query_seed=0, random_seed=42, num_workload=3)
JSON-passed parameters = {'data_updater': {'skew_ratio': '1e-3'}, 'random_seed': 18, 'face': {'learning_rate': '1e-4', 'epochs': 0, 'factor': 'auto'}}
MODEL-PATH=/home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
DATASET-PATH=/home/kangping/code_copy/SAUCE/data/bjaq/end2end/bjaq.npy
Workloads: ['QueryWorkload', 'DataUpdateWorkload', 'QueryWorkload']

WORKLOAD-START | Type: QueryWorkload | Progress: 1/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/evaluate/Evaluate-FACE-end2end.py', '--dataset=bjaq', '--end2end']
/home/kangping/anaconda3/envs/LndCar/lib/python3.9/site-packages/torchquad/integration/vegas_map.py:314: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/BucketizationUtils.h:33.)
  iy = torch.searchsorted(self.x_edges[d, :], x[:, d], side='right')
DEVICE NAME
 Tesla V100S-PCIE-32GB
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
data shape: (382168, 5)
Save oracle results to :
./FACE/evaluate/oracle/bjaq_rng-1234.csv
Found oracle card!
torch.Size([5, 2])
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
Took  8.207505941390991
tensor(0.9946)
result is  tensor(380104.2188)
Enter testHyper
ReportEsts: [1.176351547241211, 1.0412026643753052, 1.3888624906539917, 1.061065435409546, 1.2547225952148438, 1.1355520486831665, 1.2786699533462524, 1.052587866783142, 1.1118299961090088, 1.1932207345962524, 1.0430662631988525, 1.1919491291046143, 1.0890066623687744, 1.0758928060531616, 1.0815907716751099, 1.1796519756317139, 1.1689395904541016, 1.1973872184753418, 1.2010481357574463, 1.0892857313156128, 1.0755109786987305, 1.0267935991287231, 1.1447396278381348, 1.1273778676986694, 1.0754716396331787, 1.1663652658462524, 1.062218427658081, 1.3571428060531616, 1.0368455648422241, 1.041419506072998, 1.1195459365844727, 1.1017402410507202, 1.0272878408432007, 1.0590789318084717, 1.1993494033813477, 1.146489143371582, 1.4085044860839844, 1.301658034324646, 1.008186936378479, 1.223473072052002, 1.024032711982727, 1.131954550743103, 1.0510367155075073, 1.001801609992981, 1.0509190559387207, 1.0137931108474731, 1.1357789039611816, 1.2575647830963135, 1.2681875228881836, 1.074450135231018, 1.2195767164230347, 1.4131455421447754, 1.0435512065887451, 1.1894736289978027, 1.005419373512268, 1.0501461029052734, 1.2009345293045044, 1.0739824771881104, 1.2270042896270752, 1.3163506984710693, 1.4515399932861328, 1.201271891593933, 1.3761695623397827, 1.1565320491790771, 1.4163089990615845, 1.2857142686843872, 1.1686859130859375, 1.0303293466567993, 1.115134358406067, 1.399999976158142, 1.015522837638855, 1.0458450317382812, 1.256831407546997, 1.095149040222168, 1.198192834854126, 1.0856072902679443, 2.2579612731933594, 1.0371907949447632, 1.056005597114563, 1.422416090965271, 1.151736855506897, 1.0163140296936035, 1.0209128856658936, 1.1404849290847778, 1.2433514595031738, 1.0754377841949463, 1.047379493713379, 1.0089281797409058, 1.04677152633667, 1.080354928970337, 1.0409445762634277, 1.3034065961837769, 1.0144248008728027, 1.247506856918335, 1.0714285373687744, 1.057434320449829, 1.1429263353347778, 1.1968967914581299, 1.034482717514038, 1.0749930143356323, 1.0650194883346558, 1.0418634414672852, 1.0758963823318481, 1.00737464427948, 1.0271966457366943, 1.0141782760620117, 1.0851387977600098, 1.1902269124984741, 1.0650606155395508, 1.0425972938537598, 1.0365982055664062, 1.138969898223877, 1.1681729555130005, 1.0101433992385864, 1.0563347339630127, 1.0133928060531616, 1.0136487483978271, 1.0011621713638306, 1.181684136390686, 1.1270830631256104, 1.2499032020568848, 1.0783772468566895, 1.1021537780761719, 1.141257882118225, 1.2940891981124878, 1.1762278079986572, 1.1145652532577515, 1.1747196912765503, 1.327319622039795, 1.1349365711212158, 1.234570026397705, 1.0265532732009888, 1.3036047220230103, 1.5, 1.21648108959198, 1.200314998626709, 1.0497595071792603, 1.1635771989822388, 1.0891735553741455, 1.1919002532958984, 1.1676539182662964, 1.1284743547439575, 1.0906167030334473, 1.131407618522644, 1.065260887145996, 1.014256477355957, 1.089726209640503, 1.3923356533050537, 1.105485200881958, 1.0791903734207153, 1.1605632305145264, 1.1713882684707642, 1.2741851806640625, 1.0621883869171143, 1.1281805038452148, 1.0334982872009277, 1.0517648458480835, 1.0067988634109497, 1.0683512687683105, 1.1887197494506836, 1.1584622859954834, 1.1058090925216675, 1.0240507125854492, 1.30501389503479, 1.187421441078186, 1.0286738872528076, 1.0682215690612793, 1.3094432353973389, 1.0849604606628418, 1.0440930128097534, 1.0195122957229614, 1.1261248588562012, 1.097402572631836, 1.1706515550613403, 1.043256163597107, 1.1000308990478516, 1.0874634981155396, 1.4311496019363403, 1.0843749046325684, 1.18349289894104, 1.1111241579055786, 1.1145974397659302, 1.0113290548324585, 1.2216547727584839, 1.1379934549331665, 1.1236786842346191, 1.0042774677276611, 1.2664786577224731, 1.129521131515503, 1.2625453472137451, 1.102660894393921, 1.0105204582214355, 1.54520845413208, 1.3891340494155884, 1.8641974925994873, 1.2022331953048706, 1.0293455123901367, 1.1875, 1.140155553817749, 1.0106241703033447]
********** total_n=[200] batchn=[100]  N=[3200]  nitr=[4]  alpha=[0.4]  beta=[0.2] ******
@ Average per query          [3.7603163719177246] ms
 --  Average per query NF    [1.36519193649292] ms
 --  Average per query vegas [2.3951244354248047] ms
Mean [1.148]  Median [1.115]  95th [1.400]  99th [1.548]  max [2.258]
Mean [1.148]  Median [1.115]  95th [1.400]  99th [1.548]  max [2.258]


WORKLOAD-FINISHED | Type: QueryWorkload | Progress: 1/3 | Workload-time: 11.865539 | Model-update-time: 0.000000


WORKLOAD-START | Type: DataUpdateWorkload | Progress: 2/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/Naru/eval_model.py', '--model=face', '--data_update=tupleskew', '--dataset=bjaq', '--drift_test=js', '--end2end', '--update_size=80000', '--query_seed=0', '--eval_type=drift']
Torch device: cuda:1
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
TupleSkewSample - START
0.001
TupleSkewSample - END
load_npy_dataset_from_path - df.shape = (458601, 5)
Parsing... done, took 0.0s
Data updated!
Previous data size: (382168, 5), new data size: (76433, 5)
[3.01837921e-04 6.90817833e-05 1.05679035e-04 6.31213188e-05
 1.22845173e-04]
Distance score: 0.0001325130433542654
SAUCE Drift detection: True
Detection latency: 0.0228s

DRIFT-DETECTED after 2-th workload
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/incremental_train.py', '--dataset=bjaq', '--end2end', '--model=face', '--model_update=adapt', '--update_size=80000', '--epochs=0']
Torch device: cuda:1
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
update_size: 76433
AdaptTask - START
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
train data shape: 80000, transfer data shape:16000
Training done; evaluating likelihood on full data:
Saved to: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
AdaptTask - END

WORKLOAD-FINISHED | Type: DataUpdateWorkload | Progress: 2/3 | Workload-time: 2.003994 | Model-update-time: 2.229058


WORKLOAD-START | Type: QueryWorkload | Progress: 3/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/evaluate/Evaluate-FACE-end2end.py', '--dataset=bjaq', '--query_seed=0', '--end2end', '--num_workload=3', '--data_update=tupleskew', '--model_update=adapt']
/home/kangping/anaconda3/envs/LndCar/lib/python3.9/site-packages/torchquad/integration/vegas_map.py:314: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/BucketizationUtils.h:33.)
  iy = torch.searchsorted(self.x_edges[d, :], x[:, d], side='right')
DEVICE NAME
 Tesla V100S-PCIE-32GB
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
data shape: (458601, 5)
Save oracle results to :
./FACE/evaluate/oracle/bjaq_rng-0.csv
Found oracle card!
torch.Size([5, 2])
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
Took  8.221947193145752
tensor(0.9931)
result is  tensor(455420.2500)
Enter testHyper
ReportEsts: [1.0858397483825684, 1.095118761062622, 1.0143678188323975, 1.0326534509658813, 1.090286135673523, 1.1412923336029053, 1.2014669179916382, 1.077649474143982, 1.0163273811340332, 1.1735808849334717, 1.1436206102371216, 1.298387050628662, 1.0930787324905396, 1.0630069971084595, 1.1342861652374268, 1.0145972967147827, 2.550724744796753, 1.076064944267273, 1.041144847869873, 1.3340870141983032, 1.1253968477249146, 1.3809057474136353, 1.2943006753921509, 1.1340832710266113, 1.1046338081359863, 1.0780284404754639, 1.0596444606781006, 1.1498161554336548, 1.073542594909668, 1.6380952596664429, 1.1180124282836914, 1.8474576473236084, 1.022802710533142, 1.0151888132095337, 1.000498652458191, 1.2184505462646484, 1.1354548931121826, 1.1358473300933838, 1.2213698625564575, 1.1166603565216064, 1.0184357166290283, 1.1877394914627075, 1.1154760122299194, 1.0310606956481934, 1.0237517356872559, 1.1332067251205444, 1.1733808517456055, 1.047531008720398, 1.2619047164916992, 1.0533241033554077, 1.092341423034668, 1.0076531171798706, 1.0517487525939941, 1.1362992525100708, 1.0601823329925537, 1.3264973163604736, 1.221571445465088, 1.162378191947937, 1.3187952041625977, 1.167943000793457, 1.1143450736999512, 1.0366415977478027, 1.1911357641220093, 1.010283350944519, 1.076721429824829, 1.045108675956726, 1.2779934406280518, 1.0718753337860107, 1.143770694732666, 1.0912712812423706, 1.1662189960479736, 1.1181321144104004, 1.0581039190292358, 1.125718116760254, 1.1846084594726562, 1.632956624031067, 1.0454888343811035, 1.2732830047607422, 1.3047513961791992, 1.0339876413345337, 1.0315279960632324, 1.071084976196289, 1.0826785564422607, 1.1277971267700195, 1.3676612377166748, 1.2396694421768188, 1.043208360671997, 1.4295495748519897, 1.2281635999679565, 1.071483850479126, 1.305676817893982, 1.0429692268371582, 1.4571750164031982, 1.2067207098007202, 1.0074594020843506, 1.0670886039733887, 1.086357831954956, 1.1890941858291626, 1.1241981983184814, 1.2297221422195435, 1.108994722366333, 1.1607986688613892, 1.093137264251709, 1.0523701906204224, 1.0316725969314575, 1.016736388206482, 1.0052329301834106, 1.166980504989624, 1.0200573205947876, 1.0553816556930542, 1.0326766967773438, 1.144454002380371, 1.0942524671554565, 1.3975651264190674, 1.36778724193573, 1.073158860206604, 1.1476314067840576, 1.2127125263214111, 1.1249288320541382, 2.9065420627593994, 1.18771493434906, 1.2666010856628418, 1.1438015699386597, 1.169742465019226, 1.1411138772964478, 1.3269919157028198, 1.2138384580612183, 1.4084278345108032, 1.0307711362838745, 1.033189058303833, 1.1937569379806519, 1.2023077011108398, 1.221043348312378, 1.5417789220809937, 1.6149871349334717, 1.022255539894104, 1.0685994625091553, 1.1107972860336304, 1.0434104204177856, 1.061378002166748, 1.1392802000045776, 1.2137500047683716, 1.0282294750213623, 1.034026026725769, 1.0315754413604736, 1.0454649925231934, 1.3105590343475342, 1.0704996585845947, 1.1731171607971191, 1.0485332012176514, 1.0552189350128174, 1.0055588483810425, 1.0731967687606812, 1.148455023765564, 1.016684889793396, 1.5068690776824951, 1.0907741785049438, 2.8016529083251953, 1.1129837036132812, 1.1821104288101196, 1.1257530450820923, 1.2683159112930298, 1.488930344581604, 1.148571491241455, 1.2279366254806519, 1.2164301872253418, 1.0171254873275757, 1.1890169382095337, 1.0071409940719604, 1.2604180574417114, 1.0734636783599854, 1.0460107326507568, 1.4194464683532715, 1.0743663311004639, 1.1773585081100464, 1.0328162908554077, 1.2330232858657837, 1.035436749458313, 1.0526823997497559, 1.067828893661499, 1.7894736528396606, 1.0280306339263916, 1.1527594327926636, 1.780102252960205, 1.362498164176941, 1.508848786354065, 1.1212000846862793, 1.2736636400222778, 1.3251398801803589, 1.0482696294784546, 1.2461981773376465, 1.1154546737670898, 1.078602910041809, 1.639694094657898, 1.0474860668182373, 1.3393394947052002, 1.4787769317626953, 1.1831461191177368, 1.3686786890029907, 1.1900959014892578]
********** total_n=[200] batchn=[100]  N=[3200]  nitr=[4]  alpha=[0.4]  beta=[0.2] ******
@ Average per query          [4.571740627288818] ms
 --  Average per query NF    [1.4874541759490967] ms
 --  Average per query vegas [3.0842864513397217] ms
Mean [1.190]  Median [1.126]  95th [1.545]  99th [2.553]  max [2.907]
Mean [1.190]  Median [1.126]  95th [1.545]  99th [2.553]  max [2.907]


WORKLOAD-FINISHED | Type: QueryWorkload | Progress: 3/3 | Workload-time: 12.665819 | Model-update-time: 0.000000


Experiment Summary: #data-update-times=1, #drift=1 | total-time=28.812670