Input arguments = Namespace(data_update='tupleskew', update_size=80000, dataset='bjaq', debug=False, drift_test='js', model_update='adapt', model='face', end2end=False, query_seed=0, random_seed=42, num_workload=3)
JSON-passed parameters = {'data_updater': {'skew_ratio': '1e-3'}, 'random_seed': 70, 'face': {'learning_rate': '1e-4', 'epochs': 0, 'factor': 'auto'}}
MODEL-PATH=/home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
DATASET-PATH=/home/kangping/code_copy/SAUCE/data/bjaq/end2end/bjaq.npy
Workloads: ['QueryWorkload', 'DataUpdateWorkload', 'QueryWorkload']

WORKLOAD-START | Type: QueryWorkload | Progress: 1/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/evaluate/Evaluate-FACE-end2end.py', '--dataset=bjaq', '--end2end']
/home/kangping/anaconda3/envs/LndCar/lib/python3.9/site-packages/torchquad/integration/vegas_map.py:314: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/BucketizationUtils.h:33.)
  iy = torch.searchsorted(self.x_edges[d, :], x[:, d], side='right')
DEVICE NAME
 Tesla V100S-PCIE-32GB
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
data shape: (382168, 5)
Save oracle results to :
./FACE/evaluate/oracle/bjaq_rng-1234.csv
Found oracle card!
torch.Size([5, 2])
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
Took  8.20942759513855
tensor(0.9959)
result is  tensor(380614.8125)
Enter testHyper
ReportEsts: [1.1154056787490845, 1.052095890045166, 1.3557826280593872, 1.0061440467834473, 1.2644259929656982, 1.0931998491287231, 1.2238634824752808, 1.1492897272109985, 1.1026581525802612, 1.0643908977508545, 1.1352282762527466, 1.1192796230316162, 1.015196681022644, 1.0135747194290161, 1.0198436975479126, 1.1557528972625732, 1.3627727031707764, 1.18277907371521, 1.1421021223068237, 1.0535714626312256, 1.3318939208984375, 1.1332063674926758, 1.1098289489746094, 1.072574257850647, 1.1157395839691162, 1.1871609687805176, 1.0560247898101807, 1.3715498447418213, 1.035784363746643, 1.1261316537857056, 1.1369279623031616, 1.0444161891937256, 1.1032588481903076, 1.122658371925354, 1.1581288576126099, 1.2261370420455933, 1.3309438228607178, 1.1649491786956787, 1.096845030784607, 1.0538922548294067, 1.0355439186096191, 1.1105467081069946, 1.0057255029678345, 1.0369319915771484, 1.1117684841156006, 1.064625859260559, 1.1080641746520996, 1.278286337852478, 1.1251779794692993, 1.1201353073120117, 1.1641414165496826, 1.3766742944717407, 1.054746389389038, 1.3685121536254883, 1.0986686944961548, 1.0313172340393066, 1.1934579610824585, 1.2370476722717285, 1.0517038106918335, 1.4098985195159912, 1.2508622407913208, 1.2188136577606201, 1.3822519779205322, 1.168853759765625, 1.3692946434020996, 1.1142857074737549, 1.1452054977416992, 1.0636159181594849, 1.104390263557434, 1.2947368621826172, 1.2316975593566895, 1.0303505659103394, 1.2955560684204102, 1.2553972005844116, 1.1763012409210205, 1.2592707872390747, 2.020711660385132, 1.060150384902954, 1.0426709651947021, 1.2678394317626953, 1.2469596862792969, 1.0429656505584717, 1.0968879461288452, 1.1881605386734009, 1.2855045795440674, 1.0569640398025513, 1.0009820461273193, 1.0718284845352173, 1.129296898841858, 1.103596568107605, 1.1305513381958008, 1.510544776916504, 1.0258971452713013, 1.3227589130401611, 1.1470588445663452, 1.1828809976577759, 1.131562352180481, 1.2458690404891968, 1.0526316165924072, 1.1980152130126953, 1.2097488641738892, 1.0698034763336182, 1.0845328569412231, 1.0081180334091187, 1.039582371711731, 1.0401630401611328, 1.1605298519134521, 1.1780104637145996, 1.0407663583755493, 1.0257439613342285, 1.0625518560409546, 1.1512770652770996, 1.13330078125, 1.0037119388580322, 1.1372435092926025, 1.207446813583374, 1.0480479001998901, 1.1774797439575195, 1.165474534034729, 1.0242960453033447, 1.1691110134124756, 1.108872652053833, 1.1927051544189453, 1.169551968574524, 1.2451550960540771, 1.190672755241394, 1.1192693710327148, 1.1012448072433472, 1.362433910369873, 1.1102204322814941, 1.136861801147461, 1.3003650903701782, 1.1522685289382935, 1.4642857313156128, 1.1223206520080566, 1.0611709356307983, 1.0430784225463867, 1.2344715595245361, 1.1390517950057983, 1.149639368057251, 1.1340534687042236, 1.0998361110687256, 1.1812973022460938, 1.3559578657150269, 1.0357033014297485, 1.0696016550064087, 1.0053433179855347, 1.3791892528533936, 1.252386212348938, 1.0313210487365723, 1.1793949604034424, 1.1511344909667969, 1.3577728271484375, 1.162382960319519, 1.2409439086914062, 1.0527931451797485, 1.0009974241256714, 1.0002814531326294, 1.031601071357727, 1.3241420984268188, 1.148457646369934, 1.0813342332839966, 1.032860517501831, 1.2645074129104614, 1.204081654548645, 1.0086262226104736, 1.0776790380477905, 1.276421308517456, 1.1481468677520752, 1.056602954864502, 1.1214709281921387, 1.065033197402954, 1.1753246784210205, 1.0545059442520142, 1.1192097663879395, 1.086413860321045, 1.0409711599349976, 1.2420082092285156, 1.1999225616455078, 1.0493581295013428, 1.0387778282165527, 1.1593823432922363, 1.0101317167282104, 1.175689458847046, 1.1307497024536133, 1.1627906560897827, 1.0822093486785889, 1.251044511795044, 1.0042616128921509, 1.3728841543197632, 1.0422974824905396, 1.074289321899414, 1.4213147163391113, 1.338559627532959, 1.7243067026138306, 1.1590908765792847, 1.0917882919311523, 1.085714340209961, 1.026268482208252, 1.0805777311325073]
********** total_n=[200] batchn=[100]  N=[3200]  nitr=[4]  alpha=[0.4]  beta=[0.2] ******
@ Average per query          [3.7650883197784424] ms
 --  Average per query NF    [1.3690805435180664] ms
 --  Average per query vegas [2.396007776260376] ms
Mean [1.155]  Median [1.131]  95th [1.372]  99th [1.513]  max [2.021]
Mean [1.155]  Median [1.131]  95th [1.372]  99th [1.513]  max [2.021]


WORKLOAD-FINISHED | Type: QueryWorkload | Progress: 1/3 | Workload-time: 11.874325 | Model-update-time: 0.000000


WORKLOAD-START | Type: DataUpdateWorkload | Progress: 2/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/Naru/eval_model.py', '--model=face', '--data_update=tupleskew', '--dataset=bjaq', '--drift_test=js', '--end2end', '--update_size=80000', '--query_seed=0', '--eval_type=drift']
Torch device: cuda:1
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
TupleSkewSample - START
0.001
TupleSkewSample - END
load_npy_dataset_from_path - df.shape = (458601, 5)
Parsing... done, took 0.0s
Data updated!
Previous data size: (382168, 5), new data size: (76433, 5)
[2.3233891e-04 4.9930811e-04 9.5367432e-06 7.4505806e-06 5.4776669e-05]
Distance score: 0.0001606822042958811
SAUCE Drift detection: True
Detection latency: 0.0229s

DRIFT-DETECTED after 2-th workload
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/incremental_train.py', '--dataset=bjaq', '--end2end', '--model=face', '--model_update=adapt', '--update_size=80000', '--epochs=0']
Torch device: cuda:1
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
update_size: 76433
AdaptTask - START
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
train data shape: 80000, transfer data shape:16000
Training done; evaluating likelihood on full data:
Saved to: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
AdaptTask - END

WORKLOAD-FINISHED | Type: DataUpdateWorkload | Progress: 2/3 | Workload-time: 2.000069 | Model-update-time: 2.221289


WORKLOAD-START | Type: QueryWorkload | Progress: 3/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/evaluate/Evaluate-FACE-end2end.py', '--dataset=bjaq', '--query_seed=0', '--end2end', '--num_workload=3', '--data_update=tupleskew', '--model_update=adapt']
/home/kangping/anaconda3/envs/LndCar/lib/python3.9/site-packages/torchquad/integration/vegas_map.py:314: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/BucketizationUtils.h:33.)
  iy = torch.searchsorted(self.x_edges[d, :], x[:, d], side='right')
DEVICE NAME
 Tesla V100S-PCIE-32GB
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
data shape: (458601, 5)
Save oracle results to :
./FACE/evaluate/oracle/bjaq_rng-0.csv
Found oracle card!
torch.Size([5, 2])
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
Took  8.172643899917603
tensor(0.9960)
result is  tensor(456783.5312)
Enter testHyper
ReportEsts: [1.0768691301345825, 1.0029150247573853, 1.0212464332580566, 1.0389668941497803, 1.0056657791137695, 1.0088087320327759, 1.1366205215454102, 1.1030923128128052, 1.0718470811843872, 1.0399551391601562, 1.0110688209533691, 1.0959888696670532, 1.0554156303405762, 1.0554693937301636, 1.1038774251937866, 1.0185045003890991, 1.043961763381958, 1.0877175331115723, 1.0548309087753296, 1.0709547996520996, 1.2389830350875854, 1.9433481693267822, 1.2882288694381714, 1.0868093967437744, 1.1063551902770996, 1.1882399320602417, 1.1538522243499756, 1.2041122913360596, 2.1464173793792725, 1.20895516872406, 1.2269939184188843, 1.381355881690979, 1.0903183221817017, 1.0339157581329346, 1.033384919166565, 1.2028682231903076, 1.1785963773727417, 1.0205199718475342, 1.5249722003936768, 1.1780591011047363, 1.146872639656067, 1.236270785331726, 2.3590033054351807, 1.1141945123672485, 1.0408319234848022, 1.1118296384811401, 1.0581269264221191, 1.10685396194458, 1.3952380418777466, 2.7931034564971924, 1.0213501453399658, 1.053297758102417, 1.0427570343017578, 1.099965214729309, 1.1294671297073364, 1.0106170177459717, 1.2690645456314087, 1.0005860328674316, 1.3053532838821411, 1.1071089506149292, 1.0167909860610962, 1.0217108726501465, 1.182803988456726, 1.1715734004974365, 1.1176624298095703, 1.1250720024108887, 1.26120924949646, 1.2094260454177856, 1.1907790899276733, 1.0086824893951416, 1.051233172416687, 1.1736021041870117, 1.2259379625320435, 1.0191009044647217, 1.1307402849197388, 1.0301047563552856, 1.027711272239685, 1.0094308853149414, 1.2606016397476196, 1.0207144021987915, 1.0564360618591309, 1.0097417831420898, 1.0970650911331177, 1.1619950532913208, 1.3095018863677979, 1.6382979154586792, 1.2085156440734863, 1.317647099494934, 1.1232483386993408, 1.276300311088562, 1.0047824382781982, 1.1959127187728882, 1.3034584522247314, 1.055384635925293, 1.183978796005249, 1.0661025047302246, 1.0583434104919434, 1.2532764673233032, 1.0897157192230225, 1.3906949758529663, 1.2038376331329346, 1.1832619905471802, 1.0980392694473267, 1.008799433708191, 1.0687265396118164, 1.1964713335037231, 1.0044219493865967, 1.156114101409912, 1.1260745525360107, 1.1890565156936646, 1.095145583152771, 1.1208001375198364, 1.1256202459335327, 1.15382719039917, 1.2975413799285889, 1.1781907081604004, 1.0790928602218628, 1.3538532257080078, 1.1059222221374512, 1.0796459913253784, 1.0968551635742188, 1.1709641218185425, 1.0968265533447266, 1.1113379001617432, 1.012886643409729, 1.1447635889053345, 1.1760183572769165, 1.053125023841858, 1.039062261581421, 1.0147004127502441, 1.06851327419281, 1.050666093826294, 1.0349581241607666, 1.9289740324020386, 1.43423593044281, 1.062448263168335, 1.0700088739395142, 1.0991122722625732, 1.0733215808868408, 1.0952198505401611, 1.0088940858840942, 1.092736840248108, 1.0749367475509644, 1.048482060432434, 1.0193042755126953, 1.1915898323059082, 1.2057143449783325, 1.1032418012619019, 1.098410725593567, 1.0584838390350342, 1.4788235425949097, 1.0915467739105225, 1.2600948810577393, 1.1160751581192017, 1.1360751390457153, 1.0060672760009766, 1.0407543182373047, 1.0620155334472656, 1.5086653232574463, 1.1902809143066406, 1.0691862106323242, 1.1674846410751343, 1.1827627420425415, 1.0038387775421143, 1.1970933675765991, 1.2087541818618774, 1.1795774698257446, 1.1150391101837158, 1.1235480308532715, 1.365648627281189, 1.0774599313735962, 1.1191763877868652, 1.380414366722107, 1.148321509361267, 1.0081576108932495, 1.0110363960266113, 1.2105722427368164, 1.314112901687622, 1.0779261589050293, 1.1489759683609009, 1.7894736528396606, 1.3325210809707642, 1.0207231044769287, 1.4417455196380615, 1.542673945426941, 1.141514778137207, 1.319591760635376, 1.1781976222991943, 1.3937008380889893, 1.1274601221084595, 1.2418125867843628, 1.0694340467453003, 1.0637391805648804, 2.1131985187530518, 1.0021405220031738, 1.346934199333191, 1.128499984741211, 1.0479962825775146, 1.2730768918991089, 1.209964394569397]
********** total_n=[200] batchn=[100]  N=[3200]  nitr=[4]  alpha=[0.4]  beta=[0.2] ******
@ Average per query          [3.7426018714904785] ms
 --  Average per query NF    [1.3633406162261963] ms
 --  Average per query vegas [2.3792612552642822] ms
Mean [1.177]  Median [1.115]  95th [1.509]  99th [2.149]  max [2.793]
Mean [1.177]  Median [1.115]  95th [1.509]  99th [2.149]  max [2.793]


WORKLOAD-FINISHED | Type: QueryWorkload | Progress: 3/3 | Workload-time: 12.369163 | Model-update-time: 0.000000


Experiment Summary: #data-update-times=1, #drift=1 | total-time=28.489941