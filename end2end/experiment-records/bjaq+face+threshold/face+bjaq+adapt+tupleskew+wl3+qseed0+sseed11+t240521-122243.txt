Input arguments = Namespace(data_update='tupleskew', update_size=80000, dataset='bjaq', debug=False, drift_test='js', model_update='adapt', model='face', end2end=False, query_seed=0, random_seed=42, num_workload=3)
JSON-passed parameters = {'data_updater': {'skew_ratio': '1e-3'}, 'random_seed': 11, 'face': {'learning_rate': '1e-4', 'epochs': 0, 'factor': 'auto'}}
MODEL-PATH=/home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
DATASET-PATH=/home/kangping/code_copy/SAUCE/data/bjaq/end2end/bjaq.npy
Workloads: ['QueryWorkload', 'DataUpdateWorkload', 'QueryWorkload']

WORKLOAD-START | Type: QueryWorkload | Progress: 1/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/evaluate/Evaluate-FACE-end2end.py', '--dataset=bjaq', '--end2end']
/home/kangping/anaconda3/envs/LndCar/lib/python3.9/site-packages/torchquad/integration/vegas_map.py:314: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/BucketizationUtils.h:33.)
  iy = torch.searchsorted(self.x_edges[d, :], x[:, d], side='right')
DEVICE NAME
 Tesla V100S-PCIE-32GB
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
data shape: (382168, 5)
Save oracle results to :
./FACE/evaluate/oracle/bjaq_rng-1234.csv
Found oracle card!
torch.Size([5, 2])
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
Took  8.163288593292236
tensor(0.9998)
result is  tensor(382085.5000)
Enter testHyper
ReportEsts: [1.0935232639312744, 1.104117751121521, 1.185968279838562, 1.1815118789672852, 1.2839021682739258, 1.1085540056228638, 1.252026081085205, 1.1929209232330322, 1.0706021785736084, 1.1952093839645386, 1.1274763345718384, 1.1815677881240845, 1.0791172981262207, 1.0647321939468384, 1.0218065977096558, 1.1070330142974854, 1.0690642595291138, 1.122209072113037, 1.1974351406097412, 1.0255101919174194, 1.0312179327011108, 1.101244568824768, 1.1503630876541138, 1.0247684717178345, 1.1141060590744019, 1.100361704826355, 1.0050675868988037, 1.4285714626312256, 1.0004042387008667, 1.1090428829193115, 1.2021993398666382, 1.1996289491653442, 1.076432228088379, 1.052079200744629, 1.2240982055664062, 1.1732333898544312, 1.3612440824508667, 1.093209147453308, 1.134984016418457, 1.052934169769287, 1.0432369709014893, 1.052370548248291, 1.1229681968688965, 1.0283446311950684, 1.077118158340454, 1.1190476417541504, 1.171019196510315, 1.309943675994873, 1.1607623100280762, 1.094754695892334, 1.188144326210022, 1.3193488121032715, 1.0683963298797607, 1.0865384340286255, 1.1145734786987305, 1.1012980937957764, 1.1943925619125366, 1.0677891969680786, 1.2459794282913208, 1.406922698020935, 1.394128441810608, 1.1388829946517944, 1.1771240234375, 1.1231491565704346, 1.226765751838684, 1.1714285612106323, 1.13279128074646, 1.0249555110931396, 1.0656718015670776, 1.5684210062026978, 1.0120675563812256, 1.0348776578903198, 1.2244746685028076, 1.1678134202957153, 1.0012985467910767, 1.1801657676696777, 1.9977011680603027, 1.0339041948318481, 1.1330008506774902, 1.5183390378952026, 1.1332311630249023, 1.0174447298049927, 1.018353819847107, 1.280860424041748, 1.244797706604004, 1.0488853454589844, 1.0073894262313843, 1.0778814554214478, 1.070221185684204, 1.1825193166732788, 1.0178381204605103, 1.3545756340026855, 1.0238009691238403, 1.324350357055664, 1.0833333730697632, 1.0357441902160645, 1.0981379747390747, 1.2576923370361328, 1.01694917678833, 1.104203701019287, 1.1835260391235352, 1.0566452741622925, 1.090377688407898, 1.0073206424713135, 1.0348325967788696, 1.0277560949325562, 1.1928882598876953, 1.165794014930725, 1.0611886978149414, 1.0370105504989624, 1.3242805004119873, 1.1004694700241089, 1.200250506401062, 1.0091733932495117, 1.168076992034912, 1.1522842645645142, 1.0310723781585693, 1.010576605796814, 1.311102032661438, 1.1144278049468994, 1.2317817211151123, 1.0784462690353394, 1.1832365989685059, 1.26382315158844, 1.2843992710113525, 1.167044997215271, 1.1496374607086182, 1.1975961923599243, 1.3672566413879395, 1.101202368736267, 1.2002813816070557, 1.0811418294906616, 1.3048477172851562, 1.3928571939468384, 1.207116961479187, 1.0487334728240967, 1.025552749633789, 1.166829228401184, 1.1328455209732056, 1.1366606950759888, 1.0917418003082275, 1.1471593379974365, 1.027904987335205, 1.3853416442871094, 1.0003899335861206, 1.021196722984314, 1.0949654579162598, 1.3564776182174683, 1.0143787860870361, 1.1003551483154297, 1.093212366104126, 1.1569029092788696, 1.2990506887435913, 1.1604032516479492, 1.0509027242660522, 1.1529176235198975, 1.0193527936935425, 1.0030951499938965, 1.0282585620880127, 1.2059683799743652, 1.278584599494934, 1.0366376638412476, 1.0498634576797485, 1.3271955251693726, 1.0850574970245361, 1.0248336791992188, 1.0936288833618164, 1.2704212665557861, 1.152976393699646, 1.0570377111434937, 1.0197521448135376, 1.0225354433059692, 1.1233766078948975, 1.272362470626831, 1.0967302322387695, 1.009686827659607, 1.0160349607467651, 1.1797431707382202, 1.0022082328796387, 1.16463303565979, 1.0281075239181519, 1.007838487625122, 1.008750081062317, 1.1624106168746948, 1.13473379611969, 1.1479915380477905, 1.014394998550415, 1.2607017755508423, 1.0047392845153809, 1.3689602613449097, 1.0305789709091187, 1.0089938640594482, 1.599775791168213, 1.5844604969024658, 1.6902084350585938, 1.2234848737716675, 1.1260942220687866, 1.1014492511749268, 1.1409943103790283, 1.070849895477295]
********** total_n=[200] batchn=[100]  N=[3200]  nitr=[4]  alpha=[0.4]  beta=[0.2] ******
@ Average per query          [3.648257255554199] ms
 --  Average per query NF    [1.3664400577545166] ms
 --  Average per query vegas [2.2818171977996826] ms
Mean [1.147]  Median [1.115]  95th [1.386]  99th [1.601]  max [1.998]
Mean [1.147]  Median [1.115]  95th [1.386]  99th [1.601]  max [1.998]


WORKLOAD-FINISHED | Type: QueryWorkload | Progress: 1/3 | Workload-time: 11.766071 | Model-update-time: 0.000000


WORKLOAD-START | Type: DataUpdateWorkload | Progress: 2/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/Naru/eval_model.py', '--model=face', '--data_update=tupleskew', '--dataset=bjaq', '--drift_test=js', '--end2end', '--update_size=80000', '--query_seed=0', '--eval_type=drift']
Torch device: cuda:1
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
TupleSkewSample - START
0.001
TupleSkewSample - END
load_npy_dataset_from_path - df.shape = (458601, 5)
Parsing... done, took 0.0s
Data updated!
Previous data size: (382168, 5), new data size: (76433, 5)
[1.7768145e-04 2.3221970e-04 8.1062317e-06 2.6321411e-04 8.6367130e-05]
Distance score: 0.00015351772890426219
SAUCE Drift detection: True
Detection latency: 0.0241s

DRIFT-DETECTED after 2-th workload
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/incremental_train.py', '--dataset=bjaq', '--end2end', '--model=face', '--model_update=adapt', '--update_size=80000', '--epochs=0']
Torch device: cuda:1
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
update_size: 76433
AdaptTask - START
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
train data shape: 80000, transfer data shape:16000
Training done; evaluating likelihood on full data:
Saved to: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
AdaptTask - END

WORKLOAD-FINISHED | Type: DataUpdateWorkload | Progress: 2/3 | Workload-time: 2.000544 | Model-update-time: 2.216679


WORKLOAD-START | Type: QueryWorkload | Progress: 3/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/evaluate/Evaluate-FACE-end2end.py', '--dataset=bjaq', '--query_seed=0', '--end2end', '--num_workload=3', '--data_update=tupleskew', '--model_update=adapt']
/home/kangping/anaconda3/envs/LndCar/lib/python3.9/site-packages/torchquad/integration/vegas_map.py:314: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/BucketizationUtils.h:33.)
  iy = torch.searchsorted(self.x_edges[d, :], x[:, d], side='right')
DEVICE NAME
 Tesla V100S-PCIE-32GB
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
data shape: (458601, 5)
Save oracle results to :
./FACE/evaluate/oracle/bjaq_rng-0.csv
Found oracle card!
torch.Size([5, 2])
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
Took  8.163296699523926
tensor(0.9950)
result is  tensor(456298.7188)
Enter testHyper
ReportEsts: [1.0820096731185913, 1.0237735509872437, 1.9112426042556763, 1.1658191680908203, 1.1083163022994995, 1.351567029953003, 1.143473744392395, 1.0472896099090576, 1.1223140954971313, 1.0803396701812744, 1.1257182359695435, 1.3958849906921387, 1.3262711763381958, 1.2607072591781616, 1.2080256938934326, 1.0981694459915161, 2.9889256954193115, 1.0674382448196411, 1.0486029386520386, 1.248892903327942, 1.182242512702942, 1.212934970855713, 1.0934932231903076, 1.035199761390686, 1.048864722251892, 1.2862870693206787, 1.1091089248657227, 1.1731557846069336, 1.333118200302124, 1.4657210111618042, 1.0336886644363403, 1.5593219995498657, 1.1500192880630493, 1.0720864534378052, 1.0724366903305054, 1.0965114831924438, 1.1623518466949463, 1.2239025831222534, 2.261261224746704, 1.0461220741271973, 1.0001283884048462, 1.154533863067627, 1.0997310876846313, 1.1035127639770508, 1.018269419670105, 1.1962718963623047, 4.28804349899292, 1.197174310684204, 1.2190476655960083, 1.1112202405929565, 1.043573260307312, 1.038130760192871, 1.120414137840271, 1.230529546737671, 1.0224852561950684, 1.4423893690109253, 1.3341847658157349, 1.1634525060653687, 1.3412253856658936, 1.110913634300232, 2.0280778408050537, 1.010201334953308, 1.157165288925171, 1.163749098777771, 1.1204500198364258, 1.0347071886062622, 1.3234620094299316, 1.1777551174163818, 1.19782292842865, 1.1565502882003784, 1.085869550704956, 1.17561674118042, 1.1496284008026123, 1.149375319480896, 1.1672052145004272, 1.168707013130188, 1.2534246444702148, 1.1096572875976562, 1.3160338401794434, 1.122287631034851, 1.0117782354354858, 1.0102534294128418, 1.131758689880371, 1.0469114780426025, 1.2517590522766113, 1.612765908241272, 1.4646472930908203, 1.2410516738891602, 1.10305917263031, 1.2523959875106812, 1.1215707063674927, 1.0334397554397583, 1.5505374670028687, 1.0875622034072876, 1.0776917934417725, 1.0033634901046753, 1.0259467363357544, 1.2441219091415405, 1.1126161813735962, 1.5488179922103882, 1.1408300399780273, 1.0755833387374878, 1.0563725233078003, 1.2458444833755493, 1.0189473628997803, 1.208182454109192, 1.0757954120635986, 1.0705537796020508, 1.0200573205947876, 1.1043728590011597, 1.085714340209961, 1.0613045692443848, 1.1268588304519653, 1.08518385887146, 1.3171099424362183, 1.039995789527893, 1.062966227531433, 1.3215539455413818, 1.031801700592041, 1.0442477464675903, 1.1692548990249634, 1.0830860137939453, 1.1575287580490112, 1.3123563528060913, 1.0575865507125854, 1.0089659690856934, 1.1397191286087036, 1.1140495538711548, 1.2584797143936157, 1.0764588117599487, 1.1139942407608032, 1.1734744310379028, 1.031497836112976, 1.4276458024978638, 1.6328740119934082, 1.2086235284805298, 1.1036919355392456, 1.223504662513733, 1.0711867809295654, 1.019201397895813, 1.0678107738494873, 1.094659686088562, 1.0116420984268188, 1.0698782205581665, 1.0305354595184326, 1.0590965747833252, 1.2634730339050293, 1.113258957862854, 1.026584506034851, 1.0832290649414062, 1.1601158380508423, 1.00308358669281, 1.1704610586166382, 1.040419340133667, 1.0117952823638916, 1.0762438774108887, 1.065458059310913, 1.0378787517547607, 1.5140122175216675, 1.2106682062149048, 1.096440076828003, 1.0909574031829834, 1.301675796508789, 1.237188696861267, 1.2452305555343628, 1.1240997314453125, 1.030202031135559, 1.126955270767212, 1.1849315166473389, 1.2836157083511353, 1.0299252271652222, 1.1964213848114014, 1.5636645555496216, 1.3312400579452515, 1.0690083503723145, 1.0027960538864136, 1.1419875621795654, 1.01909601688385, 1.0591883659362793, 1.0587998628616333, 1.736842155456543, 1.028562068939209, 1.0000332593917847, 1.694542407989502, 1.307468056678772, 1.0580182075500488, 1.9722627401351929, 1.1300252676010132, 1.313401699066162, 1.0044023990631104, 1.2172458171844482, 1.2211253643035889, 1.13773512840271, 2.2498817443847656, 1.0542622804641724, 1.2565741539001465, 1.2510267496109009, 1.2327535152435303, 1.0746753215789795, 1.024057149887085]
********** total_n=[200] batchn=[100]  N=[3200]  nitr=[4]  alpha=[0.4]  beta=[0.2] ******
@ Average per query          [3.7256181240081787] ms
 --  Average per query NF    [1.3552892208099365] ms
 --  Average per query vegas [2.370328903198242] ms
Mean [1.208]  Median [1.123]  95th [1.614]  99th [2.269]  max [4.288]
Mean [1.208]  Median [1.123]  95th [1.614]  99th [2.269]  max [4.288]


WORKLOAD-FINISHED | Type: QueryWorkload | Progress: 3/3 | Workload-time: 12.412412 | Model-update-time: 0.000000


Experiment Summary: #data-update-times=1, #drift=1 | total-time=28.420597