Input arguments = Namespace(data_update='tupleskew', update_size=80000, dataset='bjaq', debug=False, drift_test='js', model_update='adapt', model='face', end2end=False, query_seed=0, random_seed=42, num_workload=3)
JSON-passed parameters = {'data_updater': {'skew_ratio': '1e-3'}, 'random_seed': 27, 'face': {'learning_rate': '1e-4', 'epochs': 0, 'factor': 'auto'}}
MODEL-PATH=/home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
DATASET-PATH=/home/kangping/code_copy/SAUCE/data/bjaq/end2end/bjaq.npy
Workloads: ['QueryWorkload', 'DataUpdateWorkload', 'QueryWorkload']

WORKLOAD-START | Type: QueryWorkload | Progress: 1/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/evaluate/Evaluate-FACE-end2end.py', '--dataset=bjaq', '--end2end']
/home/kangping/anaconda3/envs/LndCar/lib/python3.9/site-packages/torchquad/integration/vegas_map.py:314: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/BucketizationUtils.h:33.)
  iy = torch.searchsorted(self.x_edges[d, :], x[:, d], side='right')
DEVICE NAME
 Tesla V100S-PCIE-32GB
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
data shape: (382168, 5)
Save oracle results to :
./FACE/evaluate/oracle/bjaq_rng-1234.csv
Found oracle card!
torch.Size([5, 2])
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
Took  8.16014838218689
tensor(0.9970)
result is  tensor(381036.5000)
Enter testHyper
ReportEsts: [1.0738980770111084, 1.066225290298462, 1.4196089506149292, 1.026953101158142, 1.2636741399765015, 1.0960347652435303, 1.3487657308578491, 1.1271204948425293, 1.0792464017868042, 1.0654758214950562, 1.0534021854400635, 1.1711864471435547, 1.0529632568359375, 1.046875, 1.0058296918869019, 1.352403163909912, 1.408342957496643, 1.1261283159255981, 1.1934669017791748, 1.0535714626312256, 1.0305927991867065, 1.0059682130813599, 1.1296141147613525, 1.06841242313385, 1.1280732154846191, 1.106690764427185, 1.0219594240188599, 1.5293560028076172, 1.0209147930145264, 1.0133252143859863, 1.22100031375885, 1.1699405908584595, 1.1288758516311646, 1.0088316202163696, 1.239087462425232, 1.1339982748031616, 1.2861082553863525, 1.0420465469360352, 1.0390759706497192, 1.1055209636688232, 1.017333745956421, 1.1384856700897217, 1.0131068229675293, 1.0226366519927979, 1.0851467847824097, 1.026077151298523, 1.0261119604110718, 1.1994502544403076, 1.2333742380142212, 1.094754695892334, 1.1850899457931519, 1.3739811182022095, 1.0146995782852173, 1.0862400531768799, 1.1321277618408203, 1.0377676486968994, 1.213084101676941, 1.0085421800613403, 1.0133925676345825, 1.37104070186615, 1.6585592031478882, 1.137439250946045, 1.4437074661254883, 1.1166526079177856, 1.1785714626312256, 1.1428571939468384, 1.1431175470352173, 1.0271050930023193, 1.10769784450531, 1.2947368621826172, 1.0619468688964844, 1.0055232048034668, 1.1115461587905884, 1.2282991409301758, 1.090696096420288, 1.2370065450668335, 1.8964170217514038, 1.0107247829437256, 1.0478894710540771, 1.3374143838882446, 1.3118481636047363, 1.0856081247329712, 1.0104626417160034, 1.2627310752868652, 1.2304842472076416, 1.1835918426513672, 1.0030872821807861, 1.0926103591918945, 1.0365351438522339, 1.009121298789978, 1.0074491500854492, 1.3036315441131592, 1.010818600654602, 1.2644153833389282, 1.0, 1.211585283279419, 1.0990324020385742, 1.3478494882583618, 1.0714285373687744, 1.189027190208435, 1.215133547782898, 1.0040874481201172, 1.036988615989685, 1.004392385482788, 1.1203974485397339, 1.0197455883026123, 1.1345031261444092, 1.1954624652862549, 1.0478785037994385, 1.0444926023483276, 1.016818881034851, 1.1109005212783813, 1.2684767246246338, 1.0127463340759277, 1.011823296546936, 1.1761658191680908, 1.0665390491485596, 1.0746808052062988, 1.2422189712524414, 1.0723567008972168, 1.3767590522766113, 1.0818959474563599, 1.1568313837051392, 1.1095014810562134, 1.2015503644943237, 1.177672266960144, 1.1204924583435059, 1.1401655673980713, 1.3093219995498657, 1.139278531074524, 1.09260892868042, 1.0734069347381592, 1.2526414394378662, 1.4642857313156128, 1.1922240257263184, 1.0249409675598145, 1.0589536428451538, 1.1697560548782349, 1.145265817642212, 1.3193103075027466, 1.1275324821472168, 1.0854893922805786, 1.011225938796997, 1.2926987409591675, 1.014072060585022, 1.0712451934814453, 1.277818202972412, 1.5260424613952637, 1.0590307712554932, 1.2639914751052856, 1.255710244178772, 1.2166389226913452, 1.274844765663147, 1.171262264251709, 1.1980574131011963, 1.0287882089614868, 1.0037237405776978, 1.0559931993484497, 1.1181927919387817, 1.2215458154678345, 1.1048483848571777, 1.1187926530838013, 1.0144480466842651, 1.329077959060669, 1.191919207572937, 1.051457166671753, 1.0903160572052002, 1.2734708786010742, 1.0586763620376587, 1.0069597959518433, 1.0543508529663086, 1.007453203201294, 1.1883116960525513, 1.2267179489135742, 1.0861716270446777, 1.2276400327682495, 1.0077745914459229, 1.2433419227600098, 1.3230488300323486, 1.1762458086013794, 1.1590442657470703, 1.1316108703613281, 1.0428392887115479, 1.062308430671692, 1.0731618404388428, 1.1057082414627075, 1.0361402034759521, 1.2673721313476562, 1.0704327821731567, 1.4212294816970825, 1.2537086009979248, 1.018649935722351, 1.5385444164276123, 1.2652599811553955, 1.9273080825805664, 1.2000000476837158, 1.1049187183380127, 1.0410958528518677, 1.0709420442581177, 1.1191234588623047]
********** total_n=[200] batchn=[100]  N=[3200]  nitr=[4]  alpha=[0.4]  beta=[0.2] ******
@ Average per query          [3.727571964263916] ms
 --  Average per query NF    [1.3593244552612305] ms
 --  Average per query vegas [2.3682475090026855] ms
Mean [1.151]  Median [1.114]  95th [1.409]  99th [1.661]  max [1.927]
Mean [1.151]  Median [1.114]  95th [1.409]  99th [1.661]  max [1.927]


WORKLOAD-FINISHED | Type: QueryWorkload | Progress: 1/3 | Workload-time: 11.803703 | Model-update-time: 0.000000


WORKLOAD-START | Type: DataUpdateWorkload | Progress: 2/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/Naru/eval_model.py', '--model=face', '--data_update=tupleskew', '--dataset=bjaq', '--drift_test=js', '--end2end', '--update_size=80000', '--query_seed=0', '--eval_type=drift']
Torch device: cuda:1
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
TupleSkewSample - START
0.001
TupleSkewSample - END
load_npy_dataset_from_path - df.shape = (458601, 5)
Parsing... done, took 0.0s
Data updated!
Previous data size: (382168, 5), new data size: (76433, 5)
[1.5377998e-05 1.2516975e-05 1.7720461e-04 1.9484758e-04 2.6166439e-05]
Distance score: 8.522272401023656e-05
SAUCE Drift detection: True
Detection latency: 0.0231s

DRIFT-DETECTED after 2-th workload
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/incremental_train.py', '--dataset=bjaq', '--end2end', '--model=face', '--model_update=adapt', '--update_size=80000', '--epochs=0']
Torch device: cuda:1
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
update_size: 76433
AdaptTask - START
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
train data shape: 80000, transfer data shape:16000
Training done; evaluating likelihood on full data:
Saved to: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
AdaptTask - END

WORKLOAD-FINISHED | Type: DataUpdateWorkload | Progress: 2/3 | Workload-time: 1.999498 | Model-update-time: 2.230470


WORKLOAD-START | Type: QueryWorkload | Progress: 3/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/evaluate/Evaluate-FACE-end2end.py', '--dataset=bjaq', '--query_seed=0', '--end2end', '--num_workload=3', '--data_update=tupleskew', '--model_update=adapt']
/home/kangping/anaconda3/envs/LndCar/lib/python3.9/site-packages/torchquad/integration/vegas_map.py:314: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/BucketizationUtils.h:33.)
  iy = torch.searchsorted(self.x_edges[d, :], x[:, d], side='right')
DEVICE NAME
 Tesla V100S-PCIE-32GB
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
data shape: (458601, 5)
Save oracle results to :
./FACE/evaluate/oracle/bjaq_rng-0.csv
Found oracle card!
torch.Size([5, 2])
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
Took  8.172711849212646
tensor(0.9929)
result is  tensor(455350.3750)
Enter testHyper
ReportEsts: [1.0830514430999756, 1.1057766675949097, 1.2911931276321411, 1.7060998678207397, 1.022376298904419, 1.129822850227356, 1.0088435411453247, 1.0651944875717163, 1.144998550415039, 1.004751443862915, 1.1523689031600952, 1.0948795080184937, 1.3650442361831665, 1.3304070234298706, 1.0810471773147583, 1.0078461170196533, 4.920000076293945, 1.120466947555542, 1.0432401895523071, 1.3147708177566528, 1.1867241859436035, 1.385102391242981, 1.5733073949813843, 1.0407027006149292, 1.0964783430099487, 1.0864238739013672, 1.0192689895629883, 1.1354838609695435, 1.440671443939209, 1.5157232284545898, 1.1852818727493286, 1.686813235282898, 1.02173912525177, 1.0011234283447266, 1.1097357273101807, 1.0693256855010986, 1.0157736539840698, 1.151337742805481, 1.189123511314392, 1.1650066375732422, 1.0991660356521606, 1.471222996711731, 1.0464304685592651, 1.0743507146835327, 1.0530732870101929, 1.152231216430664, 1.0694146156311035, 1.1652965545654297, 2.2564103603363037, 1.0282673835754395, 1.0105763673782349, 1.0304838418960571, 1.2115674018859863, 1.2525845766067505, 1.0951728820800781, 1.0531128644943237, 1.113738775253296, 1.0559055805206299, 1.3062005043029785, 1.0598082542419434, 1.2701421976089478, 1.0826760530471802, 1.1029417514801025, 1.045183539390564, 1.0586223602294922, 1.1289986371994019, 1.3788806200027466, 1.031615972518921, 1.1274648904800415, 1.0402246713638306, 1.1623367071151733, 1.2006160020828247, 1.0011169910430908, 1.1025354862213135, 1.0483999252319336, 1.1026626825332642, 1.7211077213287354, 1.046487808227539, 1.199180006980896, 1.143652319908142, 1.2185945510864258, 1.006974220275879, 1.2080607414245605, 1.2749011516571045, 1.4204955101013184, 1.557446837425232, 1.1734941005706787, 1.6861209869384766, 11.380952835083008, 1.007161259651184, 1.0389987230300903, 1.0904215574264526, 1.3402528762817383, 1.1163372993469238, 1.075441598892212, 1.0359632968902588, 1.0243518352508545, 1.194509506225586, 1.1872793436050415, 2.3687808513641357, 1.2158578634262085, 1.2236393690109253, 1.031862735748291, 1.0602201223373413, 1.148247241973877, 1.1040828227996826, 1.2042986154556274, 1.1727012395858765, 1.4972375631332397, 1.1561533212661743, 1.0450867414474487, 1.1274762153625488, 1.2724930047988892, 1.1341516971588135, 1.0629316568374634, 1.2515450716018677, 1.0472570657730103, 1.1294832229614258, 1.2537065744400024, 1.2212389707565308, 1.006179928779602, 1.2710280418395996, 1.0778213739395142, 1.4164557456970215, 1.1454071998596191, 1.1561881303787231, 1.2225350141525269, 1.0784000158309937, 1.0924509763717651, 1.0479601621627808, 1.0402532815933228, 1.0214393138885498, 1.0857032537460327, 1.3767186403274536, 1.5946236848831177, 1.1482017040252686, 1.0077017545700073, 1.051120400428772, 1.1156847476959229, 1.0167148113250732, 1.0793651342391968, 1.089728832244873, 1.0728996992111206, 1.0555860996246338, 1.0521677732467651, 1.0871813297271729, 2.373563289642334, 1.0583072900772095, 1.3153793811798096, 1.0192973613739014, 1.2640904188156128, 1.0245848894119263, 1.0931161642074585, 1.0522969961166382, 1.018992304801941, 1.085714340209961, 1.2183098793029785, 1.078740119934082, 1.3559541702270508, 1.0652427673339844, 1.1102018356323242, 1.0665944814682007, 1.2308158874511719, 1.0554003715515137, 1.16982102394104, 1.1531277894973755, 1.1337919235229492, 1.1150321960449219, 1.288751244544983, 1.1755740642547607, 1.0182679891586304, 1.1007492542266846, 1.7117563486099243, 1.2528810501098633, 1.0080113410949707, 1.021508812904358, 1.017837643623352, 1.0471522808074951, 1.0795214176177979, 1.0544507503509521, 1.8684210777282715, 1.1084564924240112, 1.0254936218261719, 1.5030651092529297, 1.2908943891525269, 1.1755139827728271, 1.5142663717269897, 1.156036615371704, 1.6146423816680908, 1.033750057220459, 1.147121787071228, 1.2054176330566406, 1.1572540998458862, 1.689346432685852, 1.0424325466156006, 1.2010301351547241, 1.4581496715545654, 1.0219241380691528, 1.1614035367965698, 1.1261810064315796]
********** total_n=[200] batchn=[100]  N=[3200]  nitr=[4]  alpha=[0.4]  beta=[0.2] ******
@ Average per query          [3.7554514408111572] ms
 --  Average per query NF    [1.3705635070800781] ms
 --  Average per query vegas [2.384887933731079] ms
Mean [1.256]  Median [1.116]  95th [1.687]  99th [2.399]  max [11.381]
Mean [1.256]  Median [1.116]  95th [1.687]  99th [2.399]  max [11.381]


WORKLOAD-FINISHED | Type: QueryWorkload | Progress: 3/3 | Workload-time: 12.399496 | Model-update-time: 0.000000


Experiment Summary: #data-update-times=1, #drift=1 | total-time=28.458250