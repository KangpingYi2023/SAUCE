Input arguments = Namespace(data_update='tupleskew', update_size=80000, dataset='bjaq', debug=False, drift_test='js', model_update='adapt', model='face', end2end=False, query_seed=0, random_seed=42, num_workload=3)
JSON-passed parameters = {'data_updater': {'skew_ratio': '1e-3'}, 'random_seed': 41, 'face': {'learning_rate': '1e-4', 'epochs': 0, 'factor': 'auto'}}
MODEL-PATH=/home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
DATASET-PATH=/home/kangping/code_copy/SAUCE/data/bjaq/end2end/bjaq.npy
Workloads: ['QueryWorkload', 'DataUpdateWorkload', 'QueryWorkload']

WORKLOAD-START | Type: QueryWorkload | Progress: 1/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/evaluate/Evaluate-FACE-end2end.py', '--dataset=bjaq', '--end2end']
/home/kangping/anaconda3/envs/LndCar/lib/python3.9/site-packages/torchquad/integration/vegas_map.py:314: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/BucketizationUtils.h:33.)
  iy = torch.searchsorted(self.x_edges[d, :], x[:, d], side='right')
DEVICE NAME
 Tesla V100S-PCIE-32GB
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
data shape: (382168, 5)
Save oracle results to :
./FACE/evaluate/oracle/bjaq_rng-1234.csv
Found oracle card!
torch.Size([5, 2])
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
Took  8.161799192428589
tensor(0.9951)
result is  tensor(380294.9688)
Enter testHyper
ReportEsts: [1.1024112701416016, 1.0562952756881714, 1.317690134048462, 1.0726468563079834, 1.2300347089767456, 1.0975956916809082, 1.3583097457885742, 1.2051714658737183, 1.0919818878173828, 1.112405776977539, 1.0921618938446045, 1.1294491291046143, 1.0806459188461304, 1.0245535373687744, 1.2076576948165894, 1.2733023166656494, 1.3496681451797485, 1.1249406337738037, 1.1580156087875366, 1.086734652519226, 1.0238641500473022, 1.101244568824768, 1.167823076248169, 1.0978951454162598, 1.302703619003296, 1.0406872034072876, 1.060247778892517, 1.4019097089767456, 1.0554832220077515, 1.1398637294769287, 1.211067795753479, 1.0725032091140747, 1.112951397895813, 1.0531805753707886, 1.1514577865600586, 1.2034372091293335, 1.2987146377563477, 1.0210096836090088, 1.041380763053894, 1.0329341888427734, 1.0016582012176514, 1.08780837059021, 1.03620183467865, 1.0652904510498047, 1.1233890056610107, 1.057554006576538, 1.103022813796997, 1.2664611339569092, 1.1085323095321655, 1.0439932346343994, 1.1354680061340332, 1.2816301584243774, 1.0015535354614258, 1.3366001844406128, 1.1725965738296509, 1.0709317922592163, 1.351401925086975, 1.1029330492019653, 1.1089913845062256, 1.4004201889038086, 1.1459840536117554, 1.1997994184494019, 1.2867814302444458, 1.1628105640411377, 1.2992125749588013, 1.1142857074737549, 1.0600168704986572, 1.0138187408447266, 1.1076915264129639, 1.2526315450668335, 1.0048270225524902, 1.07672917842865, 1.3197928667068481, 1.1588995456695557, 1.1255199909210205, 1.360147476196289, 1.893725872039795, 1.0231858491897583, 1.1023958921432495, 1.4921436309814453, 1.2511792182922363, 1.0898077487945557, 1.056931734085083, 1.3768123388290405, 1.2550169229507446, 1.0951435565948486, 1.012549877166748, 1.0017179250717163, 1.1056970357894897, 1.0421744585037231, 1.0432552099227905, 1.4175968170166016, 1.027417540550232, 1.3005671501159668, 1.1818181276321411, 1.1228028535842896, 1.1411542892456055, 1.2285200357437134, 1.0714285373687744, 1.17751145362854, 1.1390820741653442, 1.1970734596252441, 1.113669514656067, 1.0131771564483643, 1.0544979572296143, 1.084964394569397, 1.2617825269699097, 1.169284462928772, 1.0705195665359497, 1.068651556968689, 1.1611266136169434, 1.0943044424057007, 1.0919302701950073, 1.0214248895645142, 1.0585371255874634, 1.1073170900344849, 1.0227556228637695, 1.0624274015426636, 1.2869055271148682, 1.0284650325775146, 1.2532997131347656, 1.0880364179611206, 1.0538774728775024, 1.2923083305358887, 1.2843992710113525, 1.152084231376648, 1.0421438217163086, 1.2131637334823608, 1.2716048955917358, 1.1302604675292969, 1.1622999906539917, 1.1848005056381226, 1.1625232696533203, 1.3928571939468384, 1.103899359703064, 1.1113152503967285, 1.0039325952529907, 1.206829309463501, 1.0457344055175781, 1.225496530532837, 1.070237398147583, 1.1359858512878418, 1.0527212619781494, 1.3524744510650635, 1.001846194267273, 1.098022222518921, 1.0257679224014282, 1.480146050453186, 1.0564090013504028, 1.1676846742630005, 1.2361403703689575, 1.1594667434692383, 1.3256189823150635, 1.179356336593628, 1.1167508363723755, 1.061028003692627, 1.054648995399475, 1.0194147825241089, 1.0601294040679932, 1.1231274604797363, 1.1739349365234375, 1.0426071882247925, 1.0058144330978394, 1.2628031969070435, 1.2791328430175781, 1.0880578756332397, 1.2048918008804321, 1.2417858839035034, 1.040795922279358, 1.0020745992660522, 1.1287060976028442, 1.0386461019515991, 1.1103895902633667, 1.1284066438674927, 1.0446184873580933, 1.2430108785629272, 1.0568512678146362, 1.252217173576355, 1.2209352254867554, 1.0245132446289062, 1.0522887706756592, 1.0742847919464111, 1.025421380996704, 1.1154239177703857, 1.1104671955108643, 1.121564507484436, 1.0754808187484741, 1.2012704610824585, 1.0202244520187378, 1.3174529075622559, 1.0691057443618774, 1.0215498208999634, 1.4472616910934448, 1.4412016868591309, 1.64650297164917, 1.1157166957855225, 1.0156731605529785, 1.1515151262283325, 1.0359700918197632, 1.0114209651947021]
********** total_n=[200] batchn=[100]  N=[3200]  nitr=[4]  alpha=[0.4]  beta=[0.2] ******
@ Average per query          [3.7392079830169678] ms
 --  Average per query NF    [1.3693797588348389] ms
 --  Average per query vegas [2.369828224182129] ms
Mean [1.149]  Median [1.112]  95th [1.378]  99th [1.494]  max [1.894]
Mean [1.149]  Median [1.112]  95th [1.378]  99th [1.494]  max [1.894]


WORKLOAD-FINISHED | Type: QueryWorkload | Progress: 1/3 | Workload-time: 11.832917 | Model-update-time: 0.000000


WORKLOAD-START | Type: DataUpdateWorkload | Progress: 2/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/Naru/eval_model.py', '--model=face', '--data_update=tupleskew', '--dataset=bjaq', '--drift_test=js', '--end2end', '--update_size=80000', '--query_seed=0', '--eval_type=drift']
Torch device: cuda:1
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
TupleSkewSample - START
0.001
TupleSkewSample - END
load_npy_dataset_from_path - df.shape = (458601, 5)
Parsing... done, took 0.0s
Data updated!
Previous data size: (382168, 5), new data size: (76433, 5)
[3.0308962e-04 7.4458122e-04 9.6499920e-05 1.1193752e-04 1.3041496e-04]
Distance score: 0.00027730464353226125
SAUCE Drift detection: True
Detection latency: 0.0232s

DRIFT-DETECTED after 2-th workload
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/incremental_train.py', '--dataset=bjaq', '--end2end', '--model=face', '--model_update=adapt', '--update_size=80000', '--epochs=0']
Torch device: cuda:1
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
update_size: 76433
AdaptTask - START
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
train data shape: 80000, transfer data shape:16000
Training done; evaluating likelihood on full data:
Saved to: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
AdaptTask - END

WORKLOAD-FINISHED | Type: DataUpdateWorkload | Progress: 2/3 | Workload-time: 2.000922 | Model-update-time: 2.252538


WORKLOAD-START | Type: QueryWorkload | Progress: 3/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/evaluate/Evaluate-FACE-end2end.py', '--dataset=bjaq', '--query_seed=0', '--end2end', '--num_workload=3', '--data_update=tupleskew', '--model_update=adapt']
/home/kangping/anaconda3/envs/LndCar/lib/python3.9/site-packages/torchquad/integration/vegas_map.py:314: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/BucketizationUtils.h:33.)
  iy = torch.searchsorted(self.x_edges[d, :], x[:, d], side='right')
DEVICE NAME
 Tesla V100S-PCIE-32GB
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
data shape: (458601, 5)
Save oracle results to :
./FACE/evaluate/oracle/bjaq_rng-0.csv
Found oracle card!
torch.Size([5, 2])
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
Took  8.164484977722168
tensor(0.9962)
result is  tensor(456865.5312)
Enter testHyper
ReportEsts: [1.1060144901275635, 1.130373477935791, 1.2773925065994263, 1.1489607095718384, 1.030274510383606, 1.2215590476989746, 1.2519503831863403, 1.0331401824951172, 1.1821677684783936, 1.0336337089538574, 1.0490094423294067, 1.2719178199768066, 1.0371286869049072, 1.219298243522644, 1.1568061113357544, 1.075243353843689, 1.4863158464431763, 1.1414443254470825, 1.1523364782333374, 1.1318029165267944, 1.1775833368301392, 1.404421091079712, 1.117441177368164, 1.113618016242981, 1.0486031770706177, 1.2389106750488281, 1.0333431959152222, 1.2753623723983765, 1.2092640399932861, 1.3597064018249512, 1.198328971862793, 1.4253393411636353, 1.149436593055725, 1.1690962314605713, 1.0573564767837524, 1.0281263589859009, 1.2754629850387573, 1.2436836957931519, 1.1154886484146118, 1.033272624015808, 1.0936861038208008, 1.185185194015503, 1.1465837955474854, 1.0973122119903564, 1.0562950372695923, 1.1131151914596558, 1.1899750232696533, 1.0348342657089233, 1.578740119934082, 1.20162832736969, 1.0292112827301025, 1.0590596199035645, 1.0862542390823364, 1.1647108793258667, 1.0528550148010254, 1.0571194887161255, 1.1124935150146484, 1.133396029472351, 1.1787679195404053, 1.079079270362854, 1.6844444274902344, 1.044140338897705, 1.147152304649353, 1.3888773918151855, 1.0103811025619507, 1.058698296546936, 1.2639422416687012, 1.0062962770462036, 1.2077854871749878, 1.097519874572754, 1.1571152210235596, 1.2400556802749634, 1.0603744983673096, 1.1583172082901, 1.264267921447754, 1.0830861330032349, 1.0896474123001099, 1.0797470808029175, 1.2845540046691895, 1.0813580751419067, 1.0181584358215332, 1.1586582660675049, 1.0884771347045898, 1.1707874536514282, 1.313969612121582, 1.6042553186416626, 1.2022625207901, 1.9408133029937744, 1.124412178993225, 1.391845941543579, 1.1884983777999878, 1.1703609228134155, 1.409543752670288, 1.0651767253875732, 1.8694736957550049, 1.1290757656097412, 1.0483373403549194, 1.1747812032699585, 1.1243171691894531, 1.225886583328247, 1.1849232912063599, 1.1664685010910034, 1.0569947957992554, 1.0779410600662231, 1.2606865167617798, 1.2482647895812988, 1.0377691984176636, 1.0503532886505127, 1.0234603881835938, 1.0645300149917603, 1.009880542755127, 1.01076340675354, 1.1359891891479492, 1.1543431282043457, 1.2890115976333618, 1.1432273387908936, 1.1467260122299194, 1.2554786205291748, 2.257619857788086, 1.1327433586120605, 1.0362730026245117, 1.0437556505203247, 1.0224248170852661, 1.0383578538894653, 1.0860812664031982, 1.141944169998169, 1.264329195022583, 1.394944667816162, 1.1770566701889038, 1.0600391626358032, 2.255213499069214, 1.14256751537323, 1.0924948453903198, 1.3215094804763794, 2.040156602859497, 1.076047658920288, 1.023694634437561, 1.0849103927612305, 1.0779705047607422, 1.0428663492202759, 1.0368081331253052, 1.184121012687683, 1.1286886930465698, 1.0606327056884766, 1.1018152236938477, 1.0189642906188965, 1.3525640964508057, 1.1427589654922485, 1.0126547813415527, 1.0752511024475098, 1.1340456008911133, 1.0938047170639038, 1.1623128652572632, 1.1300170421600342, 1.0374306440353394, 1.0276172161102295, 1.2518843412399292, 1.0620155334472656, 1.5714391469955444, 1.0796915292739868, 1.0219734907150269, 1.228782296180725, 1.8949249982833862, 1.1487793922424316, 1.1779221296310425, 1.1274785995483398, 1.036309003829956, 1.1738488674163818, 1.0705021619796753, 1.3535465002059937, 1.0267308950424194, 1.0136739015579224, 1.9263999462127686, 1.0697970390319824, 1.0166122913360596, 1.0470318794250488, 1.4986690282821655, 1.1793121099472046, 1.1617319583892822, 1.0073310136795044, 1.9210525751113892, 1.1433292627334595, 1.1484375, 1.6734824180603027, 1.3622881174087524, 1.0965315103530884, 1.9921568632125854, 1.0444238185882568, 1.3562451601028442, 1.0078212022781372, 1.1347675323486328, 1.0903640985488892, 1.1356730461120605, 1.6949872970581055, 1.070088267326355, 1.0438750982284546, 1.350077748298645, 1.2019442319869995, 1.112605094909668, 1.1360746622085571]
********** total_n=[200] batchn=[100]  N=[3200]  nitr=[4]  alpha=[0.4]  beta=[0.2] ******
@ Average per query          [3.6212575435638428] ms
 --  Average per query NF    [1.3581526279449463] ms
 --  Average per query vegas [2.2631049156188965] ms
Mean [1.195]  Median [1.135]  95th [1.685]  99th [2.042]  max [2.258]
Mean [1.195]  Median [1.135]  95th [1.685]  99th [2.042]  max [2.258]


WORKLOAD-FINISHED | Type: QueryWorkload | Progress: 3/3 | Workload-time: 12.334768 | Model-update-time: 0.000000


Experiment Summary: #data-update-times=1, #drift=1 | total-time=28.475227