Input arguments = Namespace(data_update='tupleskew', update_size=80000, dataset='bjaq', debug=False, drift_test='js', model_update='adapt', model='face', end2end=False, query_seed=0, random_seed=42, num_workload=3)
JSON-passed parameters = {'data_updater': {'skew_ratio': '1e-3'}, 'random_seed': 83, 'face': {'learning_rate': '1e-4', 'epochs': 0, 'factor': 'auto'}}
MODEL-PATH=/home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
DATASET-PATH=/home/kangping/code_copy/SAUCE/data/bjaq/end2end/bjaq.npy
Workloads: ['QueryWorkload', 'DataUpdateWorkload', 'QueryWorkload']

WORKLOAD-START | Type: QueryWorkload | Progress: 1/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/evaluate/Evaluate-FACE-end2end.py', '--dataset=bjaq', '--end2end']
/home/kangping/anaconda3/envs/LndCar/lib/python3.9/site-packages/torchquad/integration/vegas_map.py:314: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/BucketizationUtils.h:33.)
  iy = torch.searchsorted(self.x_edges[d, :], x[:, d], side='right')
DEVICE NAME
 Tesla V100S-PCIE-32GB
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
data shape: (382168, 5)
Save oracle results to :
./FACE/evaluate/oracle/bjaq_rng-1234.csv
Found oracle card!
torch.Size([5, 2])
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
Took  8.166373014450073
tensor(0.9959)
result is  tensor(380587.2188)
Enter testHyper
ReportEsts: [1.091177225112915, 1.1191858053207397, 1.530837059020996, 1.141292929649353, 1.271992802619934, 1.1256014108657837, 1.1578627824783325, 1.0840075016021729, 1.1026581525802612, 1.0337406396865845, 1.0447889566421509, 1.1322033405303955, 1.1057910919189453, 1.0446428060531616, 1.1752527952194214, 1.3621188402175903, 1.3399232625961304, 1.341092586517334, 1.1079695224761963, 1.084183692932129, 1.1553171873092651, 1.0650159120559692, 1.1876516342163086, 1.1687015295028687, 1.1451441049575806, 1.0813742876052856, 1.0396959781646729, 1.3645964860916138, 1.0400153398513794, 1.0034704208374023, 1.1387016773223877, 1.1696193218231201, 1.0978552103042603, 1.026537299156189, 1.1278207302093506, 1.1347078084945679, 1.316041350364685, 1.336338996887207, 1.0362087488174438, 1.0996407270431519, 1.024832010269165, 1.0162070989608765, 1.0699278116226196, 1.0554382801055908, 1.043524146080017, 1.0566893815994263, 1.1373142004013062, 1.2787545919418335, 1.0540882349014282, 1.1641285419464111, 1.2293332815170288, 1.4421628713607788, 1.1053041219711304, 1.3338954448699951, 1.1476202011108398, 1.1132903099060059, 1.286915898323059, 1.077210545539856, 1.097172498703003, 1.3019530773162842, 1.3110854625701904, 1.190513014793396, 1.249786615371704, 1.0271276235580444, 1.4042552709579468, 1.2285714149475098, 1.165427565574646, 1.008431077003479, 1.0205180644989014, 1.2000000476837158, 1.0571198463439941, 1.0113449096679688, 1.1916686296463013, 1.100890874862671, 1.1848629713058472, 1.214242935180664, 1.821447730064392, 1.1107075214385986, 1.0554289817810059, 1.3528711795806885, 1.2149863243103027, 1.0938459634780884, 1.024274468421936, 1.1920589208602905, 1.2607671022415161, 1.0441162586212158, 1.0595688819885254, 1.142194151878357, 1.03866446018219, 1.0936230421066284, 1.0308213233947754, 1.1475322246551514, 1.0086548328399658, 1.254330039024353, 1.1890244483947754, 1.061521053314209, 1.082708477973938, 1.4492027759552002, 1.01694917678833, 1.0712480545043945, 1.1818181276321411, 1.0362640619277954, 1.007066249847412, 1.0109809637069702, 1.0275150537490845, 1.040964961051941, 1.18679940700531, 1.1675392389297485, 1.090221881866455, 1.0382500886917114, 1.2543233633041382, 1.0673952102661133, 1.1604251861572266, 1.0154109001159668, 1.0245842933654785, 1.0809524059295654, 1.0367612838745117, 1.1033148765563965, 1.3985742330551147, 1.0070042610168457, 1.2715636491775513, 1.1034221649169922, 1.081749677658081, 1.1001337766647339, 1.2335271835327148, 1.1510523557662964, 1.1117455959320068, 1.0330199003219604, 1.2864279747009277, 1.1372745037078857, 1.056503176689148, 1.2463539838790894, 1.209446907043457, 1.3928571939468384, 1.1198856830596924, 1.133893370628357, 1.0051698684692383, 1.3037397861480713, 1.0243175029754639, 1.3396358489990234, 1.1243964433670044, 1.0970401763916016, 1.0528703927993774, 1.2942878007888794, 1.0770165920257568, 1.0310612916946411, 1.1249783039093018, 1.445569634437561, 1.0805784463882446, 1.1140625476837158, 1.1902347803115845, 1.1780540943145752, 1.3073248863220215, 1.1359957456588745, 1.1802607774734497, 1.0138503313064575, 1.0943515300750732, 1.0160382986068726, 1.0050511360168457, 1.1172784566879272, 1.0140628814697266, 1.0105957984924316, 1.1105629205703735, 1.2888582944869995, 1.0875575542449951, 1.1155476570129395, 1.172599196434021, 1.2442668676376343, 1.0461026430130005, 1.033412218093872, 1.061391830444336, 1.0605725049972534, 1.1818181276321411, 1.1457949876785278, 1.1123977899551392, 1.0384958982467651, 1.0573371648788452, 1.3192799091339111, 1.3473659753799438, 1.077601671218872, 1.074553370475769, 1.0540670156478882, 1.016953945159912, 1.1430031061172485, 1.161897897720337, 1.1226215362548828, 1.0442267656326294, 1.2419633865356445, 1.130513072013855, 1.3475953340530396, 1.0875489711761475, 1.0058977603912354, 1.4298597574234009, 1.5031334161758423, 1.9405176639556885, 1.225806474685669, 1.0994998216629028, 1.1014492511749268, 1.001169204711914, 1.1200863122940063]
********** total_n=[200] batchn=[100]  N=[3200]  nitr=[4]  alpha=[0.4]  beta=[0.2] ******
@ Average per query          [3.7714624404907227] ms
 --  Average per query NF    [1.381821632385254] ms
 --  Average per query vegas [2.3896408081054688] ms
Mean [1.149]  Median [1.113]  95th [1.393]  99th [1.534]  max [1.941]
Mean [1.149]  Median [1.113]  95th [1.393]  99th [1.534]  max [1.941]


WORKLOAD-FINISHED | Type: QueryWorkload | Progress: 1/3 | Workload-time: 11.839562 | Model-update-time: 0.000000


WORKLOAD-START | Type: DataUpdateWorkload | Progress: 2/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/Naru/eval_model.py', '--model=face', '--data_update=tupleskew', '--dataset=bjaq', '--drift_test=js', '--end2end', '--update_size=80000', '--query_seed=0', '--eval_type=drift']
Torch device: cuda:1
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
TupleSkewSample - START
0.001
TupleSkewSample - END
load_npy_dataset_from_path - df.shape = (458601, 5)
Parsing... done, took 0.0s
Data updated!
Previous data size: (382168, 5), new data size: (76433, 5)
[8.8810921e-05 4.4465065e-05 3.7193298e-05 1.0430813e-05 1.6272068e-05]
Distance score: 3.9434431528206915e-05
SAUCE Drift detection: True
Detection latency: 0.0229s

DRIFT-DETECTED after 2-th workload
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/incremental_train.py', '--dataset=bjaq', '--end2end', '--model=face', '--model_update=adapt', '--update_size=80000', '--epochs=0']
Torch device: cuda:1
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
update_size: 76433
AdaptTask - START
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
train data shape: 80000, transfer data shape:16000
Training done; evaluating likelihood on full data:
Saved to: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
AdaptTask - END

WORKLOAD-FINISHED | Type: DataUpdateWorkload | Progress: 2/3 | Workload-time: 2.001925 | Model-update-time: 2.242256


WORKLOAD-START | Type: QueryWorkload | Progress: 3/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/evaluate/Evaluate-FACE-end2end.py', '--dataset=bjaq', '--query_seed=0', '--end2end', '--num_workload=3', '--data_update=tupleskew', '--model_update=adapt']
/home/kangping/anaconda3/envs/LndCar/lib/python3.9/site-packages/torchquad/integration/vegas_map.py:314: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/BucketizationUtils.h:33.)
  iy = torch.searchsorted(self.x_edges[d, :], x[:, d], side='right')
DEVICE NAME
 Tesla V100S-PCIE-32GB
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
data shape: (458601, 5)
Save oracle results to :
./FACE/evaluate/oracle/bjaq_rng-0.csv
Found oracle card!
torch.Size([5, 2])
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
Took  8.17246389389038
tensor(0.9975)
result is  tensor(457447.2500)
Enter testHyper
ReportEsts: [1.2572755813598633, 1.05998957157135, 1.062322974205017, 1.5439814329147339, 1.0857815742492676, 1.0161449909210205, 1.1722886562347412, 1.0191909074783325, 1.0773322582244873, 1.0652070045471191, 1.180473804473877, 1.016775131225586, 1.51889169216156, 1.03732168674469, 1.050188422203064, 1.0153882503509521, 1.7110414505004883, 1.064125895500183, 1.05642569065094, 1.4301973581314087, 1.272076964378357, 1.209973692893982, 1.0003869533538818, 1.0409362316131592, 1.1465706825256348, 1.0840685367584229, 1.021669864654541, 1.3163716793060303, 1.6308804750442505, 1.59039306640625, 1.3817015886306763, 1.4322034120559692, 1.017716884613037, 1.1508694887161255, 1.0807342529296875, 1.0074622631072998, 1.0071423053741455, 1.2419928312301636, 1.1831965446472168, 1.2004106044769287, 1.1344796419143677, 1.1315453052520752, 1.0104501247406006, 1.3944584131240845, 1.0161690711975098, 1.0550061464309692, 1.137213110923767, 1.0149472951889038, 1.3857142925262451, 1.077163577079773, 1.0686864852905273, 1.0105160474777222, 1.0839409828186035, 1.031679391860962, 1.2943775653839111, 1.2604604959487915, 1.1057018041610718, 1.0925114154815674, 1.362777829170227, 1.2250075340270996, 1.297820806503296, 1.0534011125564575, 1.2639206647872925, 1.457249641418457, 1.0075429677963257, 1.176163673400879, 1.247840166091919, 1.2398923635482788, 1.0766342878341675, 1.0761178731918335, 1.156904697418213, 1.2996087074279785, 1.1932084560394287, 1.0971945524215698, 1.002008318901062, 1.1031930446624756, 1.1137630939483643, 1.0410523414611816, 1.1337352991104126, 1.1706123352050781, 1.0750141143798828, 1.200316309928894, 1.0333365201950073, 1.1021380424499512, 1.20945143699646, 1.1482479572296143, 1.2587642669677734, 1.365126132965088, 1.2146806716918945, 1.5339689254760742, 1.3665121793746948, 1.1614762544631958, 1.3041847944259644, 1.2508368492126465, 1.1705076694488525, 1.0703848600387573, 1.0752110481262207, 1.2290382385253906, 1.2375684976577759, 1.232108235359192, 1.2194403409957886, 1.1487165689468384, 1.4468084573745728, 1.0460882186889648, 1.1426260471343994, 1.062989592552185, 1.0886818170547485, 1.1954059600830078, 1.0601719617843628, 1.1228210926055908, 1.0692906379699707, 1.071122407913208, 1.223458170890808, 1.3405990600585938, 1.0921543836593628, 1.2117812633514404, 1.1351410150527954, 1.2687264680862427, 1.1628705263137817, 1.0530973672866821, 1.0598366260528564, 1.1915966272354126, 1.0544825792312622, 1.585698127746582, 1.2034722566604614, 1.0229545831680298, 1.0943400859832764, 1.6971608400344849, 1.0058166980743408, 1.0830796957015991, 1.4944435358047485, 1.1306915283203125, 1.1734769344329834, 2.1688125133514404, 1.1998381614685059, 1.1547654867172241, 1.0879786014556885, 1.113459587097168, 1.1124895811080933, 1.0793538093566895, 1.0792113542556763, 1.247248649597168, 1.0027343034744263, 1.184464931488037, 1.0426784753799438, 1.251006841659546, 1.2634730339050293, 1.0311872959136963, 1.0481648445129395, 1.0164575576782227, 1.148330569267273, 1.0574229955673218, 1.1335077285766602, 1.0184916257858276, 1.0430678129196167, 1.6126984357833862, 1.033584475517273, 1.1048387289047241, 1.105954885482788, 1.1154210567474365, 1.1136324405670166, 1.5279569625854492, 1.1173175573349, 1.0668684244155884, 1.1364943981170654, 1.0353724956512451, 1.142093539237976, 1.1712555885314941, 1.0622718334197998, 1.2135400772094727, 1.129585862159729, 1.1159777641296387, 1.4019745588302612, 1.1214280128479004, 1.0103793144226074, 1.0731089115142822, 1.0483089685440063, 1.121730089187622, 1.0661484003067017, 1.0365972518920898, 1.9736841917037964, 1.0771503448486328, 1.008507490158081, 1.4379332065582275, 1.244360089302063, 1.2596733570098877, 1.1454864740371704, 1.0009146928787231, 1.405146837234497, 1.021700143814087, 1.2195826768875122, 1.2154649496078491, 1.2951555252075195, 1.7060301303863525, 1.1388888359069824, 1.382513403892517, 1.1386041641235352, 1.1919023990631104, 1.5165793895721436, 1.132383108139038]
********** total_n=[200] batchn=[100]  N=[3200]  nitr=[4]  alpha=[0.4]  beta=[0.2] ******
@ Average per query          [3.726060390472412] ms
 --  Average per query NF    [1.3593947887420654] ms
 --  Average per query vegas [2.3666656017303467] ms
Mean [1.182]  Median [1.134]  95th [1.534]  99th [1.714]  max [2.169]
Mean [1.182]  Median [1.134]  95th [1.534]  99th [1.714]  max [2.169]


WORKLOAD-FINISHED | Type: QueryWorkload | Progress: 3/3 | Workload-time: 12.387805 | Model-update-time: 0.000000


Experiment Summary: #data-update-times=1, #drift=1 | total-time=28.496727