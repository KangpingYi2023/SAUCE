Input arguments = Namespace(data_update='valueskew', update_size=80000, dataset='bjaq', debug=False, drift_test='js', model_update='adapt', model='face', end2end=False, query_seed=0, random_seed=42, num_workload=3)
JSON-passed parameters = {'data_updater': {'skew_ratio': '1e-3'}, 'random_seed': 72, 'face': {'learning_rate': '1e-4', 'epochs': 0, 'factor': 'auto'}}
MODEL-PATH=/home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
DATASET-PATH=/home/kangping/code_copy/SAUCE/data/bjaq/end2end/bjaq.npy
Workloads: ['QueryWorkload', 'DataUpdateWorkload', 'QueryWorkload']

WORKLOAD-START | Type: QueryWorkload | Progress: 1/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/evaluate/Evaluate-FACE-end2end.py', '--dataset=bjaq', '--end2end']
/home/kangping/anaconda3/envs/LndCar/lib/python3.9/site-packages/torchquad/integration/vegas_map.py:314: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/BucketizationUtils.h:33.)
  iy = torch.searchsorted(self.x_edges[d, :], x[:, d], side='right')
DEVICE NAME
 Tesla V100S-PCIE-32GB
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
data shape: (382168, 5)
Save oracle results to :
./FACE/evaluate/oracle/bjaq_rng-1234.csv
Found oracle card!
torch.Size([5, 2])
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
Took  8.163689136505127
tensor(0.9984)
result is  tensor(381545.5938)
Enter testHyper
ReportEsts: [1.1753321886062622, 1.027904748916626, 1.5010799169540405, 1.1539270877838135, 1.2484581470489502, 1.1220347881317139, 1.1929383277893066, 1.242676854133606, 1.0805388689041138, 1.173647403717041, 1.101636528968811, 1.2059322595596313, 1.025485873222351, 1.0357142686843872, 1.108781099319458, 1.244552731513977, 1.3437116146087646, 1.2856295108795166, 1.0732226371765137, 1.0816326141357422, 1.0622479915618896, 1.1154390573501587, 1.1635342836380005, 1.0800508260726929, 1.257044792175293, 1.1925859451293945, 1.0653153657913208, 1.4419642686843872, 1.0856120586395264, 1.0756016969680786, 1.0436325073242188, 1.1459312438964844, 1.204748511314392, 1.0340876579284668, 1.0514742136001587, 1.0254266262054443, 1.5524193048477173, 1.0448870658874512, 1.0079872608184814, 1.085748553276062, 1.0298957824707031, 1.1104257106781006, 1.1426554918289185, 1.0098305940628052, 1.1428269147872925, 1.0537633895874023, 1.282754898071289, 1.3375478982925415, 1.1491775512695312, 1.1421319246292114, 1.2392473220825195, 1.3789267539978027, 1.111716389656067, 1.1463768482208252, 1.0930136442184448, 1.0760315656661987, 1.242056131362915, 1.2018423080444336, 1.304139256477356, 1.3778420686721802, 1.2351808547973633, 1.1750518083572388, 1.3744666576385498, 1.1081793308258057, 1.2132352590560913, 1.0285714864730835, 1.1166517734527588, 1.1767183542251587, 1.0900195837020874, 1.2315789461135864, 1.0865384340286255, 1.032292366027832, 1.3090903759002686, 1.0263454914093018, 1.130668044090271, 1.186202883720398, 1.7904647588729858, 1.1040871143341064, 1.024895429611206, 1.3144012689590454, 1.141821026802063, 1.1755774021148682, 1.0064668655395508, 1.263240098953247, 1.2860194444656372, 1.1026405096054077, 1.0562249422073364, 1.0831778049468994, 1.0431482791900635, 1.147358775138855, 1.0329864025115967, 1.4129581451416016, 1.0082942247390747, 1.3067426681518555, 1.05978262424469, 1.20386803150177, 1.2191532850265503, 1.2991881370544434, 1.01694917678833, 1.2832131385803223, 1.1097561120986938, 1.02866792678833, 1.0925586223602295, 1.0014641284942627, 1.0035691261291504, 1.020590901374817, 1.2247412204742432, 1.1849912405014038, 1.057344913482666, 1.0323270559310913, 1.3528386354446411, 1.1004694700241089, 1.1761622428894043, 1.0540224313735962, 1.2197750806808472, 1.1182266473770142, 1.0422346591949463, 1.0057076215744019, 1.1337231397628784, 1.0536154508590698, 1.193089485168457, 1.099696397781372, 1.1190237998962402, 1.0881534814834595, 1.1482558250427246, 1.1486793756484985, 1.1820627450942993, 1.1703832149505615, 1.287500023841858, 1.0985304117202759, 1.1633549928665161, 1.2144027948379517, 1.1401491165161133, 1.5, 1.1969468593597412, 1.0170648097991943, 1.0099586248397827, 1.1811381578445435, 1.0582046508789062, 1.3452882766723633, 1.1300711631774902, 1.1753239631652832, 1.0243476629257202, 1.3005884885787964, 1.0062544345855713, 1.0171111822128296, 1.074251413345337, 1.4314446449279785, 1.2001556158065796, 1.0928267240524292, 1.2334624528884888, 1.1604923009872437, 1.331351399421692, 1.0325100421905518, 1.1667560338974, 1.0235801935195923, 1.179419994354248, 1.0039547681808472, 1.0571789741516113, 1.304267406463623, 1.1691148281097412, 1.0996102094650269, 1.0550612211227417, 1.2818057537078857, 1.3277074098587036, 1.0094324350357056, 1.186759114265442, 1.314731478691101, 1.0929535627365112, 1.0268216133117676, 1.029462218284607, 1.0613148212432861, 1.1688311100006104, 1.099593162536621, 1.0095367431640625, 1.0523022413253784, 1.0083292722702026, 1.1045602560043335, 1.0705307722091675, 1.0188528299331665, 1.0416712760925293, 1.0376735925674438, 1.0618956089019775, 1.0449438095092773, 1.111915946006775, 1.0951374769210815, 1.0512363910675049, 1.2024766206741333, 1.0551420450210571, 1.6232495307922363, 1.0100314617156982, 1.087836742401123, 1.7344272136688232, 1.4117152690887451, 1.6917413473129272, 1.189686894416809, 1.1156314611434937, 1.1343283653259277, 1.0518970489501953, 1.15670645236969]
********** total_n=[200] batchn=[100]  N=[3200]  nitr=[4]  alpha=[0.4]  beta=[0.2] ******
@ Average per query          [3.7730610370635986] ms
 --  Average per query NF    [1.3771092891693115] ms
 --  Average per query vegas [2.395951747894287] ms
Mean [1.155]  Median [1.117]  95th [1.412]  99th [1.692]  max [1.790]
Mean [1.155]  Median [1.117]  95th [1.412]  99th [1.692]  max [1.790]


WORKLOAD-FINISHED | Type: QueryWorkload | Progress: 1/3 | Workload-time: 11.809436 | Model-update-time: 0.000000


WORKLOAD-START | Type: DataUpdateWorkload | Progress: 2/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/Naru/eval_model.py', '--model=face', '--data_update=valueskew', '--dataset=bjaq', '--drift_test=js', '--end2end', '--update_size=80000', '--query_seed=0', '--eval_type=drift']
Torch device: cuda:1
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
ValueSample - START
ValueSample - END
load_npy_dataset_from_path - df.shape = (458601, 5)
Parsing... done, took 0.0s
Data updated!
Previous data size: (382168, 5), new data size: (76433, 5)
[0.0000000e+00 0.0000000e+00 5.9604645e-08 1.9921184e-02 2.3841858e-07]
Distance score: 0.003984296228736639
SAUCE Drift detection: True
Detection latency: 0.0233s

DRIFT-DETECTED after 2-th workload
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/incremental_train.py', '--dataset=bjaq', '--end2end', '--model=face', '--model_update=adapt', '--update_size=80000', '--epochs=0']
Torch device: cuda:1
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
update_size: 76433
AdaptTask - START
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
train data shape: 80000, transfer data shape:16000
Training done; evaluating likelihood on full data:
Saved to: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
AdaptTask - END

WORKLOAD-FINISHED | Type: DataUpdateWorkload | Progress: 2/3 | Workload-time: 2.054520 | Model-update-time: 2.241723


WORKLOAD-START | Type: QueryWorkload | Progress: 3/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/evaluate/Evaluate-FACE-end2end.py', '--dataset=bjaq', '--query_seed=0', '--end2end', '--num_workload=3', '--data_update=valueskew', '--model_update=adapt']
/home/kangping/anaconda3/envs/LndCar/lib/python3.9/site-packages/torchquad/integration/vegas_map.py:314: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/BucketizationUtils.h:33.)
  iy = torch.searchsorted(self.x_edges[d, :], x[:, d], side='right')
DEVICE NAME
 Tesla V100S-PCIE-32GB
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
data shape: (458601, 5)
Save oracle results to :
./FACE/evaluate/oracle/bjaq_rng-0.csv
Found oracle card!
torch.Size([5, 2])
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
Took  8.21869444847107
tensor(0.9933)
result is  tensor(455550.2812)
Enter testHyper
ReportEsts: [1.0049694776535034, 1.0629820823669434, 1.010014295578003, 1.1464545726776123, 1.1467843055725098, 1.2520332336425781, 1.019441843032837, 1.0991994142532349, 1.1700341701507568, 1.8973300457000732, 1.006852626800537, 1.3808414936065674, 1.0716112852096558, 1.225609540939331, 1.2614141702651978, 1.0335779190063477, 1.348826289176941, 1.3717225790023804, 1.2453818321228027, 1.353610873222351, 1.0484673976898193, 1.278064250946045, 1.2307264804840088, 8.116005897521973, 1.0217903852462769, 1.1572048664093018, 1.1433899402618408, 1.1295865774154663, 1.0733035802841187, 1.2342857122421265, 2.139287233352661, 1.3559322357177734, 1.0653656721115112, 1.2011228799819946, 1.1139020919799805, 1.1752901077270508, 1.0592163801193237, 1.0894840955734253, 1.417704463005066, 1.0874179601669312, 1.1895904541015625, 1.0200210809707642, 628.5714111328125, 1.2721161842346191, 1.0497705936431885, 1.1102973222732544, 1.1988036632537842, 1.298812747001648, 1.4285714626312256, 1.9352627992630005, 1.0653891563415527, 1.0890002250671387, 1.0810352563858032, 1.002096176147461, 1.1564960479736328, 1.2514806985855103, 1.011953592300415, 1.0670973062515259, 1.0349011421203613, 1.0710148811340332, 1.1551724672317505, 1.0016000270843506, 1.0650153160095215, 2.1031129360198975, 1.0128620862960815, 1.2951122522354126, 1.0893791913986206, 1.024278163909912, 1.0654035806655884, 1.1075444221496582, 1.017836093902588, 1.0005115270614624, 1.0599814653396606, 1.0324225425720215, 2.0790469646453857, 1.0909730195999146, 1.083513617515564, 1.3734855651855469, 1.285798192024231, 1.0305256843566895, 2.1290323734283447, 1.0397191047668457, 1.1471103429794312, 1.0456119775772095, 1.1432749032974243, 1.378947377204895, 1.0128130912780762, 1.236459732055664, 1.117050051689148, 1.0327138900756836, 1.0484094619750977, 1.3957834243774414, 1.0590226650238037, 1.0318928956985474, 89.3484878540039, 1.021057367324829, 1.0585670471191406, 1.1379387378692627, 1.3991585969924927, 1.0353507995605469, 1.1162784099578857, 1.1162043809890747, 1.0269607305526733, 1.0407474040985107, 4.649768352508545, 1.0519559383392334, 1.0115678310394287, 1.188515543937683, 1.0115941762924194, 1.1096233129501343, 1.1933443546295166, 1.0239828824996948, 1.088189721107483, 1.1488066911697388, 1.1260008811950684, 1.8933770656585693, 1.1640421152114868, 1.1723026037216187, 1.103737473487854, 1.3185840845108032, 1.0985816717147827, 1.004486083984375, 1.0618585348129272, 1.4722459316253662, 1.0771503448486328, 1.3176746368408203, 1.3319315910339355, 1.0977199077606201, 1.0222688913345337, 1.0075242519378662, 1.227359652519226, 1.0233112573623657, 1.1260926723480225, 1.0846524238586426, 1.3937970399856567, 1.0038340091705322, 1.205528974533081, 1.5506535768508911, 1.0324313640594482, 1.006742000579834, 1.0887879133224487, 1.1499758958816528, 1.0136022567749023, 1.0023170709609985, 1.035464882850647, 1.7665077447891235, 1.2411764860153198, 1.0889755487442017, 1.2228419780731201, 1.1126214265823364, 52.16825485229492, 1.118733525276184, 1.472447156906128, 1.0583919286727905, 1.027685523033142, 1.1086345911026, 1.0406336784362793, 1.0378787517547607, 1.938503384590149, 1.0051524639129639, 1.1057478189468384, 1.0374813079833984, 1.200850486755371, 1.315637230873108, 1.2620887756347656, 1.0193339586257935, 1.0805838108062744, 1.0919225215911865, 1.2112970352172852, 1.0564740896224976, 1.1868202686309814, 1.1154457330703735, 1.3679245710372925, 1.0423390865325928, 1.6600810289382935, 1.0951998233795166, 1.1018545627593994, 1.0553830862045288, 1.1631072759628296, 1.0230387449264526, 1.9210525751113892, 1.40334951877594, 1.1550699472427368, 1.4840630292892456, 1.3983550071716309, 1.000503659248352, 1.2769980430603027, 1.0472325086593628, 1.3381500244140625, 1.0256770849227905, 1.0411889553070068, 1.0331577062606812, 1.1419599056243896, 2.0307998657226562, 1.0549275875091553, 1.004206657409668, 1.1001806259155273, 1.1277419328689575, 1.1182432174682617, 1.096071481704712]
********** total_n=[200] batchn=[100]  N=[3200]  nitr=[4]  alpha=[0.4]  beta=[0.2] ******
@ Average per query          [3.744767904281616] ms
 --  Average per query NF    [1.364455223083496] ms
 --  Average per query vegas [2.38031268119812] ms
Mean [5.072]  Median [1.109]  95th [1.943]  99th [52.540]  max [628.571]
Mean [5.072]  Median [1.109]  95th [1.943]  99th [52.540]  max [628.571]


WORKLOAD-FINISHED | Type: QueryWorkload | Progress: 3/3 | Workload-time: 12.438651 | Model-update-time: 0.000000


Experiment Summary: #data-update-times=1, #drift=1 | total-time=28.592384