Input arguments = Namespace(data_update='valueskew', update_size=80000, dataset='bjaq', debug=False, drift_test='js', model_update='adapt', model='face', end2end=False, query_seed=0, random_seed=42, num_workload=3)
JSON-passed parameters = {'data_updater': {'skew_ratio': '1e-3'}, 'random_seed': 94, 'face': {'learning_rate': '1e-4', 'epochs': 0, 'factor': 'auto'}}
MODEL-PATH=/home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
DATASET-PATH=/home/kangping/code_copy/SAUCE/data/bjaq/end2end/bjaq.npy
Workloads: ['QueryWorkload', 'DataUpdateWorkload', 'QueryWorkload']

WORKLOAD-START | Type: QueryWorkload | Progress: 1/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/evaluate/Evaluate-FACE-end2end.py', '--dataset=bjaq', '--end2end']
/home/kangping/anaconda3/envs/LndCar/lib/python3.9/site-packages/torchquad/integration/vegas_map.py:314: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/BucketizationUtils.h:33.)
  iy = torch.searchsorted(self.x_edges[d, :], x[:, d], side='right')
DEVICE NAME
 Tesla V100S-PCIE-32GB
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
data shape: (382168, 5)
Save oracle results to :
./FACE/evaluate/oracle/bjaq_rng-1234.csv
Found oracle card!
torch.Size([5, 2])
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
Took  8.206583738327026
tensor(0.9930)
result is  tensor(379475.9062)
Enter testHyper
ReportEsts: [1.130906105041504, 1.1153570413589478, 1.017019510269165, 1.118551254272461, 1.2595555782318115, 1.0983318090438843, 1.3057831525802612, 1.1562026739120483, 1.0370689630508423, 1.1872485876083374, 1.0964685678482056, 1.207415223121643, 1.0705264806747437, 1.0535714626312256, 1.0602353811264038, 1.3084875345230103, 1.3019633293151855, 1.230760097503662, 1.2414642572402954, 1.03316330909729, 1.0575071573257446, 1.0951188802719116, 1.148400902748108, 1.0738449096679688, 1.1543738842010498, 1.0669077634811401, 1.1340090036392212, 1.531531572341919, 1.0812644958496094, 1.0756789445877075, 1.1649521589279175, 1.250846266746521, 1.3146296739578247, 1.062764048576355, 1.1859660148620605, 1.1721158027648926, 1.398402452468872, 1.1651142835617065, 1.0143811702728271, 1.0538936853408813, 1.017138957977295, 1.1081277132034302, 1.00679612159729, 1.0929313898086548, 1.0566236972808838, 1.0804988145828247, 1.055100679397583, 1.2364087104797363, 1.1218962669372559, 1.0050761699676514, 1.2099738121032715, 1.380281686782837, 1.0548497438430786, 1.3699342012405396, 1.0671535730361938, 1.005828857421875, 1.3719626665115356, 1.1636073589324951, 1.1271350383758545, 1.3761354684829712, 1.2102718353271484, 1.1920948028564453, 1.3637595176696777, 1.0915863513946533, 1.2043795585632324, 1.0285714864730835, 1.1611111164093018, 1.0310853719711304, 1.155102252960205, 1.24210524559021, 1.0427852869033813, 1.0025334358215332, 1.1825964450836182, 1.1407198905944824, 1.1422785520553589, 1.1136027574539185, 2.072145462036133, 1.0354647636413574, 1.0048673152923584, 1.3371751308441162, 1.0446043014526367, 1.4115651845932007, 1.0020601749420166, 1.2111798524856567, 1.2475388050079346, 1.0907055139541626, 1.067846655845642, 1.0344010591506958, 1.0381684303283691, 1.0363214015960693, 1.0125877857208252, 1.211365818977356, 1.0028849840164185, 1.3434220552444458, 1.0372340679168701, 1.1664257049560547, 1.034469485282898, 1.2674418687820435, 1.01694917678833, 1.2111225128173828, 1.1956204175949097, 1.0975568294525146, 1.0332611799240112, 1.0095168352127075, 1.0196245908737183, 1.0049418210983276, 1.231728434562683, 1.1640489101409912, 1.0664201974868774, 1.047515630722046, 1.3965381383895874, 1.084181308746338, 1.2712550163269043, 1.0058794021606445, 1.2146748304367065, 1.1822916269302368, 1.080204963684082, 1.0923513174057007, 1.3919204473495483, 1.069265365600586, 1.253056526184082, 1.1143921613693237, 1.1532306671142578, 1.1524949073791504, 1.2335271835327148, 1.22750723361969, 1.0575494766235352, 1.1867008209228516, 1.277915596961975, 1.1092184782028198, 1.0261415243148804, 1.0830992460250854, 1.208203911781311, 1.4642857313156128, 1.161790132522583, 1.1554213762283325, 1.0775225162506104, 1.036097526550293, 1.0519752502441406, 1.212294101715088, 1.1391308307647705, 1.1423871517181396, 1.053871512413025, 1.1985269784927368, 1.1708240509033203, 1.0935901403427124, 1.102494716644287, 1.3704997301101685, 1.0196223258972168, 1.0485795736312866, 1.1359446048736572, 1.1499807834625244, 1.3407729864120483, 1.1398130655288696, 1.2439930438995361, 1.0299105644226074, 1.2699428796768188, 1.034327507019043, 1.0039488077163696, 1.238495945930481, 1.0184876918792725, 1.011715054512024, 1.0568901300430298, 1.2730978727340698, 1.3390071392059326, 1.1315412521362305, 1.0576945543289185, 1.1951606273651123, 1.0552526712417603, 1.080430269241333, 1.1058145761489868, 1.0972182750701904, 1.1363636255264282, 1.093462586402893, 1.019444465637207, 1.238643765449524, 1.0165208578109741, 1.2179676294326782, 1.041335105895996, 1.1026997566223145, 1.1479860544204712, 1.0574778318405151, 1.0655609369277954, 1.0837589502334595, 1.1177109479904175, 1.1035940647125244, 1.045538306236267, 1.2589348554611206, 1.0642921924591064, 1.3451673984527588, 1.029049038887024, 1.0398836135864258, 1.4160257577896118, 1.0785866975784302, 1.6071157455444336, 1.247104287147522, 1.117548942565918, 1.0704225301742554, 1.0571544170379639, 1.0945218801498413]
********** total_n=[200] batchn=[100]  N=[3200]  nitr=[4]  alpha=[0.4]  beta=[0.2] ******
@ Average per query          [3.7401318550109863] ms
 --  Average per query NF    [1.3608670234680176] ms
 --  Average per query vegas [2.3792648315429688] ms
Mean [1.148]  Median [1.114]  95th [1.376]  99th [1.532]  max [2.072]
Mean [1.148]  Median [1.114]  95th [1.376]  99th [1.532]  max [2.072]


WORKLOAD-FINISHED | Type: QueryWorkload | Progress: 1/3 | Workload-time: 11.868941 | Model-update-time: 0.000000


WORKLOAD-START | Type: DataUpdateWorkload | Progress: 2/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/Naru/eval_model.py', '--model=face', '--data_update=valueskew', '--dataset=bjaq', '--drift_test=js', '--end2end', '--update_size=80000', '--query_seed=0', '--eval_type=drift']
Torch device: cuda:1
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
ValueSample - START
ValueSample - END
load_npy_dataset_from_path - df.shape = (458601, 5)
Parsing... done, took 0.0s
Data updated!
Previous data size: (382168, 5), new data size: (76433, 5)
[7.7486038e-07 1.0728836e-06 9.5486641e-03 7.7486038e-07 5.9604645e-08]
Distance score: 0.0019102692604064941
SAUCE Drift detection: True
Detection latency: 0.0231s

DRIFT-DETECTED after 2-th workload
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/incremental_train.py', '--dataset=bjaq', '--end2end', '--model=face', '--model_update=adapt', '--update_size=80000', '--epochs=0']
Torch device: cuda:1
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
update_size: 76433
AdaptTask - START
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
train data shape: 80000, transfer data shape:16000
Training done; evaluating likelihood on full data:
Saved to: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
AdaptTask - END

WORKLOAD-FINISHED | Type: DataUpdateWorkload | Progress: 2/3 | Workload-time: 2.049183 | Model-update-time: 2.234848


WORKLOAD-START | Type: QueryWorkload | Progress: 3/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/evaluate/Evaluate-FACE-end2end.py', '--dataset=bjaq', '--query_seed=0', '--end2end', '--num_workload=3', '--data_update=valueskew', '--model_update=adapt']
/home/kangping/anaconda3/envs/LndCar/lib/python3.9/site-packages/torchquad/integration/vegas_map.py:314: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/BucketizationUtils.h:33.)
  iy = torch.searchsorted(self.x_edges[d, :], x[:, d], side='right')
DEVICE NAME
 Tesla V100S-PCIE-32GB
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
data shape: (458601, 5)
Save oracle results to :
./FACE/evaluate/oracle/bjaq_rng-0.csv
Found oracle card!
torch.Size([5, 2])
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
Took  8.154927492141724
tensor(0.9978)
result is  tensor(457590.5000)
Enter testHyper
ReportEsts: [3.111295461654663, 1.4760730266571045, 1.016997218132019, 1.4893378019332886, 1.1470588445663452, 1.5132609605789185, 1.116114616394043, 1.067972183227539, 1.165289282798767, 1.02816903591156, 2.3945817947387695, 1.2256762981414795, 1.1833332777023315, 2.7319180965423584, 1.0934780836105347, 1.0031137466430664, 1.0085736513137817, 1.2846959829330444, 1.1110267639160156, 1.0494766235351562, 1.0932327508926392, 1.1943676471710205, 1.2267580032348633, 1.917113184928894, 1.0021270513534546, 1.0294398069381714, 1.0317052602767944, 1.4069702625274658, 1.0625, 1.1268861293792725, 1.1579971313476562, 1.491525411605835, 1.1663167476654053, 13.24106502532959, 1.1236741542816162, 1.0263645648956299, 1.0986056327819824, 1.2107387781143188, 1.311367154121399, 1.1967136859893799, 1.071191430091858, 1.1637743711471558, 1.112981915473938, 1.045409917831421, 1.0768015384674072, 1.0891672372817993, 1.3801424503326416, 1.0408155918121338, 1.0740740299224854, 1.1142550706863403, 1.1582446098327637, 1.023390531539917, 1.0052472352981567, 1.0807901620864868, 1.3258548974990845, 3.6379785537719727, 1.1629109382629395, 1.0672719478607178, 1.2390038967132568, 1.2888861894607544, 1.3850128650665283, 1.2721339464187622, 1.4176270961761475, 1.1888799667358398, 1.0089350938796997, 1.622776985168457, 1.1231714487075806, 1.1227152347564697, 1.0554779767990112, 1.1625200510025024, 1.22194242477417, 1.3112223148345947, 1.0640058517456055, 1.0356066226959229, 1.004103183746338, 1.3996095657348633, 11.573333740234375, 1.0652027130126953, 1.3487390279769897, 1.1166127920150757, 1.4719045162200928, 1.5795416831970215, 1.190273642539978, 1.09778892993927, 1.288360357284546, 1.3797909021377563, 1.205439567565918, 1.5324010848999023, 2.874396800994873, 1.0609045028686523, 1.095842719078064, 1.2224384546279907, 1.3041925430297852, 1.1908905506134033, 1.1284748315811157, 1.0054644346237183, 1.8026740550994873, 1.3008655309677124, 1.1952548027038574, 1.490822434425354, 1.0957965850830078, 1.0122740268707275, 1.0124069452285767, 1.078321099281311, 1.236894130706787, 4.6768951416015625, 1.0782184600830078, 1.0027371644973755, 1.031518578529358, 1.0086103677749634, 1.1795114278793335, 1.0584499835968018, 1.2486073970794678, 1.3063052892684937, 1.0252482891082764, 1.039721131324768, 1.1372995376586914, 1.294689416885376, 1.9755799770355225, 1.0272727012634277, 1.0946131944656372, 1.2654205560684204, 6.3942694664001465, 1.4845921993255615, 1.202677607536316, 1.1361109018325806, 1.0224870443344116, 1.0547730922698975, 1.0470420122146606, 1.0483978986740112, 1.6412498950958252, 1.0568734407424927, 1.086786150932312, 2.887359142303467, 1.0993328094482422, 1.3358393907546997, 1.097157597541809, 1.7576754093170166, 1.026119351387024, 1.2160543203353882, 1.0849226713180542, 1.0636425018310547, 1.0409409999847412, 1.355242371559143, 1.3255661725997925, 1.1517976522445679, 1.5582822561264038, 1.0717310905456543, 1.0838054418563843, 1.0697300434112549, 12.946398735046387, 1.044585943222046, 1.1718502044677734, 1.1017388105392456, 1.0476270914077759, 1.3112787008285522, 1.047786831855774, 1.263157844543457, 1.0058974027633667, 1.2911839485168457, 1.0045459270477295, 1.0630252361297607, 1.3214011192321777, 1.0388214588165283, 1.2262840270996094, 1.2809367179870605, 1.6046788692474365, 1.07459557056427, 10.77368450164795, 1.1939009428024292, 1.0182207822799683, 1.0444036722183228, 1.7084639072418213, 1.2070320844650269, 1.0464047193527222, 1.0190989971160889, 1.1015794277191162, 1.0565170049667358, 1.0020616054534912, 1.421798825263977, 1.301886796951294, 1.3305084705352783, 2.0902326107025146, 1.2858572006225586, 1.2965505123138428, 2.185606002807617, 1.430904507637024, 1.037054419517517, 1.1224418878555298, 1.0810638666152954, 1.1954063177108765, 1.1118717193603516, 1.3604408502578735, 1.6790306568145752, 1.1903811693191528, 1.149221420288086, 2.0824501514434814, 1.1731706857681274, 1.4293286800384521, 1.099118947982788]
********** total_n=[200] batchn=[100]  N=[3200]  nitr=[4]  alpha=[0.4]  beta=[0.2] ******
@ Average per query          [3.7175345420837402] ms
 --  Average per query NF    [1.35603666305542] ms
 --  Average per query vegas [2.3614978790283203] ms
Mean [1.524]  Median [1.160]  95th [2.739]  99th [11.587]  max [13.241]
Mean [1.524]  Median [1.160]  95th [2.739]  99th [11.587]  max [13.241]


WORKLOAD-FINISHED | Type: QueryWorkload | Progress: 3/3 | Workload-time: 12.378722 | Model-update-time: 0.000000


Experiment Summary: #data-update-times=1, #drift=1 | total-time=28.585319