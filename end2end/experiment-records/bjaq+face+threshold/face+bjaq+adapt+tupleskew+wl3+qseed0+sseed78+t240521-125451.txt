Input arguments = Namespace(data_update='tupleskew', update_size=80000, dataset='bjaq', debug=False, drift_test='js', model_update='adapt', model='face', end2end=False, query_seed=0, random_seed=42, num_workload=3)
JSON-passed parameters = {'data_updater': {'skew_ratio': '1e-3'}, 'random_seed': 78, 'face': {'learning_rate': '1e-4', 'epochs': 0, 'factor': 'auto'}}
MODEL-PATH=/home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
DATASET-PATH=/home/kangping/code_copy/SAUCE/data/bjaq/end2end/bjaq.npy
Workloads: ['QueryWorkload', 'DataUpdateWorkload', 'QueryWorkload']

WORKLOAD-START | Type: QueryWorkload | Progress: 1/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/evaluate/Evaluate-FACE-end2end.py', '--dataset=bjaq', '--end2end']
/home/kangping/anaconda3/envs/LndCar/lib/python3.9/site-packages/torchquad/integration/vegas_map.py:314: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/BucketizationUtils.h:33.)
  iy = torch.searchsorted(self.x_edges[d, :], x[:, d], side='right')
DEVICE NAME
 Tesla V100S-PCIE-32GB
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
data shape: (382168, 5)
Save oracle results to :
./FACE/evaluate/oracle/bjaq_rng-1234.csv
Found oracle card!
torch.Size([5, 2])
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
Took  8.222692966461182
tensor(1.0001)
result is  tensor(382213.0625)
Enter testHyper
ReportEsts: [1.1790785789489746, 1.0846034288406372, 1.3373957872390747, 1.1174983978271484, 1.2448023557662964, 1.066503882408142, 1.199791669845581, 1.1243243217468262, 1.0276196002960205, 1.1349859237670898, 1.1231696605682373, 1.1953389644622803, 1.0053237676620483, 1.0158730745315552, 1.1424933671951294, 1.3120944499969482, 1.4719619750976562, 1.1806412935256958, 1.1372932195663452, 1.461734652519226, 1.2058558464050293, 1.0643329620361328, 1.2065411806106567, 1.0979135036468506, 1.2049334049224854, 1.0250232219696045, 1.1382319927215576, 1.5672004222869873, 1.0197629928588867, 1.0311260223388672, 1.1532458066940308, 1.1950957775115967, 1.057212233543396, 1.0341405868530273, 1.2584829330444336, 1.1345210075378418, 1.2189093828201294, 1.113720417022705, 1.041380763053894, 1.0062274932861328, 1.030187964439392, 1.2455248832702637, 1.1191965341567993, 1.0293786525726318, 1.0642298460006714, 1.0068492889404297, 1.101663589477539, 1.2229812145233154, 1.2332160472869873, 1.1082910299301147, 1.2195767164230347, 1.399535059928894, 1.1308976411819458, 1.0446381568908691, 1.130007028579712, 1.0630505084991455, 1.3392523527145386, 1.0056627988815308, 1.003947138786316, 1.480675220489502, 1.0908852815628052, 1.1287347078323364, 1.1545106172561646, 1.127117395401001, 1.170212745666504, 1.2285714149475098, 1.195424199104309, 1.0411136150360107, 1.0257551670074463, 1.3473684787750244, 1.2306931018829346, 1.0985130071640015, 1.231636881828308, 1.2871143817901611, 1.1053271293640137, 1.069442629814148, 1.9238531589508057, 1.007318377494812, 1.0404220819473267, 1.2774970531463623, 1.1232681274414062, 1.1579712629318237, 1.0586885213851929, 1.1717771291732788, 1.2478621006011963, 1.074256181716919, 1.055283784866333, 1.1952080726623535, 1.0197622776031494, 1.0299794673919678, 1.031410813331604, 1.2419261932373047, 1.0072648525238037, 1.2307692766189575, 1.1271675825119019, 1.0644115209579468, 1.1112178564071655, 1.3001987934112549, 1.0714285373687744, 1.209811806678772, 1.2465753555297852, 1.0315608978271484, 1.175957441329956, 1.0066323280334473, 1.0968619585037231, 1.0061497688293457, 1.2941982746124268, 1.1902269124984741, 1.1140334606170654, 1.0462539196014404, 1.1233035326004028, 1.135658860206604, 1.2625865936279297, 1.009204387664795, 1.1723657846450806, 1.164102554321289, 1.044748067855835, 1.0193268060684204, 1.2469147443771362, 1.0089672803878784, 1.1682648658752441, 1.1283979415893555, 1.1927051544189453, 1.1768379211425781, 1.3042635917663574, 1.2265785932540894, 1.1747498512268066, 1.2775930166244507, 1.3353500366210938, 1.1122244596481323, 1.0921399593353271, 1.1182984113693237, 1.1118707656860352, 1.4285714626312256, 1.1324776411056519, 1.0060383081436157, 1.01658296585083, 1.0399999618530273, 1.0818712711334229, 1.343398928642273, 1.14097261428833, 1.0831929445266724, 1.0775800943374634, 1.240901231765747, 1.0562576055526733, 1.0620118379592896, 1.1650879383087158, 1.2121387720108032, 1.0987260341644287, 1.2805397510528564, 1.2734822034835815, 1.1770285367965698, 1.2470885515213013, 1.0589306354522705, 1.1421488523483276, 1.113326907157898, 1.0428813695907593, 1.0087225437164307, 1.0177396535873413, 1.1381677389144897, 1.0856155157089233, 1.1450848579406738, 1.1099462509155273, 1.2853223085403442, 1.2949244976043701, 1.0771068334579468, 1.1138724088668823, 1.376249074935913, 1.1367195844650269, 1.0258935689926147, 1.0048227310180664, 1.0804691314697266, 1.2077921628952026, 1.1179289817810059, 1.0247818231582642, 1.0533337593078613, 1.0152934789657593, 1.3259276151657104, 1.1551929712295532, 1.027023196220398, 1.1163759231567383, 1.034620761871338, 1.0023947954177856, 1.0980592966079712, 1.0727996826171875, 1.0993657112121582, 1.0096166133880615, 1.2633614540100098, 1.006080150604248, 1.38987135887146, 1.083130121231079, 1.0144809484481812, 1.3834222555160522, 1.4884752035140991, 1.6284922361373901, 1.0772651433944702, 1.009087085723877, 1.085714340209961, 1.0768135786056519, 1.0838313102722168]
********** total_n=[200] batchn=[100]  N=[3200]  nitr=[4]  alpha=[0.4]  beta=[0.2] ******
@ Average per query          [3.7652945518493652] ms
 --  Average per query NF    [1.3715612888336182] ms
 --  Average per query vegas [2.393733263015747] ms
Mean [1.149]  Median [1.121]  95th [1.384]  99th [1.568]  max [1.924]
Mean [1.149]  Median [1.121]  95th [1.384]  99th [1.568]  max [1.924]


WORKLOAD-FINISHED | Type: QueryWorkload | Progress: 1/3 | Workload-time: 11.900373 | Model-update-time: 0.000000


WORKLOAD-START | Type: DataUpdateWorkload | Progress: 2/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/Naru/eval_model.py', '--model=face', '--data_update=tupleskew', '--dataset=bjaq', '--drift_test=js', '--end2end', '--update_size=80000', '--query_seed=0', '--eval_type=drift']
Torch device: cuda:1
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
TupleSkewSample - START
0.001
TupleSkewSample - END
load_npy_dataset_from_path - df.shape = (458601, 5)
Parsing... done, took 0.0s
Data updated!
Previous data size: (382168, 5), new data size: (76433, 5)
[7.1704388e-05 6.3657761e-05 3.7372112e-05 8.9406967e-07 3.9875507e-05]
Distance score: 4.2700768972281367e-05
SAUCE Drift detection: True
Detection latency: 0.0227s

DRIFT-DETECTED after 2-th workload
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/incremental_train.py', '--dataset=bjaq', '--end2end', '--model=face', '--model_update=adapt', '--update_size=80000', '--epochs=0']
Torch device: cuda:1
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
update_size: 76433
AdaptTask - START
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
train data shape: 80000, transfer data shape:16000
Training done; evaluating likelihood on full data:
Saved to: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
AdaptTask - END

WORKLOAD-FINISHED | Type: DataUpdateWorkload | Progress: 2/3 | Workload-time: 1.998773 | Model-update-time: 2.250590


WORKLOAD-START | Type: QueryWorkload | Progress: 3/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/evaluate/Evaluate-FACE-end2end.py', '--dataset=bjaq', '--query_seed=0', '--end2end', '--num_workload=3', '--data_update=tupleskew', '--model_update=adapt']
/home/kangping/anaconda3/envs/LndCar/lib/python3.9/site-packages/torchquad/integration/vegas_map.py:314: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/BucketizationUtils.h:33.)
  iy = torch.searchsorted(self.x_edges[d, :], x[:, d], side='right')
DEVICE NAME
 Tesla V100S-PCIE-32GB
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
data shape: (458601, 5)
Save oracle results to :
./FACE/evaluate/oracle/bjaq_rng-0.csv
Found oracle card!
torch.Size([5, 2])
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
Took  8.15772032737732
tensor(0.9908)
result is  tensor(454395.5625)
Enter testHyper
ReportEsts: [1.2985347509384155, 1.011484980583191, 1.0945736169815063, 1.2690150737762451, 1.0383163690567017, 1.0020596981048584, 1.255352258682251, 1.1890441179275513, 1.1431148052215576, 1.0282764434814453, 1.0231856107711792, 1.0657384395599365, 1.0883116722106934, 1.1206401586532593, 1.113166332244873, 1.0097583532333374, 1.0971603393554688, 1.157165288925171, 1.0638477802276611, 1.2856680154800415, 1.0201464891433716, 1.0618234872817993, 1.266411304473877, 1.1302201747894287, 1.2279572486877441, 1.2223411798477173, 1.1977347135543823, 1.0815192461013794, 2.2356021404266357, 1.4507462978363037, 1.1331228017807007, 2.767195701599121, 1.0853163003921509, 1.0110282897949219, 1.124570369720459, 1.1342582702636719, 1.2323267459869385, 1.0498462915420532, 1.6074379682540894, 1.0067633390426636, 1.0686272382736206, 1.0561940670013428, 1.150303840637207, 1.1003319025039673, 1.105317234992981, 1.1517080068588257, 2.3346378803253174, 1.1060423851013184, 1.5047619342803955, 1.0194963216781616, 1.0573583841323853, 1.0221493244171143, 1.0043554306030273, 1.0935726165771484, 1.2317736148834229, 1.0158249139785767, 1.1022175550460815, 1.0790317058563232, 1.1754530668258667, 1.087751865386963, 1.7281323671340942, 1.0004373788833618, 1.2913179397583008, 1.0487920045852661, 1.0210211277008057, 1.0102512836456299, 1.3142601251602173, 1.0974667072296143, 1.0913444757461548, 1.0990393161773682, 1.0854856967926025, 1.2569605112075806, 1.2035473585128784, 1.1061980724334717, 1.0781195163726807, 1.1494473218917847, 1.2884492874145508, 1.0786303281784058, 1.0794233083724976, 1.0924038887023926, 1.061739444732666, 1.0291355848312378, 1.0578712224960327, 1.1679863929748535, 1.2724868059158325, 1.18863046169281, 1.3639527559280396, 1.6773737668991089, 1.2173264026641846, 1.2244898080825806, 1.1157222986221313, 1.1280763149261475, 1.4281448125839233, 1.230130910873413, 1.1519451141357422, 1.0516334772109985, 1.1513158082962036, 1.199765920639038, 1.1785842180252075, 1.326622724533081, 1.0812312364578247, 1.1653236150741577, 1.0149253606796265, 1.04222571849823, 1.2635035514831543, 1.1397875547409058, 1.0314100980758667, 1.0145224332809448, 1.0575757026672363, 1.131790041923523, 1.0983065366744995, 1.1312326192855835, 1.2359563112258911, 1.122638463973999, 1.2134006023406982, 1.0761584043502808, 1.002713680267334, 1.2076176404953003, 1.2208553552627563, 1.2831858396530151, 1.0087119340896606, 1.1792035102844238, 1.0168592929840088, 1.6896551847457886, 1.1216065883636475, 1.149670958518982, 1.2072632312774658, 1.3220338821411133, 1.1336712837219238, 1.0062199831008911, 1.0145585536956787, 1.1210002899169922, 1.2609107494354248, 1.6543209552764893, 1.7392219305038452, 1.0598416328430176, 1.1749651432037354, 1.0703753232955933, 1.0625922679901123, 1.0345213413238525, 1.0480402708053589, 1.2422535419464111, 1.1007006168365479, 1.02767014503479, 1.0366588830947876, 1.1049786806106567, 1.3024691343307495, 1.074035406112671, 1.0414990186691284, 1.1712812185287476, 1.021889328956604, 1.1000968217849731, 1.1003447771072388, 1.0703526735305786, 1.010575771331787, 1.1457068920135498, 1.1232554912567139, 1.0620155334472656, 1.333651065826416, 1.2913793325424194, 1.1639461517333984, 1.015616536140442, 1.278279423713684, 1.0210269689559937, 1.102461338043213, 1.0852359533309937, 1.1210204362869263, 1.0271084308624268, 1.0574655532836914, 1.2340234518051147, 1.142321228981018, 1.1905491352081299, 1.574679970741272, 1.0424553155899048, 1.159456491470337, 1.016156792640686, 1.086584210395813, 1.0804718732833862, 1.0287468433380127, 1.2593605518341064, 2.026315689086914, 1.3258928060531616, 1.2678951025009155, 1.8494598865509033, 1.4015520811080933, 1.7258566617965698, 1.1081081628799438, 1.1639894247055054, 1.42445707321167, 1.0452194213867188, 1.2446346282958984, 1.0316613912582397, 1.215242862701416, 1.5354223251342773, 1.1643704175949097, 1.26274836063385, 1.2077162265777588, 4.651515007019043, 1.129692792892456, 1.0478310585021973]
********** total_n=[200] batchn=[100]  N=[3200]  nitr=[4]  alpha=[0.4]  beta=[0.2] ******
@ Average per query          [3.7234580516815186] ms
 --  Average per query NF    [1.3606739044189453] ms
 --  Average per query vegas [2.3627841472625732] ms
Mean [1.203]  Median [1.122]  95th [1.678]  99th [2.339]  max [4.652]
Mean [1.203]  Median [1.122]  95th [1.678]  99th [2.339]  max [4.652]


WORKLOAD-FINISHED | Type: QueryWorkload | Progress: 3/3 | Workload-time: 12.395365 | Model-update-time: 0.000000


Experiment Summary: #data-update-times=1, #drift=1 | total-time=28.570376