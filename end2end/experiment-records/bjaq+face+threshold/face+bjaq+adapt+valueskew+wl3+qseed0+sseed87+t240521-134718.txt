Input arguments = Namespace(data_update='valueskew', update_size=80000, dataset='bjaq', debug=False, drift_test='js', model_update='adapt', model='face', end2end=False, query_seed=0, random_seed=42, num_workload=3)
JSON-passed parameters = {'data_updater': {'skew_ratio': '1e-3'}, 'random_seed': 87, 'face': {'learning_rate': '1e-4', 'epochs': 0, 'factor': 'auto'}}
MODEL-PATH=/home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
DATASET-PATH=/home/kangping/code_copy/SAUCE/data/bjaq/end2end/bjaq.npy
Workloads: ['QueryWorkload', 'DataUpdateWorkload', 'QueryWorkload']

WORKLOAD-START | Type: QueryWorkload | Progress: 1/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/evaluate/Evaluate-FACE-end2end.py', '--dataset=bjaq', '--end2end']
/home/kangping/anaconda3/envs/LndCar/lib/python3.9/site-packages/torchquad/integration/vegas_map.py:314: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/BucketizationUtils.h:33.)
  iy = torch.searchsorted(self.x_edges[d, :], x[:, d], side='right')
DEVICE NAME
 Tesla V100S-PCIE-32GB
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
data shape: (382168, 5)
Save oracle results to :
./FACE/evaluate/oracle/bjaq_rng-1234.csv
Found oracle card!
torch.Size([5, 2])
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
Took  8.166555404663086
tensor(0.9993)
result is  tensor(381909.8438)
Enter testHyper
ReportEsts: [1.2164424657821655, 1.0934712886810303, 1.4288091659545898, 1.0587491989135742, 1.2804217338562012, 1.094844102859497, 1.246993064880371, 1.047602891921997, 1.1163007020950317, 1.1502265930175781, 1.058570146560669, 1.105084776878357, 1.0502294301986694, 1.0580357313156128, 1.2213046550750732, 1.228162169456482, 1.2846390008926392, 1.2155581712722778, 1.1528962850570679, 1.0208333730697632, 1.1196179389953613, 1.0714285373687744, 1.1668111085891724, 1.030035138130188, 1.200114369392395, 1.1618444919586182, 1.0872747898101807, 1.3994801044464111, 1.0184816122055054, 1.1167734861373901, 1.1404753923416138, 1.3972026109695435, 1.095872402191162, 1.051772117614746, 1.107478141784668, 1.225352168083191, 1.6296192407608032, 1.1409037113189697, 1.1215388774871826, 1.0089415311813354, 1.0425552129745483, 1.1747702360153198, 1.0160046815872192, 1.0394870042800903, 1.0959222316741943, 1.0102040767669678, 1.1365008354187012, 1.2836918830871582, 1.2957061529159546, 1.0626057386398315, 1.2131578922271729, 1.4451303482055664, 1.0074645280838013, 1.2184226512908936, 1.2380419969558716, 1.0192396640777588, 1.2803738117218018, 1.2629355192184448, 1.1418474912643433, 1.4298584461212158, 1.2551281452178955, 1.1675341129302979, 1.262275218963623, 1.0734809637069702, 1.2547528743743896, 1.2857142686843872, 1.1665116548538208, 1.0250256061553955, 1.095023274421692, 1.4421052932739258, 1.225838303565979, 1.0091371536254883, 1.2453863620758057, 1.1412664651870728, 1.0829682350158691, 1.0952439308166504, 1.7854173183441162, 1.0064414739608765, 1.1101516485214233, 1.5101009607315063, 1.2479161024093628, 1.0898077487945557, 1.0432476997375488, 1.1399145126342773, 1.254199743270874, 1.0210736989974976, 1.0494937896728516, 1.010088324546814, 1.0503120422363281, 1.0227291584014893, 1.0005549192428589, 1.1809799671173096, 1.0234403610229492, 1.2158162593841553, 1.2264150381088257, 1.0946346521377563, 1.2112585306167603, 1.242165207862854, 1.0333333015441895, 1.0630090236663818, 1.1584157943725586, 1.182814121246338, 1.0178836584091187, 1.012445092201233, 1.0536611080169678, 1.2984806299209595, 1.1084381341934204, 1.191972017288208, 1.0704723596572876, 1.066290259361267, 1.0336812734603882, 1.085185170173645, 1.092029094696045, 1.0001975297927856, 1.001509189605713, 1.1464647054672241, 1.050872564315796, 1.0047723054885864, 1.221651315689087, 1.0975704193115234, 1.2053388357162476, 1.1253622770309448, 1.197106122970581, 1.1017906665802002, 1.2781007289886475, 1.2206975221633911, 1.1375638246536255, 1.125256896018982, 1.2928869724273682, 1.1165664196014404, 1.0450735092163086, 1.1656216382980347, 1.1333125829696655, 1.4285714626312256, 1.0823155641555786, 1.1244421005249023, 1.0514453649520874, 1.1040650606155396, 1.1783651113510132, 1.2278562784194946, 1.1536164283752441, 1.0733778476715088, 1.222324013710022, 1.2953180074691772, 1.0116207599639893, 1.0619009733200073, 1.0541965961456299, 1.328819990158081, 1.0890172719955444, 1.055255651473999, 1.260006308555603, 1.1743366718292236, 1.2990506887435913, 1.3201967477798462, 1.1065620183944702, 1.0372413396835327, 1.044317364692688, 1.0286540985107422, 1.048425555229187, 1.0968068838119507, 1.0435682535171509, 1.0396970510482788, 1.1843008995056152, 1.3253182172775269, 1.0161464214324951, 1.113776683807373, 1.1776832342147827, 1.2539613246917725, 1.0478943586349487, 1.0010141134262085, 1.0313822031021118, 1.012699007987976, 1.1428571939468384, 1.120158314704895, 1.1127383708953857, 1.0747653245925903, 1.0471330881118774, 1.2298704385757446, 1.287469506263733, 1.1666377782821655, 1.024357795715332, 1.0108532905578613, 1.0377635955810547, 1.1358529329299927, 1.164070963859558, 1.0940803289413452, 1.0187022686004639, 1.2554157972335815, 1.0405604839324951, 1.3408334255218506, 1.0991718769073486, 1.1190824508666992, 1.577667236328125, 1.337138295173645, 1.7697287797927856, 1.1494661569595337, 1.0950813293457031, 1.1014492511749268, 1.0422958135604858, 1.128353238105774]
********** total_n=[200] batchn=[100]  N=[3200]  nitr=[4]  alpha=[0.4]  beta=[0.2] ******
@ Average per query          [3.6730802059173584] ms
 --  Average per query NF    [1.3623380661010742] ms
 --  Average per query vegas [2.310742139816284] ms
Mean [1.151]  Median [1.118]  95th [1.401]  99th [1.631]  max [1.785]
Mean [1.151]  Median [1.118]  95th [1.401]  99th [1.631]  max [1.785]


WORKLOAD-FINISHED | Type: QueryWorkload | Progress: 1/3 | Workload-time: 11.795929 | Model-update-time: 0.000000


WORKLOAD-START | Type: DataUpdateWorkload | Progress: 2/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/Naru/eval_model.py', '--model=face', '--data_update=valueskew', '--dataset=bjaq', '--drift_test=js', '--end2end', '--update_size=80000', '--query_seed=0', '--eval_type=drift']
Torch device: cuda:1
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
ValueSample - START
ValueSample - END
load_npy_dataset_from_path - df.shape = (458601, 5)
Parsing... done, took 0.0s
Data updated!
Previous data size: (382168, 5), new data size: (76433, 5)
[9.5367432e-07 5.3644180e-07 1.7881393e-07 9.4927549e-03 4.7683716e-07]
Distance score: 0.0018989801174029708
SAUCE Drift detection: True
Detection latency: 0.0234s

DRIFT-DETECTED after 2-th workload
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/incremental_train.py', '--dataset=bjaq', '--end2end', '--model=face', '--model_update=adapt', '--update_size=80000', '--epochs=0']
Torch device: cuda:1
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
update_size: 76433
AdaptTask - START
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
train data shape: 80000, transfer data shape:16000
Training done; evaluating likelihood on full data:
Saved to: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
AdaptTask - END

WORKLOAD-FINISHED | Type: DataUpdateWorkload | Progress: 2/3 | Workload-time: 2.043309 | Model-update-time: 2.235016


WORKLOAD-START | Type: QueryWorkload | Progress: 3/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/evaluate/Evaluate-FACE-end2end.py', '--dataset=bjaq', '--query_seed=0', '--end2end', '--num_workload=3', '--data_update=valueskew', '--model_update=adapt']
/home/kangping/anaconda3/envs/LndCar/lib/python3.9/site-packages/torchquad/integration/vegas_map.py:314: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/BucketizationUtils.h:33.)
  iy = torch.searchsorted(self.x_edges[d, :], x[:, d], side='right')
DEVICE NAME
 Tesla V100S-PCIE-32GB
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
data shape: (458601, 5)
Save oracle results to :
./FACE/evaluate/oracle/bjaq_rng-0.csv
Found oracle card!
torch.Size([5, 2])
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
Took  8.165887355804443
tensor(0.9970)
result is  tensor(457215.4062)
Enter testHyper
ReportEsts: [1.1179594993591309, 1.0157904624938965, 1.064856767654419, 1.0380897521972656, 1.1666291952133179, 1.5315696001052856, 1.0242723226547241, 1.0505239963531494, 1.0835803747177124, 1.2343907356262207, 1.0639636516571045, 1.2234537601470947, 1.0286396741867065, 1.1184403896331787, 1.0277762413024902, 1.0263384580612183, 1.715796947479248, 1.0087093114852905, 1.2914204597473145, 1.1917922496795654, 1.1647838354110718, 1.5929187536239624, 1.9522655010223389, 2.1657512187957764, 1.0561244487762451, 1.2043845653533936, 1.1144474744796753, 1.1426573991775513, 1.1299992799758911, 1.1445388793945312, 1.2803030014038086, 1.4745762348175049, 1.0502848625183105, 1.0123101472854614, 1.0617096424102783, 1.1338350772857666, 1.1037002801895142, 1.2471110820770264, 1.1972169876098633, 1.0050538778305054, 1.1512713432312012, 1.116144061088562, 18.1126766204834, 1.0026522874832153, 1.0141704082489014, 1.0801414251327515, 1.2907466888427734, 1.0320314168930054, 1.4571428298950195, 1.1629054546356201, 1.0227116346359253, 1.3677226305007935, 1.1937031745910645, 1.0025442838668823, 1.0227965116500854, 1.3734599351882935, 1.0468209981918335, 1.0755541324615479, 1.030545711517334, 1.1809065341949463, 1.0428014993667603, 1.0079156160354614, 1.1487340927124023, 1.9464236497879028, 1.0027821063995361, 1.05170476436615, 1.0843486785888672, 1.099915623664856, 1.0291731357574463, 1.0992799997329712, 1.0476953983306885, 1.0451332330703735, 1.0441608428955078, 1.1527318954467773, 1.531930923461914, 1.1680562496185303, 1.1263858079910278, 3.109511137008667, 1.3386754989624023, 1.1493760347366333, 7.435118675231934, 1.052082896232605, 1.0466214418411255, 1.0965780019760132, 1.0791168212890625, 1.3485915660858154, 1.0438685417175293, 1.348703145980835, 1.1024658679962158, 1.003043293952942, 1.0699862241744995, 1.3585617542266846, 1.1622395515441895, 1.0091168880462646, 38.458499908447266, 1.0851067304611206, 1.0310388803482056, 1.1665788888931274, 1.165415644645691, 1.0249494314193726, 1.2182358503341675, 1.1107069253921509, 1.0245097875595093, 1.0561695098876953, 2.082890272140503, 1.3602818250656128, 1.363118052482605, 1.0317573547363281, 1.065902590751648, 1.1247222423553467, 1.3403091430664062, 1.0934935808181763, 1.0115132331848145, 1.0901046991348267, 1.0863986015319824, 5.153039455413818, 1.0083917379379272, 1.063801646232605, 1.0138087272644043, 1.2389380931854248, 1.021958827972412, 1.1297988891601562, 1.1386247873306274, 1.2763527631759644, 1.0697442293167114, 1.3712304830551147, 1.1737494468688965, 1.040123462677002, 1.0688185691833496, 1.0199103355407715, 1.1703940629959106, 1.1173838376998901, 1.0680056810379028, 1.489661693572998, 1.4756219387054443, 1.10570228099823, 1.229534387588501, 2.8878684043884277, 1.1312490701675415, 1.0107667446136475, 1.0588903427124023, 1.1571574211120605, 1.0160075426101685, 1.0719982385635376, 1.1070377826690674, 3.358443021774292, 1.14673912525177, 1.1448938846588135, 1.0967111587524414, 1.0913375616073608, 43.00654602050781, 1.2076894044876099, 1.3358206748962402, 1.0579127073287964, 1.0435155630111694, 1.1071566343307495, 1.0341132879257202, 1.0538461208343506, 2.2684483528137207, 1.0145580768585205, 1.0321874618530273, 1.0835129022598267, 1.1002812385559082, 1.1657301187515259, 1.0529967546463013, 1.1343302726745605, 1.0736442804336548, 1.0300610065460205, 1.0354334115982056, 1.0862820148468018, 1.1967666149139404, 1.0711045265197754, 1.44548499584198, 1.0410304069519043, 2.0573513507843018, 1.054604411125183, 1.0729314088821411, 1.253242015838623, 1.1794136762619019, 1.2133930921554565, 1.8421052694320679, 1.4957627058029175, 1.0411455631256104, 1.63494074344635, 1.2585748434066772, 1.1004694700241089, 1.0, 1.1562540531158447, 1.456722617149353, 1.0658462047576904, 1.0590940713882446, 1.0481793880462646, 1.2068208456039429, 1.4065934419631958, 1.1017993688583374, 1.0455095767974854, 1.155489444732666, 1.2206119298934937, 1.0030303001403809, 1.2574180364608765]
********** total_n=[200] batchn=[100]  N=[3200]  nitr=[4]  alpha=[0.4]  beta=[0.2] ******
@ Average per query          [3.7083137035369873] ms
 --  Average per query NF    [1.3559198379516602] ms
 --  Average per query vegas [2.352393865585327] ms
Mean [1.736]  Median [1.106]  95th [2.087]  99th [18.316]  max [43.007]
Mean [1.736]  Median [1.106]  95th [2.087]  99th [18.316]  max [43.007]


WORKLOAD-FINISHED | Type: QueryWorkload | Progress: 3/3 | Workload-time: 12.423037 | Model-update-time: 0.000000


Experiment Summary: #data-update-times=1, #drift=1 | total-time=28.557395