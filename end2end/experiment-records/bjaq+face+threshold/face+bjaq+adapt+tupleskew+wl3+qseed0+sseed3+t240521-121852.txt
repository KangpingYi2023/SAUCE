Input arguments = Namespace(data_update='tupleskew', update_size=80000, dataset='bjaq', debug=False, drift_test='js', model_update='adapt', model='face', end2end=False, query_seed=0, random_seed=42, num_workload=3)
JSON-passed parameters = {'data_updater': {'skew_ratio': '1e-3'}, 'random_seed': 3, 'face': {'learning_rate': '1e-4', 'epochs': 0, 'factor': 'auto'}}
MODEL-PATH=/home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
DATASET-PATH=/home/kangping/code_copy/SAUCE/data/bjaq/end2end/bjaq.npy
Workloads: ['QueryWorkload', 'DataUpdateWorkload', 'QueryWorkload']

WORKLOAD-START | Type: QueryWorkload | Progress: 1/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/evaluate/Evaluate-FACE-end2end.py', '--dataset=bjaq', '--end2end']
/home/kangping/anaconda3/envs/LndCar/lib/python3.9/site-packages/torchquad/integration/vegas_map.py:314: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/BucketizationUtils.h:33.)
  iy = torch.searchsorted(self.x_edges[d, :], x[:, d], side='right')
DEVICE NAME
 Tesla V100S-PCIE-32GB
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
data shape: (382168, 5)
Save oracle results to :
./FACE/evaluate/oracle/bjaq_rng-1234.csv
Found oracle card!
torch.Size([5, 2])
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
Took  8.175350904464722
tensor(0.9970)
result is  tensor(381014.0938)
Enter testHyper
ReportEsts: [1.0793102979660034, 1.107946515083313, 1.4302934408187866, 1.2737418413162231, 1.2617987394332886, 1.2001465559005737, 1.1983826160430908, 1.1410791873931885, 1.0652302503585815, 1.136743187904358, 1.0422049760818481, 1.1292372941970825, 1.187343716621399, 1.0892857313156128, 1.0152997970581055, 1.2959444522857666, 1.5278213024139404, 1.1230404376983643, 1.2590311765670776, 1.1122448444366455, 1.101956844329834, 1.0987861156463623, 1.1597760915756226, 1.1285011768341064, 1.1560075283050537, 1.141048789024353, 1.1123310327529907, 1.4510332345962524, 1.0393110513687134, 1.0416030883789062, 1.0713019371032715, 1.2509111166000366, 1.1938719749450684, 1.007232666015625, 1.0812468528747559, 1.1337742805480957, 1.3615938425064087, 1.0814507007598877, 1.0019339323043823, 1.0137724876403809, 1.0182101726531982, 1.151064395904541, 1.1116561889648438, 1.0090702772140503, 1.0927530527114868, 1.0442177057266235, 1.11427903175354, 1.290573000907898, 1.088960886001587, 1.1692047119140625, 1.1943005323410034, 1.2886849641799927, 1.0159863233566284, 1.2886934280395508, 1.31844961643219, 1.0074176788330078, 1.2345794439315796, 1.1497032642364502, 1.096761703491211, 1.3928123712539673, 1.3764162063598633, 1.1488475799560547, 1.2016712427139282, 1.0742501020431519, 1.3524590730667114, 1.1428571939468384, 1.2577732801437378, 1.0530985593795776, 1.200515627861023, 1.263157844543457, 1.1222847700119019, 1.0012518167495728, 1.217881441116333, 1.0889687538146973, 1.1975477933883667, 1.1635842323303223, 1.919899344444275, 1.00890052318573, 1.0595952272415161, 1.297614336013794, 1.205562710762024, 1.1699241399765015, 1.0434733629226685, 1.2328611612319946, 1.234110713005066, 1.0876343250274658, 1.0005321502685547, 1.0640867948532104, 1.0027602910995483, 1.0003317594528198, 1.023245096206665, 1.3594495058059692, 1.0100973844528198, 1.3843058347702026, 1.1403508186340332, 1.0874325037002563, 1.0869653224945068, 1.3114211559295654, 1.034482717514038, 1.1846269369125366, 1.1699999570846558, 1.0197652578353882, 1.0282648801803589, 1.004392385482788, 1.0967572927474976, 1.0876953601837158, 1.2814686298370361, 1.191972017288208, 1.1168580055236816, 1.0214016437530518, 1.1318737268447876, 1.1151283979415894, 1.1185169219970703, 1.0257916450500488, 1.1341137886047363, 1.1822916269302368, 1.0448181629180908, 1.054686427116394, 1.3068243265151978, 1.2105008363723755, 1.1410143375396729, 1.0765143632888794, 1.0330065488815308, 1.0732841491699219, 1.336240291595459, 1.1564176082611084, 1.0018237829208374, 1.1999167203903198, 1.2632869482040405, 1.1229125261306763, 1.0900884866714478, 1.0966157913208008, 1.1836544275283813, 1.4285714626312256, 1.330112338066101, 1.0189940929412842, 1.0130949020385742, 1.1824389696121216, 1.0390344858169556, 1.3120713233947754, 1.082632303237915, 1.1145260334014893, 1.0258287191390991, 1.3117396831512451, 1.0871038436889648, 1.0516518354415894, 1.0077272653579712, 1.4130483865737915, 1.0503052473068237, 1.0705255270004272, 1.2222620248794556, 1.1495962142944336, 1.3263328075408936, 1.088959813117981, 1.1956229209899902, 1.0503135919570923, 1.070435881614685, 1.044175624847412, 1.0108458995819092, 1.2019098997116089, 1.128053903579712, 1.0373882055282593, 1.0718879699707031, 1.2526737451553345, 1.1844416856765747, 1.0384868383407593, 1.067327618598938, 1.2752848863601685, 1.0875071287155151, 1.023522138595581, 1.0194141864776611, 1.1557483673095703, 1.1623376607894897, 1.0878894329071045, 1.041893720626831, 1.2479969263076782, 1.0092322826385498, 1.0191901922225952, 1.1955002546310425, 1.124063491821289, 1.1091177463531494, 1.130662441253662, 1.002118468284607, 1.1705821752548218, 1.1314741373062134, 1.0887949466705322, 1.0023261308670044, 1.2620302438735962, 1.0941787958145142, 1.4877805709838867, 1.1867047548294067, 1.0295923948287964, 1.5249799489974976, 1.1984297037124634, 1.7850708961486816, 1.2165725231170654, 1.175197958946228, 1.0270270109176636, 1.0250877141952515, 1.0460491180419922]
********** total_n=[200] batchn=[100]  N=[3200]  nitr=[4]  alpha=[0.4]  beta=[0.2] ******
@ Average per query          [3.7644660472869873] ms
 --  Average per query NF    [1.3615286350250244] ms
 --  Average per query vegas [2.402937412261963] ms
Mean [1.151]  Median [1.123]  95th [1.385]  99th [1.530]  max [1.920]
Mean [1.151]  Median [1.123]  95th [1.385]  99th [1.530]  max [1.920]


WORKLOAD-FINISHED | Type: QueryWorkload | Progress: 1/3 | Workload-time: 11.871538 | Model-update-time: 0.000000


WORKLOAD-START | Type: DataUpdateWorkload | Progress: 2/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/Naru/eval_model.py', '--model=face', '--data_update=tupleskew', '--dataset=bjaq', '--drift_test=js', '--end2end', '--update_size=80000', '--query_seed=0', '--eval_type=drift']
Torch device: cuda:1
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
TupleSkewSample - START
0.001
TupleSkewSample - END
load_npy_dataset_from_path - df.shape = (458601, 5)
Parsing... done, took 0.0s
Data updated!
Previous data size: (382168, 5), new data size: (76433, 5)
[8.60095024e-05 7.21216202e-06 1.03354454e-04 1.10268593e-05
 3.83257866e-05]
Distance score: 4.9185753596248105e-05
SAUCE Drift detection: True
Detection latency: 0.0228s

DRIFT-DETECTED after 2-th workload
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/incremental_train.py', '--dataset=bjaq', '--end2end', '--model=face', '--model_update=adapt', '--update_size=80000', '--epochs=0']
Torch device: cuda:1
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
update_size: 76433
AdaptTask - START
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
train data shape: 80000, transfer data shape:16000
Training done; evaluating likelihood on full data:
Saved to: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
AdaptTask - END

WORKLOAD-FINISHED | Type: DataUpdateWorkload | Progress: 2/3 | Workload-time: 1.991052 | Model-update-time: 2.236049


WORKLOAD-START | Type: QueryWorkload | Progress: 3/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/evaluate/Evaluate-FACE-end2end.py', '--dataset=bjaq', '--query_seed=0', '--end2end', '--num_workload=3', '--data_update=tupleskew', '--model_update=adapt']
/home/kangping/anaconda3/envs/LndCar/lib/python3.9/site-packages/torchquad/integration/vegas_map.py:314: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/BucketizationUtils.h:33.)
  iy = torch.searchsorted(self.x_edges[d, :], x[:, d], side='right')
DEVICE NAME
 Tesla V100S-PCIE-32GB
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
data shape: (458601, 5)
Save oracle results to :
./FACE/evaluate/oracle/bjaq_rng-0.csv
Found oracle card!
torch.Size([5, 2])
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
Took  8.17006540298462
tensor(0.9938)
result is  tensor(455756.0938)
Enter testHyper
ReportEsts: [1.0973368883132935, 1.021233320236206, 1.8448275327682495, 1.1927059888839722, 1.0030936002731323, 1.0186316967010498, 1.3249247074127197, 1.0905917882919312, 1.0000516176223755, 1.1900372505187988, 1.0185648202896118, 1.2406113147735596, 1.0422885417938232, 1.1437758207321167, 1.0170232057571411, 1.1085457801818848, 1.2987371683120728, 1.0721485614776611, 1.0193079710006714, 1.033972144126892, 1.049398422241211, 1.035792350769043, 1.1558729410171509, 1.0440136194229126, 1.099696159362793, 1.2767300605773926, 1.0355948209762573, 1.3218098878860474, 1.2436500787734985, 1.0377224683761597, 1.2317596673965454, 1.51694917678833, 1.018061637878418, 1.0346331596374512, 1.106207013130188, 1.1420068740844727, 1.2209177017211914, 1.0408433675765991, 1.1364059448242188, 1.0518789291381836, 1.0662336349487305, 1.1047253608703613, 5.234177112579346, 1.0379981994628906, 1.014721155166626, 1.0219613313674927, 1.0668134689331055, 1.3037197589874268, 1.4792453050613403, 1.1109031438827515, 1.0188173055648804, 1.0026068687438965, 1.042614459991455, 1.019446849822998, 1.0597842931747437, 1.3760405778884888, 1.3149797916412354, 1.082210659980774, 1.2547876834869385, 1.1042773723602295, 1.1526881456375122, 1.0764734745025635, 1.1232548952102661, 1.1034765243530273, 1.0237065553665161, 1.0791198015213013, 1.2811256647109985, 1.072056531906128, 1.1722815036773682, 1.1809595823287964, 1.0297293663024902, 1.0923389196395874, 1.0818743705749512, 1.2161147594451904, 1.0456910133361816, 1.2777844667434692, 1.1428298950195312, 1.029847264289856, 1.1148065328598022, 1.1439833641052246, 1.1541823148727417, 1.101922631263733, 1.0513657331466675, 1.0371928215026855, 1.3034398555755615, 1.6170213222503662, 1.1126850843429565, 1.3954163789749146, 1.182037115097046, 1.6463245153427124, 1.061772108078003, 1.2158278226852417, 1.3353580236434937, 1.115594744682312, 1.1819758415222168, 1.0291281938552856, 1.0155974626541138, 1.175856113433838, 1.2575089931488037, 1.1476616859436035, 1.1837741136550903, 1.2015438079833984, 1.50632905960083, 1.0434874296188354, 1.1387041807174683, 1.0648658275604248, 1.2089908123016357, 1.0079238414764404, 1.048710584640503, 1.156274437904358, 1.0344539880752563, 1.0616133213043213, 1.1378568410873413, 1.3625668287277222, 1.0326213836669922, 1.0080004930496216, 1.0886406898498535, 1.1704254150390625, 1.2551599740982056, 1.0796459913253784, 1.0982999801635742, 1.2878504991531372, 1.046634554862976, 1.032501220703125, 1.0985112190246582, 1.0295376777648926, 1.1480618715286255, 1.298325777053833, 1.068618893623352, 1.0151941776275635, 1.7188146114349365, 1.1975120306015015, 1.1414401531219482, 1.344862699508667, 1.706421971321106, 1.0353879928588867, 1.0396461486816406, 1.1015220880508423, 1.2285795211791992, 1.0298103094100952, 1.0799739360809326, 1.1157866716384888, 1.0748635530471802, 1.06256103515625, 1.2028148174285889, 1.2160825729370117, 1.318750023841858, 1.0931899547576904, 1.0600554943084717, 1.3243175745010376, 1.0525109767913818, 1.1966924667358398, 1.1918257474899292, 1.0130829811096191, 1.0975847244262695, 1.1154204607009888, 1.1072810888290405, 1.1138211488723755, 1.0739428997039795, 1.1909055709838867, 1.040873646736145, 1.1559523344039917, 1.117722511291504, 1.1374341249465942, 1.1971725225448608, 1.0684285163879395, 1.0251545906066895, 1.1313252449035645, 1.0112123489379883, 1.3698058128356934, 1.1844497919082642, 1.08052396774292, 1.3973370790481567, 1.343946099281311, 1.09550940990448, 1.060980200767517, 1.0637372732162476, 1.1121854782104492, 1.005023717880249, 1.024735450744629, 1.7105263471603394, 1.0557782649993896, 1.0334293842315674, 1.6834282875061035, 1.2023111581802368, 1.0600241422653198, 1.1216555833816528, 1.1462347507476807, 1.481065034866333, 1.020953893661499, 1.3451313972473145, 1.1922153234481812, 1.2818928956985474, 1.786966323852539, 1.0302841663360596, 1.1201063394546509, 1.3839350938796997, 1.0593349933624268, 1.4108020067214966, 1.0086162090301514]
********** total_n=[200] batchn=[100]  N=[3200]  nitr=[4]  alpha=[0.4]  beta=[0.2] ******
@ Average per query          [3.742736577987671] ms
 --  Average per query NF    [1.3560795783996582] ms
 --  Average per query vegas [2.3866569995880127] ms
Mean [1.179]  Median [1.108]  95th [1.507]  99th [1.788]  max [5.234]
Mean [1.179]  Median [1.108]  95th [1.507]  99th [1.788]  max [5.234]


WORKLOAD-FINISHED | Type: QueryWorkload | Progress: 3/3 | Workload-time: 12.403327 | Model-update-time: 0.000000


Experiment Summary: #data-update-times=1, #drift=1 | total-time=28.557467