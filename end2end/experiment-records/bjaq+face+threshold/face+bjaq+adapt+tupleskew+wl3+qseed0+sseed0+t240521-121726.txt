Input arguments = Namespace(data_update='tupleskew', update_size=80000, dataset='bjaq', debug=False, drift_test='js', model_update='adapt', model='face', end2end=False, query_seed=0, random_seed=42, num_workload=3)
JSON-passed parameters = {'data_updater': {'skew_ratio': '1e-3'}, 'random_seed': 0, 'face': {'learning_rate': '1e-4', 'epochs': 0, 'factor': 'auto'}}
MODEL-PATH=/home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
DATASET-PATH=/home/kangping/code_copy/SAUCE/data/bjaq/end2end/bjaq.npy
Workloads: ['QueryWorkload', 'DataUpdateWorkload', 'QueryWorkload']

WORKLOAD-START | Type: QueryWorkload | Progress: 1/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/evaluate/Evaluate-FACE-end2end.py', '--dataset=bjaq', '--end2end']
/home/kangping/anaconda3/envs/LndCar/lib/python3.9/site-packages/torchquad/integration/vegas_map.py:314: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/BucketizationUtils.h:33.)
  iy = torch.searchsorted(self.x_edges[d, :], x[:, d], side='right')
DEVICE NAME
 Tesla V100S-PCIE-32GB
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
data shape: (382168, 5)
Save oracle results to :
./FACE/evaluate/oracle/bjaq_rng-1234.csv
Found oracle card!
torch.Size([5, 2])
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
Took  8.17357325553894
tensor(0.9902)
result is  tensor(378417.6562)
Enter testHyper
ReportEsts: [1.2244958877563477, 1.089346170425415, 1.2751179933547974, 1.2419456243515015, 1.2632986307144165, 1.0716506242752075, 1.254618763923645, 1.1446406841278076, 1.0757079124450684, 1.1645172834396362, 1.0826873779296875, 1.1853814125061035, 1.0030934810638428, 1.0825892686843872, 1.0868629217147827, 1.3798049688339233, 1.3901902437210083, 1.2745842933654785, 1.1698375940322876, 1.163265347480774, 1.0375041961669922, 1.0244568586349487, 1.1160380840301514, 1.0747840404510498, 1.0814342498779297, 1.1193490028381348, 1.0748873949050903, 1.4037375450134277, 1.022932767868042, 1.107618808746338, 1.153955340385437, 1.1170337200164795, 1.155686855316162, 1.0109624862670898, 1.1429336071014404, 1.0171833038330078, 1.286250114440918, 1.2480512857437134, 1.0631943941116333, 1.1022754907608032, 1.023930549621582, 1.3184566497802734, 1.0755819082260132, 1.063335657119751, 1.0654975175857544, 1.0827664136886597, 1.1081500053405762, 1.2174367904663086, 1.0521113872528076, 1.027072787284851, 1.152500033378601, 1.1843732595443726, 1.0836427211761475, 1.2225656509399414, 1.2428722381591797, 1.0092720985412598, 1.2373831272125244, 1.1503016948699951, 1.1979471445083618, 1.3158310651779175, 1.4665104150772095, 1.1594873666763306, 1.3046576976776123, 1.1113554239273071, 1.235954999923706, 1.2285714149475098, 1.2186589241027832, 1.0149867534637451, 1.1313581466674805, 1.3368420600891113, 1.0951541662216187, 1.0199100971221924, 1.2436931133270264, 1.1505995988845825, 1.0019477605819702, 1.0752516984939575, 1.9847559928894043, 1.0247361660003662, 1.1070233583450317, 1.2720001935958862, 1.3861403465270996, 1.081085443496704, 1.0742480754852295, 1.228872537612915, 1.239192008972168, 1.0994535684585571, 1.0008784532546997, 1.1368473768234253, 1.0372251272201538, 1.1333444118499756, 1.003105878829956, 1.2851190567016602, 1.0201947689056396, 1.3111004829406738, 1.1963189840316772, 1.032525658607483, 1.0632598400115967, 1.2465918064117432, 1.034482717514038, 1.1113191843032837, 1.1454545259475708, 1.0801993608474731, 1.0470207929611206, 1.0007320642471313, 1.0989539623260498, 1.0208882093429565, 1.0224971771240234, 1.1762652397155762, 1.066279411315918, 1.0392438173294067, 1.2782647609710693, 1.0963517427444458, 1.3737283945083618, 1.0684360265731812, 1.093079924583435, 1.1237623691558838, 1.084486484527588, 1.0318586826324463, 1.3016674518585205, 1.0438662767410278, 1.1470954418182373, 1.0765833854675293, 1.0977529287338257, 1.243537187576294, 1.4370155334472656, 1.2400949001312256, 1.1751986742019653, 1.0410116910934448, 1.329604148864746, 1.1148964166641235, 1.1106617450714111, 1.1293140649795532, 1.259167194366455, 1.4642857313156128, 1.0657788515090942, 1.0546075105667114, 1.028635859489441, 1.0969105958938599, 1.0859463214874268, 1.4265474081039429, 1.1637712717056274, 1.091017484664917, 1.0867185592651367, 1.2579058408737183, 1.0845643281936646, 1.004302740097046, 1.0124709606170654, 1.395700216293335, 1.0309134721755981, 1.121519923210144, 1.0335426330566406, 1.1643378734588623, 1.2695876359939575, 1.134934902191162, 1.02751624584198, 1.0352603197097778, 1.1005401611328125, 1.0036578178405762, 1.0184838771820068, 1.2222620248794556, 1.2373617887496948, 1.0291385650634766, 1.0088979005813599, 1.2783082723617554, 1.1770573854446411, 1.045364260673523, 1.0799602270126343, 1.2773319482803345, 1.0300015211105347, 1.0259889364242554, 1.047060251235962, 1.0142481327056885, 1.201298713684082, 1.0306737422943115, 1.0269073247909546, 1.0769543647766113, 1.001943588256836, 1.2285653352737427, 1.1619304418563843, 1.1220271587371826, 1.0237013101577759, 1.0462429523468018, 1.0228424072265625, 1.226762056350708, 1.210793137550354, 1.1575052738189697, 1.0121768712997437, 1.305121660232544, 1.0455739498138428, 1.2888911962509155, 1.0243425369262695, 1.0985331535339355, 1.3561415672302246, 1.2111084461212158, 1.6259870529174805, 1.1144335269927979, 1.0571905374526978, 1.1014492511749268, 1.0564162731170654, 1.175298810005188]
********** total_n=[200] batchn=[100]  N=[3200]  nitr=[4]  alpha=[0.4]  beta=[0.2] ******
@ Average per query          [3.7851011753082275] ms
 --  Average per query NF    [1.378800868988037] ms
 --  Average per query vegas [2.4063003063201904] ms
Mean [1.145]  Median [1.108]  95th [1.380]  99th [1.468]  max [1.985]
Mean [1.145]  Median [1.108]  95th [1.380]  99th [1.468]  max [1.985]


WORKLOAD-FINISHED | Type: QueryWorkload | Progress: 1/3 | Workload-time: 11.858955 | Model-update-time: 0.000000


WORKLOAD-START | Type: DataUpdateWorkload | Progress: 2/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/Naru/eval_model.py', '--model=face', '--data_update=tupleskew', '--dataset=bjaq', '--drift_test=js', '--end2end', '--update_size=80000', '--query_seed=0', '--eval_type=drift']
Torch device: cuda:1
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
TupleSkewSample - START
0.001
TupleSkewSample - END
load_npy_dataset_from_path - df.shape = (458601, 5)
Parsing... done, took 0.0s
Data updated!
Previous data size: (382168, 5), new data size: (76433, 5)
[4.6348572e-04 2.1886826e-04 4.7719479e-04 1.1605024e-04 1.2099743e-05]
Distance score: 0.00025753973750397563
SAUCE Drift detection: True
Detection latency: 0.0230s

DRIFT-DETECTED after 2-th workload
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/incremental_train.py', '--dataset=bjaq', '--end2end', '--model=face', '--model_update=adapt', '--update_size=80000', '--epochs=0']
Torch device: cuda:1
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
update_size: 76433
AdaptTask - START
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
train data shape: 80000, transfer data shape:16000
Training done; evaluating likelihood on full data:
Saved to: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
AdaptTask - END

WORKLOAD-FINISHED | Type: DataUpdateWorkload | Progress: 2/3 | Workload-time: 1.993897 | Model-update-time: 2.222044


WORKLOAD-START | Type: QueryWorkload | Progress: 3/3
Going to run: ['python', '/home/kangping/code_copy/SAUCE/FACE/evaluate/Evaluate-FACE-end2end.py', '--dataset=bjaq', '--query_seed=0', '--end2end', '--num_workload=3', '--data_update=tupleskew', '--model_update=adapt']
/home/kangping/anaconda3/envs/LndCar/lib/python3.9/site-packages/torchquad/integration/vegas_map.py:314: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/BucketizationUtils.h:33.)
  iy = torch.searchsorted(self.x_edges[d, :], x[:, d], side='right')
DEVICE NAME
 Tesla V100S-PCIE-32GB
GET-MODEL-PATH=./models/end2end/face/bjaq-id2-best-val.t
Load model - START
Load model from: /home/kangping/code_copy/SAUCE/models/end2end/face/bjaq-id2-best-val.t
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
Load model - END
There are 104179 trainable parameters in this model.
Parameters total size is 0.3974113464355469 MB
GET-DATASET-PATH=./data/bjaq/end2end/bjaq.npy
data shape: (458601, 5)
Save oracle results to :
./FACE/evaluate/oracle/bjaq_rng-0.csv
Found oracle card!
torch.Size([5, 2])
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
******** ANY
Took  8.197937488555908
tensor(0.9936)
result is  tensor(455656.0625)
Enter testHyper
ReportEsts: [1.0196956396102905, 1.0570425987243652, 1.3849765062332153, 1.0980664491653442, 1.0042104721069336, 1.3283932209014893, 1.1377516984939575, 1.0549322366714478, 1.122630000114441, 1.0196647644042969, 1.2827872037887573, 5.360000133514404, 1.0572792291641235, 1.1341540813446045, 1.1380465030670166, 1.2356854677200317, 1.090320110321045, 1.0266309976577759, 1.0268585681915283, 1.2908638715744019, 1.1425822973251343, 1.2162628173828125, 1.1091639995574951, 1.0627551078796387, 1.135686993598938, 1.275912880897522, 1.1636254787445068, 1.2592031955718994, 1.3278441429138184, 1.188260555267334, 1.425407886505127, 1.3728814125061035, 1.1839723587036133, 1.1371655464172363, 1.0153025388717651, 1.022544026374817, 1.0589258670806885, 1.0152552127838135, 1.5064934492111206, 1.0123862028121948, 1.0668580532073975, 1.8681671619415283, 1.0317174196243286, 1.0647767782211304, 1.010191798210144, 1.113676905632019, 1.2572300434112549, 1.04342520236969, 1.2666666507720947, 1.1799554824829102, 1.0652179718017578, 1.0816566944122314, 1.023768424987793, 1.0922105312347412, 1.1754662990570068, 1.365723967552185, 1.34037184715271, 1.178766131401062, 1.203120470046997, 1.1864646673202515, 1.2671394348144531, 1.061034083366394, 1.0005319118499756, 1.167930245399475, 1.0108309984207153, 1.1287593841552734, 1.4064104557037354, 1.0607484579086304, 1.175748348236084, 1.1395633220672607, 1.0329519510269165, 1.2830332517623901, 1.2000682353973389, 1.0778090953826904, 1.0565032958984375, 1.1675257682800293, 1.1975529193878174, 1.2837380170822144, 1.2071136236190796, 1.1276741027832031, 1.2842357158660889, 1.0944299697875977, 1.0682592391967773, 1.134948968887329, 1.177355170249939, 1.6085106134414673, 1.226111888885498, 1.4808175563812256, 1.1293085813522339, 1.05222487449646, 1.061525583267212, 1.1488518714904785, 1.482975721359253, 1.0009808540344238, 1.0816162824630737, 1.01656973361969, 1.1786679029464722, 1.2066963911056519, 1.3376365900039673, 1.1666544675827026, 1.2216262817382812, 1.2295526266098022, 1.0196079015731812, 1.1951860189437866, 1.1857736110687256, 1.1253957748413086, 1.014629602432251, 1.168228268623352, 1.0257879495620728, 1.1000076532363892, 1.1100983619689941, 1.0329101085662842, 1.3275477886199951, 1.4027793407440186, 1.0066639184951782, 1.0406676530838013, 1.1316970586776733, 1.3798906803131104, 1.060814619064331, 1.230088472366333, 1.084458827972412, 1.2564102411270142, 1.0505592823028564, 1.3287715911865234, 1.1344839334487915, 1.142757773399353, 1.1396594047546387, 1.0801281929016113, 1.0238723754882812, 1.0997365713119507, 1.032386302947998, 1.1201496124267578, 1.1062687635421753, 1.6628787517547607, 1.2599830627441406, 1.1310136318206787, 1.0845575332641602, 1.1992443799972534, 1.0914192199707031, 1.087415337562561, 1.030697226524353, 1.148238182067871, 1.0581966638565063, 1.0553981065750122, 1.020201563835144, 1.1003243923187256, 3.867924451828003, 1.0899907350540161, 1.14789617061615, 1.1945472955703735, 1.135794758796692, 1.0710526704788208, 1.1699893474578857, 1.1221827268600464, 1.0754321813583374, 1.0734171867370605, 1.2779005765914917, 1.0703125, 1.0703693628311157, 1.0273733139038086, 1.0430768728256226, 1.1386969089508057, 2.0591349601745605, 1.0433499813079834, 1.2249565124511719, 1.0868990421295166, 1.0412667989730835, 1.06953763961792, 2.34181809425354, 1.3612180948257446, 1.1133233308792114, 1.0966848134994507, 1.4041193723678589, 1.0297043323516846, 1.179290771484375, 1.0968388319015503, 1.0806031227111816, 1.0418542623519897, 1.000272512435913, 1.0032533407211304, 1.894736886024475, 1.070064663887024, 1.0051369667053223, 1.7337883710861206, 1.2426910400390625, 1.0504000186920166, 1.0865486860275269, 1.1186021566390991, 1.3044339418411255, 1.0096690654754639, 1.1267807483673096, 1.0895280838012695, 1.043778896331787, 1.8063348531723022, 1.0855042934417725, 1.1324577331542969, 1.1627799272537231, 1.200483798980713, 1.4567062854766846, 1.2130779027938843]
********** total_n=[200] batchn=[100]  N=[3200]  nitr=[4]  alpha=[0.4]  beta=[0.2] ******
@ Average per query          [3.6374735832214355] ms
 --  Average per query NF    [1.3555848598480225] ms
 --  Average per query vegas [2.281888723373413] ms
Mean [1.207]  Median [1.128]  95th [1.512]  99th [2.357]  max [5.360]
Mean [1.207]  Median [1.128]  95th [1.512]  99th [2.357]  max [5.360]


WORKLOAD-FINISHED | Type: QueryWorkload | Progress: 3/3 | Workload-time: 12.375325 | Model-update-time: 0.000000


Experiment Summary: #data-update-times=1, #drift=1 | total-time=28.498863